
@book{antiga_deep_2020,
	address = {Shelter Island, NY},
	edition = {1st},
	title = {Deep {Learning} with {PyTorch}},
	abstract = {Intro -- Copyright -- dedication -- contents -- front matter -- foreword -- preface -- acknowledgments -- about this book -- Who should read this book -- How this book is organized: A roadmap -- About the code -- Hardware and software requirements -- liveBook discussion forum -- Other online resources -- about the authors -- about the cover illustration -- Part 1. Core PyTorch -- 1 Introducing deep learning and the PyTorch Library -- 1.1 The deep learning revolution -- 1.2 PyTorch for deep learning -- 1.3 Why PyTorch? -- 1.3.1 The deep learning competitive landscape -- 1.4 An overview of how PyTorch supports deep learning projects -- 1.5 Hardware and software requirements -- 1.5.1 Using Jupyter Notebooks -- 1.6 Exercises -- 1.7 Summary -- 2 Pretrained networks -- 2.1 A pretrained network that recognizes the subject of an image -- 2.1.1 Obtaining a pretrained network for image recognition -- 2.1.2 AlexNet -- 2.1.3 ResNet -- 2.1.4 Ready, set, almost run -- 2.1.5 Run! -- 2.2 A pretrained model that fakes it until it makes it -- 2.2.1 The GAN game -- 2.2.2 CycleGAN -- 2.2.3 A network that turns horses into zebras -- 2.3 A pretrained network that describes scenes -- 2.3.1 NeuralTalk2 -- 2.4 Torch Hub -- 2.5 Conclusion -- 2.6 Exercises -- 2.7 Summary -- 3 It starts with a tensor -- 3.1 The world as floating-point numbers -- 3.2 Tensors: Multidimensional arrays -- 3.2.1 From Python lists to PyTorch tensors -- 3.2.2 Constructing our first tensors -- 3.2.3 The essence of tensors -- 3.3 Indexing tensors -- 3.4 Named tensors -- 3.5 Tensor element types -- 3.5.1 Specifying the numeric type with dtype -- 3.5.2 A dtype for every occasion -- 3.5.3 Managing a tensor's dtype attribute -- 3.6 The tensor API -- 3.7 Tensors: Scenic views of storage -- 3.7.1 Indexing into storage -- 3.7.2 Modifying stored values: In-place operations., 3.8 Tensor metadata: Size, offset, and stride -- 3.8.1 Views of another tensor's storage -- 3.8.2 Transposing without copying -- 3.8.3 Transposing in higher dimensions -- 3.8.4 Contiguous tensors -- 3.9 Moving tensors to the GPU -- 3.9.1 Managing a tensor's device attribute -- 3.10 NumPy interoperability -- 3.11 Generalized tensors are tensors, too -- 3.12 Serializing tensors -- 3.12.1 Serializing to HDF5 with h5py -- 3.13 Conclusion -- 3.14 Exercises -- 3.15 Summary -- 4 Real-world data representation using tensors -- 4.1 Working with images -- 4.1.1 Adding color channels -- 4.1.2 Loading an image file -- 4.1.3 Changing the layout -- 4.1.4 Normalizing the data -- 4.2 3D images: Volumetric data -- 4.2.1 Loading a specialized format -- 4.3 Representing tabular data -- 4.3.1 Using a real-world dataset -- 4.3.2 Loading a wine data tensor -- 4.3.3 Representing scores -- 4.3.4 One-hot encoding -- 4.3.5 When to categorize -- 4.3.6 Finding thresholds -- 4.4 Working with time series -- 4.4.1 Adding a time dimension -- 4.4.2 Shaping the data by time period -- 4.4.3 Ready for training -- 4.5 Representing text -- 4.5.1 Converting text to numbers -- 4.5.2 One-hot-encoding characters -- 4.5.3 One-hot encoding whole words -- 4.5.4 Text embeddings -- 4.5.5 Text embeddings as a blueprint -- 4.6 Conclusion -- 4.7 Exercises -- 4.8 Summary -- 5 The mechanics of learning -- 5.1 A timeless lesson in modeling -- 5.2 Learning is just parameter estimation -- 5.2.1 A hot problem -- 5.2.2 Gathering some data -- 5.2.3 Visualizing the data -- 5.2.4 Choosing a linear model as a first try -- 5.3 Less loss is what we want -- 5.3.1 From problem back to PyTorch -- 5.4 Down along the gradient -- 5.4.1 Decreasing loss -- 5.4.2 Getting analytical -- 5.4.3 Iterating to fit the model -- 5.4.4 Normalizing inputs -- 5.4.5 Visualizing (again)., 5.5 PyTorch's autograd: Backpropagating all things -- 5.5.1 Computing the gradient automatically -- 5.5.2 Optimizers a la carte -- 5.5.3 Training, validation, and overfitting -- 5.5.4 Autograd nits and switching it off -- 5.6 Conclusion -- 5.7 Exercise -- 5.8 Summary -- 6 Using a neural network to fit the data -- 6.1 Artificial neurons -- 6.1.1 Composing a multilayer network -- 6.1.2 Understanding the error function -- 6.1.3 All we need is activation -- 6.1.4 More activation functions -- 6.1.5 Choosing the best activation function -- 6.1.6 What learning means for a neural network -- 6.2 The PyTorch nn module -- 6.2.1 Using \_\_call\_\_ rather than forward -- 6.2.2 Returning to the linear model -- 6.3 Finally a neural network -- 6.3.1 Replacing the linear model -- 6.3.2 Inspecting the parameters -- 6.3.3 Comparing to the linear model -- 6.4 Conclusion -- 6.5 Exercises -- 6.6 Summary -- 7 Telling birds from airplanes: Learning from images -- 7.1 A dataset of tiny images -- 7.1.1 Downloading CIFAR-10 -- 7.1.2 The Dataset class -- 7.1.3 Dataset transforms -- 7.1.4 Normalizing data -- 7.2 Distinguishing birds from airplanes -- 7.2.1 Building the dataset -- 7.2.2 A fully connected model -- 7.2.3 Output of a classifier -- 7.2.4 Representing the output as probabilities -- 7.2.5 A loss for classifying -- 7.2.6 Training the classifier -- 7.2.7 The limits of going fully connected -- 7.3 Conclusion -- 7.4 Exercises -- 7.5 Summary -- 8 Using convolutions to generalize -- 8.1 The case for convolutions -- 8.1.1 What convolutions do -- 8.2 Convolutions in action -- 8.2.1 Padding the boundary -- 8.2.2 Detecting features with convolutions -- 8.2.3 Looking further with depth and pooling -- 8.2.4 Putting it all together for our network -- 8.3 Subclassing nn.Module -- 8.3.1 Our network as an nn.Module -- 8.3.2 How PyTorch keeps track of parameters and submodules., 8.3.3 The functional API -- 8.4 Training our convnet -- 8.4.1 Measuring accuracy -- 8.4.2 Saving and loading our model -- 8.4.3 Training on the GPU -- 8.5 Model design -- 8.5.1 Adding memory capacity: Width -- 8.5.2 Helping our model to converge and generalize: Regularization -- 8.5.3 Going deeper to learn more complex structures: Depth -- 8.5.4 Comparing the designs from this section -- 8.5.5 It's already outdated -- 8.6 Conclusion -- 8.7 Exercises -- 8.8 Summary -- Part 2. Learning from images in the real world: Early detection of lung cancer -- 9 Using PyTorch to fight cancer -- 9.1 Introduction to the use case -- 9.2 Preparing for a large-scale project -- 9.3 What is a CT scan, exactly? -- 9.4 The project: An end-to-end detector for lung cancer -- 9.4.1 Why can't we just throw data at a neural network until it works? -- 9.4.2 What is a nodule? -- 9.4.3 Our data source: The LUNA Grand Challenge -- 9.4.4 Downloading the LUNA data -- 9.5 Conclusion -- 9.6 Summary -- 10 Combining data sources into a unified dataset -- 10.1 Raw CT data files -- 10.2 Parsing LUNA's annotation data -- 10.2.1 Training and validation sets -- 10.2.2 Unifying our annotation and candidate data -- 10.3 Loading individual CT scans -- 10.3.1 Hounsfield Units -- 10.4 Locating a nodule using the patient coordinate system -- 10.4.1 The patient coordinate system -- 10.4.2 CT scan shape and voxel sizes -- 10.4.3 Converting between millimeters and voxel addresses -- 10.4.4 Extracting a nodule from a CT scan -- 10.5 A straightforward dataset implementation -- 10.5.1 Caching candidate arrays with the getCtRawCandidate function -- 10.5.2 Constructing our dataset in LunaDataset.\_\_init\_\_ -- 10.5.3 A training/validation split -- 10.5.4 Rendering the data -- 10.6 Conclusion -- 10.7 Exercises -- Summary -- 11 Training a classification model to detect suspected tumors., 11.1 A foundational model and training loop -- 11.2 The main entry point for our application -- 11.3 Pretraining setup and initialization -- 11.3.1 Initializing the model and optimizer -- 11.3.2 Care and feeding of data loaders -- 11.4 Our first-pass neural network design -- 11.4.1 The core convolutions -- 11.4.2 The full model -- 11.5 Training and validating the model -- 11.5.1 The computeBatchLoss function -- 11.5.2 The validation loop is similar -- 11.6 Outputting performance metrics -- 11.6.1 The logMetrics function -- 11.7 Running the training script -- 11.7.1 Needed data for training -- 11.7.2 Interlude: The enumerateWithEstimate function -- 11.8 Evaluating the model: Getting 99.7\% correct means we're done, right? -- 11.9 Graphing training metrics with TensorBoard -- 11.9.1 Running TensorBoard -- 11.9.2 Adding TensorBoard support to the metrics logging function -- 11.10 Why isn't the model learning to detect nodules? -- 11.11 Conclusion -- 11.12 Exercises -- 11.13 Summary -- 12 Improving training with metrics and augmentation -- 12.1 High-level plan for improvement -- 12.2 Good dogs vs. bad guys: False positives and false negatives -- 12.3 Graphing the positives and negatives -- 12.3.1 Recall is Roxie's strength -- 12.3.2 Precision is Preston's forte -- 12.3.3 Implementing precision and recall in logMetrics -- 12.3.4 Our ultimate performance metric: The F1 score -- 12.3.5 How does our model perform with our new metrics? -- 12.4 What does an ideal dataset look like? -- 12.4.1 Making the data look less like the actual and more like the "ideal" -- 12.4.2 Contrasting training with a balanced LunaDataset to previous runs -- 12.4.3 Recognizing the symptoms of overfitting -- 12.5 Revisiting the problem of overfitting -- 12.5.1 An overfit face-to-age prediction model -- 12.6 Preventing overfitting with data augmentation., 12.6.1 Specific data augmentation techniques., Deep Learning with PyTorch teaches you to create neural networks and deep learning systems with PyTorch. This practical book quickly gets you to work building a real-world example from scratch: a tumor image classifier. Along the way, it covers best practices for the entire DL pipeline, including the PyTorch Tensor API, loading data in Python, monitoring training, and visualizing results. After covering the basics, the book will take you on a journey through larger projects. The centerpiece of the book is a neural network designed for cancer detection. You'll discover ways for training networks with limited inputs and start processing data to get some results. You'll sift through the unreliable initial results and focus on how to diagnose and fix the problems in your neural network. Finally, you'll look at ways to improve your results by training with augmented data, make improvements to the model architecture, and perform other fine tuning.},
	language = {eng},
	publisher = {Manning Publications Co.},
	author = {Antiga, Luca and Viehmann, Thomas and Stevens, Eli},
	year = {2020},
	keywords = {Electronic books},
	annote = {Helpful for:
Using image data in deep learning},
}

@inproceedings{holste_end--end_2021,
	title = {End-to-{End} {Learning} of {Fused} {Image} and {Non}-{Image} {Features} for {Improved} {Breast} {Cancer} {Classification} {From} {MRI}},
	language = {en},
	urldate = {2022-01-27},
	author = {Holste, Gregory and Partridge, Savannah C. and Rahbar, Habib and Biswas, Debosmita and Lee, Christoph I. and Alessio, Adam M.},
	year = {2021},
	pages = {3294--3303},
	annote = {citing Deep learning with multimodal representation for pancancer prognosis prediction},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\466HXQUF\\Holste et al. - 2021 - End-to-End Learning of Fused Image and Non-Image F.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\YAGCDFFT\\Holste_End-to-End_Learning_of_Fused_Image_and_Non-Image_Features_for_Improved_ICCVW_2021_paper.html:text/html},
}

@inproceedings{menegotto_computer-aided_2020,
	address = {Cham},
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {Computer-{Aided} {Hepatocarcinoma} {Diagnosis} {Using} {Multimodal} {Deep} {Learning}},
	isbn = {978-3-030-24097-4},
	doi = {10.1007/978-3-030-24097-4_1},
	abstract = {Liver cancer was the fourth most deadly cancer in 2018 worldwide. Among liver cancers, hepatocarcinoma is the most prevalent cancer type. Diagnostic protocols are complex and suggest variations based on the patient{\textquoteright}s context and the use of multiple data modalities. This paper briefly describes the steps involved in the development of a hepatocarcinoma computer-aided diagnosis using a multimodal deep learning approach with imaging and tabular data fusion. Data acquisition, preprocessing steps, architectural design decisions and possible use cases for the described architecture are discussed based on the partial results achieved on this ongoing research.},
	language = {en},
	booktitle = {Ambient {Intelligence} {\textendash} {Software} and {Applications} {\textendash},10th {International} {Symposium} on {Ambient} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Menegotto, Alan Baronio and Lopes~Becker, Carla Diniz and Cazella, Silvio Cesar},
	editor = {Novais, Paulo and Lloret, Jaime and Chamoso, Pablo and Carneiro, Davide and Navarro, Elena and Omatu, Sigeru},
	year = {2020},
	keywords = {Computer-aided diagnosis, e-health, Hepatocarcinoma, Multimodal deep learning},
	pages = {3--10},
	annote = {???},
}

@article{hua_bimodal_2019,
	title = {Bimodal learning via trilogy of skip-connection deep networks for diabetic retinopathy risk progression identification},
	volume = {132},
	issn = {1386-5056},
	url = {https://www.sciencedirect.com/science/article/pii/S1386505619303557},
	doi = {10.1016/j.ijmedinf.2019.07.005},
	abstract = {Background
Diabetic Retinopathy (DR) is considered a pathology of retinal vascular complications, which stays in the top causes of vision impairment and blindness. Therefore, precisely inspecting its progression enables the ophthalmologists to set up appropriate next-visit schedule and cost-effective treatment plans. In the literature, existing work only makes use of numerical attributes in Electronic Medical Records (EMR) for acquiring such kind of DR-oriented knowledge through conventional machine learning techniques, which require an exhaustive job of engineering most impactful risk factors.
Objective
In this paper, an approach of deep bimodal learning is introduced to leverage the performance of DR risk progression identification.
Methods
In particular, we further involve valuable clinical information of fundus photography in addition to the aforementioned systemic attributes. Accordingly, a Trilogy of Skip-connection Deep Networks, namely Tri-SDN, is proposed to exhaustively exploit underlying relationships between the baseline and follow-up information of the fundus images and EMR-based attributes. Besides that, we adopt Skip-Connection Blocks as basis components of the Tri-SDN for making the end-to-end flow of signals more efficient during feedforward and backpropagation processes.
Results
Through a 10-fold cross validation strategy on a private dataset of 96 diabetic mellitus patients, the proposed method attains superior performance over the conventional EMR-modality learning approach in terms of Accuracy (90.6\%), Sensitivity (96.5\%), Precision (88.7\%), Specificity (82.1\%), and Area Under Receiver Operating Characteristics (88.8\%).
Conclusions
The experimental results show that the proposed Tri-SDN can combine features of different modalities (i.e., fundus images and EMR-based numerical risk factors) smoothly and effectively during training and testing processes, respectively. As a consequence, with impressive performance of DR risk progression recognition, the proposed approach is able to help the ophthalmologists properly decide follow-up schedule and subsequent treatment plans.},
	language = {en},
	urldate = {2022-01-27},
	journal = {International Journal of Medical Informatics},
	author = {Hua, Cam-Hao and Huynh-The, Thien and Kim, Kiyoung and Yu, Seung-Young and Le-Tien, Thuong and Park, Gwang Hoon and Bang, Jaehun and Khan, Wajahat Ali and Bae, Sung-Ho and Lee, Sungyoung},
	month = dec,
	year = {2019},
	keywords = {Bimodal learning, Diabetic Retinopathy risk progression, EMR-based attributes, Fundus photography, Retinal fundus image, Trilogy of skip-connection deep networks},
	pages = {103926},
	annote = {???},
	file = {ScienceDirect Snapshot:C\:\\Users\\roman\\Zotero\\storage\\5KWT7D89\\S1386505619303557.html:text/html},
}

@article{gessert_skin_2020,
	title = {Skin lesion classification using ensembles of multi-resolution {EfficientNets} with meta data},
	volume = {7},
	issn = {2215-0161},
	url = {https://www.sciencedirect.com/science/article/pii/S2215016120300832},
	doi = {10.1016/j.mex.2020.100864},
	abstract = {In this paper, we describe our method for the ISIC 2019 Skin Lesion Classification Challenge. The challenge comes with two tasks. For task 1, skin lesions have to be classified based on dermoscopic images. For task 2, dermoscopic images and additional patient meta data are used. Our deep learning-based method achieved first place for both tasks. The are several problems we address with our method. First, there is an unknown class in the test set which we cover with a data-driven approach. Second, there is a severe class imbalance that we address with loss balancing. Third, there are images with different resolutions which motivates two different cropping strategies and multi-crop evaluation. Last, there is patient meta data available which we incorporate with a dense neural network branch. {\textbullet} We address skin lesion classification with an ensemble of deep learning models including EfficientNets, SENet, and ResNeXt WSL, selected by a search strategy. {\textbullet} We rely on multiple model input resolutions and employ two cropping strategies for training. We counter severe class imbalance with a loss balancing approach. {\textbullet} We predict an additional, unknown class with a data-driven approach and we make use of patient meta data with an additional input branch.},
	language = {en},
	urldate = {2022-01-27},
	journal = {MethodsX},
	author = {Gessert, Nils and Nielsen, Maximilian and Shaikh, Mohsin and Werner, Ren{\'e} and Schlaefer, Alexander},
	year = {2020},
	keywords = {Convolutional neural networks, Deep Learning, Multi-class skin lesion classification},
	pages = {100864},
	annote = {start},
	file = {Volltext:C\:\\Users\\roman\\Zotero\\storage\\DF7F6XWC\\Gessert et al. - 2020 - Skin lesion classification using ensembles of mult.pdf:application/pdf},
}

@inproceedings{vale_silva_pan-cancer_2020,
	title = {Pan-{Cancer} {Prognosis} {Prediction} {Using} {Multimodal} {Deep} {Learning}},
	doi = {10.1109/ISBI45749.2020.9098665},
	abstract = {In the age of precision medicine, cancer prognosis assessment from high-dimensional multimodal data requires powerful computational methods. We present an end-to-end multimodal Deep Learning method, named MultiSurv, for automatic patient risk prediction for a large group of 33 cancer types. The method leverages histophatology microscopy slides combined with tabular clinical information and different types of high-throughput sequencing and microarray molecular data. MultiSurv has high predictive performance over all cancer types after training on different combinations of input data modalities and it can handle missing data seamlessly. MultiSurv thus has the potential to integrate the wide variety of available patient data and assist physicians with cancer patient prognosis.},
	booktitle = {2020 {IEEE} 17th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	publisher = {IEEE},
	author = {Vale Silva, Lu{\'i}s A. and Rohr, Karl},
	year = {2020},
	keywords = {Bioinformatics, Cancer, Data models, Feature extraction, Hazards, Machine learning, Molecular and cellular screening, Multi-modality fusion, Prognostics and health management, Training},
	pages = {568--571},
	annote = {start},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\BTFUBVV5\\Vale Silva und Rohr - 2020 - Pan-Cancer Prognosis Prediction Using Multimodal D.pdf:application/pdf},
}

@article{sotiroudis_fusing_2021,
	title = {Fusing {Diverse} {Input} {Modalities} for {Path} {Loss} {Prediction}: {A} {Deep} {Learning} {Approach}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Fusing {Diverse} {Input} {Modalities} for {Path} {Loss} {Prediction}},
	doi = {10.1109/ACCESS.2021.3059589},
	abstract = {Tabular data and images have been used from machine learning models as two diverse types of inputs, in order to perform path loss predictions in urban areas. Different types of models are applied on these distinct modes of input information. The work at hand tries to incorporate both modes of input data within a single prediction model. It therefore manipulates and transforms the vectors of tabular data into images. Each feature of the tabular data vector is spread into several pixels, corresponding to the calculated importance of the particular feature. The resulting synthetic images are then fused with images representing selected regions of the area's map. Compound pseudoimages, having channels of both map-based and tabular data-based images, are then being used as inputs for a Convolutional Neural Network (CNN), which predicts the path loss value at a specific point of the area of interest. The results are clearly better than those obtained from models based on a single mode of input data, as well as from the results produced by other bimodal-input approaches. This approach could be applied for path loss prediction in urban environments for several state-of-art wireless networks like 5G and Internet of Things (IoT).},
	journal = {IEEE Access},
	author = {Sotiroudis, Sotirios P. and Sarigiannidis, Panagiotis and Goudos, Sotirios K. and Siakavara, Katherine},
	year = {2021},
	keywords = {Convolutional neural networks, Data models, Feature extraction, Machine learning, Buildings, data to image transformation, deep learning, path loss, Predictive models, pseudoimages, radio propagation, Receivers, Transmitters},
	pages = {30441--30451},
	annote = {start},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\I4RKDE7G\\9354618.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\ZGPMHKZC\\Sotiroudis et al. - 2021 - Fusing Diverse Input Modalities for Path Loss Pred.pdf:application/pdf},
}

@article{cheerla_deep_2019,
	title = {Deep learning with multimodal representation for pancancer prognosis prediction},
	volume = {35},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btz342},
	doi = {10.1093/bioinformatics/btz342},
	abstract = {Estimating the future course of patients with cancer lesions is invaluable to physicians; however, current clinical methods fail to effectively use the vast amount of multimodal data that is available for cancer patients. To tackle this problem, we constructed a multimodal neural network-based model to predict the survival of patients for 20 different cancer types using clinical data, mRNA expression data, microRNA expression data and histopathology whole slide images (WSIs). We developed an unsupervised encoder to compress these four data modalities into a single feature vector for each patient, handling missing data through a resilient, multimodal dropout method. Encoding methods were tailored to each data type{\textemdash}using deep highway networks to extract features from clinical and genomic data, and convolutional neural networks to extract features from WSIs.We used pancancer data to train these feature encodings and predict single cancer and pancancer overall survival, achieving a C-index of 0.78 overall. This work shows that it is possible to build a pancancer model for prognosis that also predicts prognosis in single cancer sites. Furthermore, our model handles multiple data modalities, efficiently analyzes WSIs and represents patient multimodal data flexibly into an unsupervised, informative representation. We thus present a powerful automated tool to accurately determine prognosis, a key step towards personalized treatment for cancer patients.https://github.com/gevaertlab/MultimodalPrognosis},
	number = {14},
	urldate = {2022-01-27},
	journal = {Bioinformatics},
	author = {Cheerla, Anika and Gevaert, Olivier},
	month = jul,
	year = {2019},
	pages = {i446--i454},
	annote = {cited by "pan-cancer prognosis prediction using multinomial deep learning"},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\S9D6J68R\\Cheerla und Gevaert - 2019 - Deep learning with multimodal representation for p.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\U3F28YL7\\5529139.html:text/html},
}

@article{mobadersany_predicting_2018,
	title = {Predicting cancer outcomes from histology and genomics using convolutional networks},
	volume = {115},
	copyright = {Copyright {\textcopyright} 2018 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/13/E2970},
	doi = {10.1073/pnas.1717139115},
	abstract = {Cancer histology reflects underlying molecular processes and disease progression and contains rich phenotypic information that is predictive of patient outcomes. In this study, we show a computational approach for learning patient outcomes from digital pathology images using deep learning to combine the power of adaptive machine learning algorithms with traditional survival models. We illustrate how these survival convolutional neural networks (SCNNs) can integrate information from both histology images and genomic biomarkers into a single unified framework to predict time-to-event outcomes and show prediction accuracy that surpasses the current clinical paradigm for predicting the overall survival of patients diagnosed with glioma. We use statistical sampling techniques to address challenges in learning survival from histology images, including tumor heterogeneity and the need for large training cohorts. We also provide insights into the prediction mechanisms of SCNNs, using heat map visualization to show that SCNNs recognize important structures, like microvascular proliferation, that are related to prognosis and that are used by pathologists in grading. These results highlight the emerging role of deep learning in precision medicine and suggest an expanding utility for computational analysis of histology in the future practice of pathology.},
	language = {en},
	number = {13},
	urldate = {2022-01-27},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mobadersany, Pooya and Yousefi, Safoora and Amgad, Mohamed and Gutman, David A. and Barnholtz-Sloan, Jill S. and Vega, Jos{\'e} E. Vel{\'a}zquez and Brat, Daniel J. and Cooper, Lee A. D.},
	month = mar,
	year = {2018},
	pmid = {29531073},
	note = {Publisher: National Academy of Sciences
Section: PNAS Plus},
	keywords = {deep learning, artificial intelligence, cancer, digital pathology, machine learning},
	pages = {E2970--E2979},
	annote = {cited by "pan-cancer prognosis prediction using multinomial deep learning"},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\XHR69CXD\\Mobadersany et al. - 2018 - Predicting cancer outcomes from histology and geno.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\9ALP5KZB\\E2970.html:text/html},
}

@inproceedings{subramanian_multimodal_2021,
	title = {Multimodal {Fusion} {Using} {Sparse} {Cca} {For} {Breast} {Cancer} {Survival} {Prediction}},
	doi = {10.1109/ISBI48211.2021.9434033},
	abstract = {Effective understanding of a disease such as cancer requires fusing multiple sources of information captured across physical scales by multimodal data. In this work, we propose a novel feature embedding module that derives from canonical correlation analyses to account for intra-modality and inter-modality correlations. Experiments on simulated and real data demonstrate how our proposed module can learn well-correlated multi-dimensional embeddings. These embeddings perform competitively on one-year survival classification of TCGA-BRCA breast cancer patients, yielding average F1 scores up to 58.69\% under 5-fold cross-validation.},
	booktitle = {2021 {IEEE} 18th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Subramanian, Vaishnavi and Syeda-Mahmood, Tanveer and Do, Minh N.},
	month = apr,
	year = {2021},
	note = {ISSN: 1945-8452},
	keywords = {Biomedical imaging, Breast cancer, Correlation},
	pages = {1429--1432},
	annote = {citing "pan-cancer prognosis prediction using multinomial deep learning"},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\ZBIWQFTV\\9434033.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\KFN22MKD\\Subramanian et al. - 2021 - Multimodal Fusion Using Sparse Cca For Breast Canc.pdf:application/pdf},
}

@article{carrillo-perez_non-small-cell_2021,
	title = {Non-small-cell lung cancer classification via {RNA}-{Seq} and histology imaging probability fusion},
	volume = {22},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-021-04376-1},
	doi = {10.1186/s12859-021-04376-1},
	abstract = {Adenocarcinoma and squamous cell carcinoma are the two most prevalent lung cancer types, and their distinction requires different screenings, such as the visual inspection of histology slides by an expert pathologist, the analysis of gene expression or computer tomography scans, among others. In recent years, there has been an increasing gathering of biological data for decision support systems in the diagnosis (e.g. histology imaging, next-generation sequencing technologies data, clinical information, etc.). Using all these sources to design integrative classification approaches may improve the final diagnosis of a patient, in the same way that doctors can use multiple types of screenings to reach a final decision on the diagnosis. In this work, we present a late fusion classification model using histology and RNA-Seq data for adenocarcinoma, squamous-cell carcinoma and healthy lung tissue.},
	number = {1},
	urldate = {2022-01-27},
	journal = {BMC Bioinformatics},
	author = {Carrillo-Perez, Francisco and Morales, Juan Carlos and Castillo-Secilla, Daniel and Molina-Castro, Y{\'e}sica and Guill{\'e}n, Alberto and Rojas, Ignacio and Herrera, Luis Javier},
	month = sep,
	year = {2021},
	keywords = {Deep learning, Gene expression, Late fusion, NSCLC, Whole slide imaging},
	pages = {454},
	annote = {citing "pan-cancer prognosis prediction using multinomial deep learning"},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\3Y52JEHN\\Carrillo-Perez et al. - 2021 - Non-small-cell lung cancer classification via RNA-.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\KDFUKLTZ\\s12859-021-04376-1.html:text/html},
}

@article{huang_fusion_2020,
	title = {Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines},
	volume = {3},
	copyright = {2020 The Author(s)},
	issn = {2398-6352},
	shorttitle = {Fusion of medical imaging and electronic health records using deep learning},
	url = {https://www.nature.com/articles/s41746-020-00341-z},
	doi = {10.1038/s41746-020-00341-z},
	abstract = {Advancements in deep learning techniques carry the potential to make significant contributions to healthcare, particularly in fields that utilize medical imaging for diagnosis, prognosis, and treatment decisions. The current state-of-the-art deep learning models for radiology applications consider only pixel-value information without data informing clinical context. Yet in practice, pertinent and accurate non-imaging data based on the clinical history and laboratory data enable physicians to interpret imaging findings in the appropriate clinical context, leading to a higher diagnostic accuracy, informative clinical decision making, and improved patient outcomes. To achieve a similar goal using deep learning, medical imaging pixel-based models must also achieve the capability to process contextual data from electronic health records (EHR) in addition to pixel data. In this paper, we describe different data fusion techniques that can be applied to combine medical imaging with EHR, and systematically review medical data fusion literature published between 2012 and 2020. We conducted a systematic search on PubMed and Scopus for original research articles leveraging deep learning for fusion of multimodality data. In total, we screened 985 studies and extracted data from 17 papers. By means of this systematic review, we present current knowledge, summarize important results and provide implementation guidelines to serve as a reference for researchers interested in the application of multimodal fusion in medical imaging.},
	language = {en},
	number = {1},
	urldate = {2022-01-27},
	journal = {npj Digital Medicine},
	author = {Huang, Shih-Cheng and Pareek, Anuj and Seyyedi, Saeed and Banerjee, Imon and Lungren, Matthew P.},
	month = oct,
	year = {2020},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Data integration;Machine learning;Medical imaging
Subject\_term\_id: data-integration;machine-learning;medical-imaging},
	keywords = {Machine learning, Data integration, Medical imaging},
	pages = {1--9},
	annote = {cited by "Non-small-cell lung cancer classification via RNA-Seq and histology imaging probability fusion"},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\B2U3HW5E\\Huang et al. - 2020 - Fusion of medical imaging and electronic health re.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\E3TBP5QQ\\s41746-020-00341-z.html:text/html},
}

@article{borisov_deep_2021,
	title = {Deep {Neural} {Networks} and {Tabular} {Data}: {A} {Survey}},
	volume = {abs/2110.01889},
	url = {https://arxiv.org/abs/2110.01889},
	journal = {CoRR},
	author = {Borisov, Vadim and Leemann, Tobias and Se{\ss}ler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
	year = {2021},
	annote = {start},
}

@article{lee_mildint_2019,
	title = {{MildInt}: {Deep} {Learning}-{Based} {Multimodal} {Longitudinal} {Data} {Integration} {Framework}},
	volume = {10},
	issn = {1664-8021},
	shorttitle = {{MildInt}},
	url = {https://www.frontiersin.org/article/10.3389/fgene.2019.00617},
	abstract = {As large amounts of heterogeneous biomedical data become available, numerous methods for integrating such datasets have been developed to extract complementary knowledge from multiple domains of sources. Recently, a deep learning approach has shown promising results in a variety of research areas. However, applying the deep learning approach requires expertise for constructing a deep architecture that can take multimodal longitudinal data. Thus, in this paper, a deep learning-based python package for data integration is developed. The python package deep learning-based multimodal longitudinal data integration framework (MildInt) provides the preconstructed deep learning architecture for a classification task. MildInt contains two learning phases: learning feature representation from each modality of data and training a classifier for the final decision. Adopting deep architecture in the first phase leads to learning more task-relevant feature representation than a linear model. In the second phase, linear regression classifier is used for detecting and investigating biomarkers from multimodal data. Thus, by combining the linear model and the deep learning model, higher accuracy and better interpretability can be achieved. We validated the performance of our package using simulation data and real data. For the real data, as a pilot study, we used clinical and multimodal neuroimaging datasets in Alzheimer{\textquoteright}s disease to predict the disease progression. MildInt is capable of integrating multiple forms of numerical data including time series and non-time series data for extracting complementary features from the multimodal dataset.},
	urldate = {2022-02-07},
	journal = {Frontiers in Genetics},
	author = {Lee, Garam and Kang, Byungkon and Nho, Kwangsik and Sohn, Kyung-Ah and Kim, Dokyoon},
	year = {2019},
	annote = {cited by Non-small-cell lung cancer classification via RNA-Seq and histology imaging probability fusion},
	annote = {more about a python package able to use different types of data individually, but not about fusing them together it seems},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\CDSF77MX\\Lee et al. - 2019 - MildInt Deep Learning-Based Multimodal Longitudin.pdf:application/pdf},
}

@article{chen_pathomic_2020,
	title = {Pathomic {Fusion}: {An} {Integrated} {Framework} for {Fusing} {Histopathology} and {Genomic} {Features} for {Cancer} {Diagnosis} and {Prognosis}},
	issn = {1558-254X},
	shorttitle = {Pathomic {Fusion}},
	doi = {10.1109/TMI.2020.3021387},
	abstract = {Cancer diagnosis, prognosis, and therapeutic response predictions are based on morphological information from histology slides and molecular profiles from genomic data. However, most deep learning-based objective outcome prediction and grading paradigms are based on histology or genomics alone and do not make use of the complementary information in an intuitive manner. In this work, we propose Pathomic Fusion, an interpretable strategy for end-to-end multimodal fusion of histology image and genomic (mutations, CNV, RNASeq) features for survival outcome prediction. Our approach models pairwise feature interactions across modalities by taking the Kronecker product of unimodal feature representations, and controls the expressiveness of each representation via a gatingbased attention mechanism. Following supervised learning, we are able to interpret and saliently localize features across each modality, and understand how feature importance shifts when conditioning on multimodal input. We validate our approach using glioma and clear cell renal cell carcinoma datasets from the Cancer Genome Atlas (TCGA), which contains paired wholeslide image, genotype, and transcriptome data with ground truth survival and histologic grade labels. In a 15-fold cross-validation, our results demonstrate that the proposed multimodal fusion paradigm improves prognostic determinations from ground truth grading and molecular subtyping, as well as unimodal deep networks trained on histology and genomic data alone. The proposed method establishes insight and theory on how to train deep networks on multimodal biomedical data in an intuitive manner, which will be useful for other problems in medicine that seek to combine heterogeneous data streams for understanding diseases and predicting response and resistance to treatment. Code and trained models are made available at: https://github.com/mahmoodlab/PathomicFusion.},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Chen, Richard J. and Lu, Ming Y. and Wang, Jingwen and Williamson, Drew F. K. and Rodig, Scott J. and Lindeman, Neal I. and Mahmood, Faisal},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Bioinformatics, Cancer, Feature extraction, Machine learning, Genomics, Graph Convolutional Networks, Microprocessors, Multimodal Learning, Survival Analysis, Tumors},
	pages = {1--1},
	annote = {citing Deep learning with multimodal representation for pancancer prognosis prediction},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\HUIDUPWN\\9186053.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\3C7HV5TQ\\Chen et al. - 2020 - Pathomic Fusion An Integrated Framework for Fusin.pdf:application/pdf},
}

@article{ning_integrative_2020,
	title = {Integrative analysis of cross-modal features for the prognosis prediction of clear cell renal cell carcinoma},
	volume = {36},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btaa056},
	doi = {10.1093/bioinformatics/btaa056},
	abstract = {As a highly heterogeneous disease, clear cell renal cell carcinoma (ccRCC) has quite variable clinical behaviors. The prognostic biomarkers play a crucial role in stratifying patients suffering from ccRCC to avoid over- and under-treatment. Researches based on hand-crafted features and single-modal data have been widely conducted to predict the prognosis of ccRCC. However, these experience-dependent methods, neglecting the synergy among multimodal data, have limited capacity to perform accurate prediction. Inspired by complementary information among multimodal data and the successful application of convolutional neural networks (CNNs) in medical image analysis, a novel framework was proposed to improve prediction performance.We proposed a cross-modal feature-based integrative framework, in which deep features extracted from computed tomography/histopathological images by using CNNs were combined with eigengenes generated from functional genomic data, to construct a prognostic model for ccRCC. Results showed that our proposed model can stratify high- and low-risk subgroups with significant difference (P-value \&lt; 0.05) and outperform the predictive performance of those models based on single-modality features in the independent testing cohort [C-index, 0.808 (0.728{\textendash}0.888)]. In addition, we also explored the relationship between deep image features and eigengenes, and make an attempt to explain deep image features from the view of genomic data. Notably, the integrative framework is available to the task of prognosis prediction of other cancer with matched multimodal data.https://github.com/zhang-de-lab/zhang-lab? from=singlemessageSupplementary data are available at Bioinformatics online.},
	number = {9},
	urldate = {2022-02-07},
	journal = {Bioinformatics},
	author = {Ning, Zhenyuan and Pan, Weihao and Chen, Yuting and Xiao, Qing and Zhang, Xinsen and Luo, Jiaxiu and Wang, Jian and Zhang, Yu},
	month = may,
	year = {2020},
	pages = {2888--2895},
	annote = {citing Deep learning with multimodal representation for pancancer prognosis prediction},
	annote = {not sure if genomic data is in tabular form or something different},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\U5DMMQTN\\Ning et al. - 2020 - Integrative analysis of cross-modal features for t.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\HIN94EHF\\5716325.html:text/html},
}

@article{wang_gpdbn_2021,
	title = {{GPDBN}: deep bilinear network integrating both genomic data and pathological images for breast cancer prognosis prediction},
	volume = {37},
	issn = {1367-4803},
	shorttitle = {{GPDBN}},
	url = {https://doi.org/10.1093/bioinformatics/btab185},
	doi = {10.1093/bioinformatics/btab185},
	abstract = {Breast cancer is a very heterogeneous disease and there is an urgent need to design computational methods that can accurately predict the prognosis of breast cancer for appropriate therapeutic regime. Recently, deep learning-based methods have achieved great success in prognosis prediction, but many of them directly combine features from different modalities that may ignore the complex inter-modality relations. In addition, existing deep learning-based methods do not take intra-modality relations into consideration that are also beneficial to prognosis prediction. Therefore, it is of great importance to develop a deep learning-based method that can take advantage of the complementary information between intra-modality and inter-modality by integrating data from different modalities for more accurate prognosis prediction of breast cancer.We present a novel unified framework named genomic and pathological deep bilinear network (GPDBN) for prognosis prediction of breast cancer by effectively integrating both genomic data and pathological images. In GPDBN, an inter-modality bilinear feature encoding module is proposed to model complex inter-modality relations for fully exploiting intrinsic relationship of the features across different modalities. Meanwhile, intra-modality relations that are also beneficial to prognosis prediction, are captured by two intra-modality bilinear feature encoding modules. Moreover, to take advantage of the complementary information between inter-modality and intra-modality relations, GPDBN further combines the inter- and intra-modality bilinear features by using a multi-layer deep neural network for final prognosis prediction. Comprehensive experiment results demonstrate that the proposed GPDBN significantly improves the performance of breast cancer prognosis prediction and compares favorably with existing methods.GPDBN is freely available at https://github.com/isfj/GPDBN.Supplementary data are available at Bioinformatics online.},
	number = {18},
	urldate = {2022-02-07},
	journal = {Bioinformatics},
	author = {Wang, Zhiqin and Li, Ruiqing and Wang, Minghui and Li, Ao},
	month = sep,
	year = {2021},
	pages = {2963--2970},
	annote = {citing Deep learning with multimodal representation for pancancer prognosis prediction},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\QVTJC3YK\\Wang et al. - 2021 - GPDBN deep bilinear network integrating both geno.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\BX9W86LA\\6178291.html:text/html},
}

@inproceedings{braman_deep_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Deep {Orthogonal} {Fusion}: {Multimodal} {Prognostic} {Biomarker} {Discovery} {Integrating} {Radiology}, {Pathology}, {Genomic}, and {Clinical} {Data}},
	isbn = {978-3-030-87240-3},
	shorttitle = {Deep {Orthogonal} {Fusion}},
	doi = {10.1007/978-3-030-87240-3_64},
	abstract = {Clinical decision-making in oncology involves multimodal data such as radiology scans, molecular profiling, histopathology slides, and clinical factors. Despite the importance of these modalities individually, no deep learning framework to date has combined them all to predict patient prognosis. Here, we predict the overall survival (OS) of glioma patients from diverse multimodal data with a Deep Orthogonal Fusion (DOF) model. The model learns to combine information from multiparametric MRI exams, biopsy-based modalities (such as H\&E slide images and/or DNA sequencing), and clinical variables into a comprehensive multimodal risk score. Prognostic embeddings from each modality are learned and combined via attention-gated tensor fusion. To maximize the information gleaned from each modality, we introduce a multimodal orthogonalization (MMO) loss term that increases model performance by incentivizing constituent embeddings to be more complementary. DOF predicts OS in glioma patients with a median C-index of 0.788 {\textpm} 0.067, significantly outperforming (p = 0.023) the best performing unimodal model with a median C-index of 0.718 {\textpm} 0.064. The prognostic model significantly stratifies glioma patients by OS within clinical subsets, adding further granularity to prognostic clinical grading and molecular subtyping.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} {\textendash} {MICCAI} 2021},
	publisher = {Springer International Publishing},
	author = {Braman, Nathaniel and Gordon, Jacob W. H. and Goossens, Emery T. and Willis, Caleb and Stumpe, Martin C. and Venkataraman, Jagadish},
	editor = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
	year = {2021},
	pages = {667--677},
	annote = {citing Deep learning with multimodal representation for pancancer prognosis prediction},
	file = {Springer Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\EILVBTFT\\Braman et al. - 2021 - Deep Orthogonal Fusion Multimodal Prognostic Biom.pdf:application/pdf},
}

@inproceedings{morales_enhancing_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Enhancing {Breast} {Cancer} {Classification} via {Information} and {Multi}-model {Integration}},
	isbn = {978-3-030-45385-5},
	doi = {10.1007/978-3-030-45385-5_67},
	abstract = {The integration of different sources of information for proper classification is of utter importance, specially in the biomedical field. Many different sources of information can be collected from a patient and they all may contribute to an accurate diagnosis. For example in cancer disease these can include gene expression (RNA-Seq) or Tissue Slide Imaging, however, their integration in order to correctly train a classification model is not straightforward. Making use of Whole-Slide-Images, this work presents a novel information integration model when different sources of data from a patient are available, named as Multi-source integration model (MSIM). Using two different Convolutional Neural Networks architectures and a Feed Forward Neural Network, the potential of a multi-model integration process which combines the information of different sources is introduced and its results are presented for Breast Cancer classification.},
	language = {en},
	booktitle = {Bioinformatics and {Biomedical} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Morales, J. C. and Carrillo-Perez, Francisco and Castillo-Secilla, Daniel and Rojas, Ignacio and Herrera, Luis Javier},
	editor = {Rojas, Ignacio and Valenzuela, Olga and Rojas, Fernando and Herrera, Luis Javier and Ortu{\~n}o, Francisco},
	year = {2020},
	keywords = {Breast cancer, CNNs, WSI},
	pages = {750--760},
	annote = {citing Deep learning with multimodal representation for pancancer prognosis prediction},
	file = {Springer Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\HB6INWUF\\Morales et al. - 2020 - Enhancing Breast Cancer Classification via Informa.pdf:application/pdf},
}

@incollection{menegotto_multimodal_2021,
	address = {Cham},
	series = {Studies in {Fuzziness} and {Soft} {Computing}},
	title = {Multimodal {Deep} {Learning} for {Computer}-{Aided} {Detection} and {Diagnosis} of {Cancer}: {Theory} and {Applications}},
	isbn = {978-3-030-70111-6},
	shorttitle = {Multimodal {Deep} {Learning} for {Computer}-{Aided} {Detection} and {Diagnosis} of {Cancer}},
	url = {https://doi.org/10.1007/978-3-030-70111-6_13},
	abstract = {Cancer is a group of diseases caused by the abnormal and disorderly growth of cells, representing the second leading cause of deaths worldwide. The number of cancer cases is growing yearly, medical systems are an essential tool to speed up the diagnosis process and increase patient survival probabilities. Electronic health record systems store the patient{\textquoteright}s health data, which can be of structured and unstructured types. Physicians use all the information available in these systems during a cancer diagnostic, regardless of its sort and modality. Deep learning is a machine learning sub-field that has algorithms able to process data end-to-end using deep architectures, often inspired by the brain{\textquoteright}s synaptic model. Using more than one data source in these architectures is known as multimodal deep learning. Computer-aided detection and diagnosis systems developed using multimodal deep learning algorithms have been achieving promising results in diagnostic performance, which are often comparable to human specialists. E-health and telemedicine applications can be boosted with these high-performance detection and diagnosis systems, which can provide real-time analysis capabilities to the healthcare staff and improve the quality of the medical services. This chapter explores the theory and the applications of multimodal deep learning techniques to develop computer-aided detection and diagnosis systems for cancer, describing examples of the state-of-the-art in this area, looking for essential and innovative aspects of these systems. Heterogeneous and hybrid fusion strategies combining high-quality imaging, clinical attributes and genetic data in deep multimodal architectures are resulting in computer-aided systems for cancer with promising diagnosis performance.},
	language = {en},
	urldate = {2022-02-07},
	booktitle = {Enhanced {Telemedicine} and e-{Health}: {Advanced} {IoT} {Enabled} {Soft} {Computing} {Framework}},
	publisher = {Springer International Publishing},
	author = {Menegotto, Alan Baronio and Cazella, Silvio Cesar},
	editor = {Marques, Gon{\c c}alo and Kumar Bhoi, Akash and de la Torre D{\'i}ez, Isabel and Garcia-Zapirain, Begonya},
	year = {2021},
	doi = {10.1007/978-3-030-70111-6_13},
	keywords = {Computer-aided diagnosis, Multimodal deep learning, Data fusion, e-Health, Intelligent systems},
	pages = {267--287},
	annote = {citing Deep learning with multimodal representation for pancancer prognosis prediction},
}

@article{stahlschmidt_multimodal_2022,
	title = {Multimodal deep learning for biomedical data fusion : a review},
	shorttitle = {Multimodal deep learning for biomedical data fusion},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:his:diva-20873},
	abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
	language = {eng},
	urldate = {2022-02-07},
	journal = {Briefings in Bioinformatics},
	author = {Stahlschmidt, S{\"o}ren Richard and Ulfenborg, Benjamin and Synnergren, Jane},
	year = {2022},
	note = {Publisher: Oxford University Press},
	annote = {citing Deep learning with multimodal representation for pancancer prognosis prediction},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\BWKM8R4J\\Stahlschmidt et al. - 2022 - Multimodal deep learning for biomedical data fusio.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\42YG23EQ\\record.html:text/html},
}

@inproceedings{lv_pg-tfnet_2021,
	title = {{PG}-{TFNet}: {Transformer}-based {Fusion} {Network} {Integrating} {Pathological} {Images} and {Genomic} {Data} for {Cancer} {Survival} {Analysis}},
	shorttitle = {{PG}-{TFNet}},
	doi = {10.1109/BIBM52615.2021.9669445},
	abstract = {Survival analysis is crucial to the evaluation of cancer treatment options and deep learning-based methods integrating pathological images and genomic data have been used for prognosis prediction. However, the most methods are based on the analysis of pathological image patches, thus ignoring the morphological structure information at larger field-of-view and intrinsic relationships between patches. Meanwhile, the existing models fail to exploit the powerful representation learning capabilities of the neural networks for effective multimodal feature fusion of pathological images and genomic data. In this paper, we propose a novel transformer-based fusion network integrating pathological images and genomic data (PGTFNet) for cancer survival analysis. Specifically, we present a transformer-based feature fusion module for multi-scale pathological slides to fully exploit the intra-modality relationships between image patches at various fields of view. Moreover, in order to make effective inter-modality feature fusion of pathological images and genomic data, we introduce a cross-attention transformer module that can exchange feature representations of different modalities between two transformers branches. The PG-TFNet is performed on the colorectal cancer dataset from the Cancer Genome Atlas (TCGA), which contains paired whole-slide images and genomic data with ground truth survival data. The experimental results from a 10-fold cross validation demonstrate that the proposed PG-TFNet facilitates the prognosis prediction of colorectal cancer and shows superiority over the existing methods.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Lv, Zhilong and Lin, Yuexiao and Yan, Rui and Yang, Zhenghe and Wang, Ying and Zhang, Fa},
	month = dec,
	year = {2021},
	keywords = {Data models, Genomics, Multimodal Learning, Survival Analysis, Genomic Data, Learning systems, Neural networks, Pathological Images, Pathology, Representation learning, Transformer, Transformers},
	pages = {491--496},
	annote = {citing Deep learning with multimodal representation for pancancer prognosis prediction},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\JHXG7SXU\\9669445.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\XX3ZCAMH\\Lv et al. - 2021 - PG-TFNet Transformer-based Fusion Network Integra.pdf:application/pdf},
}

@article{schneider_integration_2022,
	title = {Integration of deep learning-based image analysis and genomic data in cancer pathology: {A} systematic review},
	volume = {160},
	issn = {0959-8049},
	shorttitle = {Integration of deep learning-based image analysis and genomic data in cancer pathology},
	url = {https://www.sciencedirect.com/science/article/pii/S0959804921011606},
	doi = {10.1016/j.ejca.2021.10.007},
	abstract = {Background
Over the past decade, the development of molecular high-throughput methods (omics) increased rapidly and provided new insights for cancer research. In parallel, deep learning approaches revealed the enormous potential for medical image analysis, especially in digital pathology. Combining image and omics data with deep learning tools may enable the discovery of new cancer biomarkers and a more precise prediction of patient prognosis. This systematic review addresses different multimodal fusion methods of convolutional neural network-based image analyses with omics data, focussing on the impact of data combination on the classification performance.
Methods
PubMed was screened for peer-reviewed articles published in English between January 2015 and June 2021 by two independent researchers. Search terms related to deep learning, digital pathology, omics, and multimodal fusion were combined.
Results
We identified a total of 11 studies meeting the inclusion criteria, namely studies that used convolutional neural networks for haematoxylin and eosin~image analysis of patients with cancer in combination with integrated omics data. Publications were categorised according to their endpoints: 7 studies focused on survival analysis and 4 studies on prediction of cancer subtypes, malignancy or microsatellite instability with spatial analysis.
Conclusions
Image-based classifiers already show high performances in prognostic and predictive cancer diagnostics. The integration of omics data led to improved performance in all studies described here. However, these are very early studies that still require external validation to demonstrate their generalisability and robustness. Further and more comprehensive studies with larger sample sizes are needed to evaluate performance and determine clinical benefits.},
	language = {en},
	urldate = {2022-02-07},
	journal = {European Journal of Cancer},
	author = {Schneider, Lucas and Laiouar-Pedari, Sara and Kuntz, Sara and Krieghoff-Henning, Eva and Hekler, Achim and Kather, Jakob N. and Gaiser, Timo and Fr{\"o}hling, Stefan and Brinker, Titus J.},
	month = jan,
	year = {2022},
	keywords = {Convolutional neural networks, Cancer, Biomarker identification, Multimodal fusion, Omics},
	pages = {80--91},
	annote = {citing Deep learning with multimodal representation for pancancer prognosis prediction},
	file = {ScienceDirect Snapshot:C\:\\Users\\roman\\Zotero\\storage\\J2QTMH83\\S0959804921011606.html:text/html;Volltext:C\:\\Users\\roman\\Zotero\\storage\\VNAZ2AP3\\Schneider et al. - 2022 - Integration of deep learning-based image analysis .pdf:application/pdf},
}

@techreport{azher_development_2021,
	title = {Development of {Biologically} {Interpretable} {Multimodal} {Deep} {Learning} {Model} for {Cancer} {Prognosis} {Prediction}},
	copyright = {{\textcopyright} 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2021.10.30.466610v1},
	abstract = {Robust cancer prognostication can enable more effective patient care and management, which may potentially improve health outcomes. Deep learning has proven to be a powerful tool to extract meaningful information from cancer patient data. In recent years it has displayed promise in quantifying prognostication by predicting patient risk. However, most current deep learning-based cancer prognosis prediction methods use only a single data source and miss out on learning from potentially rich relationships across modalities. Existing multimodal approaches are challenging to interpret in a biological or medical context, limiting real-world clinical integration as a trustworthy prognostic decision aid. Here, we developed a multimodal modeling approach that can integrate information from the central modalities of gene expression, DNA methylation, and histopathological imaging with clinical information for cancer prognosis prediction. Our multimodal modeling approach combines pathway and gene-based sparsely coded layers with patch-based graph convolutional networks to facilitate biological interpretation of the model results. We present a preliminary analysis that compares the potential applicability of combining all modalities to uni- or bi-modal approaches. Leveraging data from four cancer subtypes from the Cancer Genome Atlas, results demonstrate the encouraging performance of our multimodal approach (C-index=0.660 without clinical features; C-index=0.665 with clinical features) across four cancer subtypes versus unimodal approaches and existing state-of-the-art approaches. This work brings insight to the development of interpretable multimodal methods of applying AI to biomedical data and can potentially serve as a foundation for clinical implementations of such software. We plan to follow up this preliminary analysis with an in-depth exploration of factors to improve multimodal modeling approaches on an in-house dataset.},
	language = {en},
	urldate = {2022-02-07},
	institution = {bioRxiv},
	author = {Azher, Zarif L. and Vaickus, Louis J. and Salas, Lucas A. and Christensen, Brock C. and Levy, Joshua J.},
	month = nov,
	year = {2021},
	doi = {10.1101/2021.10.30.466610},
	note = {Section: New Results
Type: article},
	pages = {2021.10.30.466610},
	annote = {citing Deep learning with multimodal representation for pancancer prognosis prediction},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\DQ62A3TF\\Azher et al. - 2021 - Development of Biologically Interpretable Multimod.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\XDBI75QQ\\2021.10.30.466610v1.html:text/html},
}

@article{huang_review_2022,
	title = {A review of fusion methods for omics and imaging data},
	issn = {1557-9964},
	doi = {10.1109/TCBB.2022.3143900},
	abstract = {The development of omics data and biomedical images has greatly advanced the progress of precision medicine in diagnosis, treatment, and prognosis. The fusion of omics and imaging data, i.e., omics-imaging fusion, offers a new strategy for understanding complex diseases. However, due to a variety of issues such as the limited number of samples, high dimensionality of features, and heterogeneity of different data types, efficiently learning complementary or associated discriminative fusion information from omics and imaging data remains a challenge. Recently, numerous machine learning methods have been proposed to alleviate these problems. In this review, from the perspective of fusion levels and fusion methods, we first provide an overview of preprocessing and feature extraction methods for omics and imaging data, and comprehensively analyze and summarize the basic forms and variations of commonly used and newly emerging fusion methods, along with their advantages, disadvantages and the applicable scope. We then describe public datasets and compare experimental results of various fusion methods on the ADNI and TCGA datasets. Finally, we discuss future prospects and highlight remaining challenges in the field.},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	author = {Huang, Weixian and Tan, Kaiwen and Hu, Jinlong and Zhang, Ziye and Dong, Shoubin},
	year = {2022},
	note = {Conference Name: IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	keywords = {Bioinformatics, Prognostics and health management, Genomics, Multimodal fusion, Brain, Imaging, Imaging-omics fusion, Precision medicine, Proteins, Radiogenomics, Task analysis},
	pages = {1--1},
	annote = {citing Deep learning with multimodal representation for pancancer prognosis prediction},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\EDDHWKX3\\9684949.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\SXHL29DE\\Huang et al. - 2022 - A review of fusion methods for omics and imaging d.pdf:application/pdf},
}

@article{ning_multi-constraint_2021,
	title = {Multi-{Constraint} {Latent} {Representation} {Learning} for {Prognosis} {Analysis} {Using} {Multi}-{Modal} {Data}},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2021.3112194},
	abstract = {The Cox proportional hazard model has been widely applied to cancer prognosis prediction. Nowadays, multi-modal data, such as histopathological images and gene data, have advanced this field by providing histologic phenotype and genotype information. However, how to efficiently fuse and select the complementary information of high-dimensional multi-modal data remains challenging for Cox model, as it generally does not equip with feature fusion/selection mechanism. Many previous studies typically perform feature fusion/selection in the original feature space before Cox modeling. Alternatively, learning a latent shared feature space that is tailored for Cox model and simultaneously keeps sparsity is desirable. In addition, existing Cox-based models commonly pay little attention to the actual length of the observed time that may help to boost the model's performance. In this article, we propose a novel Cox-driven multi-constraint latent representation learning framework for prognosis analysis with multi-modal data. Specifically, for efficient feature fusion, a multi-modal latent space is learned via a bi-mapping approach under ranking and regression constraints. The ranking constraint utilizes the log-partial likelihood of Cox model to induce learning discriminative representations in a task-oriented manner. Meanwhile, the representations also benefit from regression constraint, which imposes the supervision of specific survival time on representation learning. To improve generalization and alleviate overfitting, we further introduce similarity and sparsity constraints to encourage extra consistency and sparseness. Extensive experiments on three datasets acquired from The Cancer Genome Atlas (TCGA) demonstrate that the proposed method is superior to state-of-the-art Cox-based models.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Ning, Zhenyuan and Lin, Zehui and Xiao, Qing and Du, Denghui and Feng, Qianjin and Chen, Wufan and Zhang, Yu},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Cancer, Data models, Feature extraction, Prognostics and health management, Predictive models, Biological system modeling, Cox, human cancers, latent representation learning, Medical diagnostic imaging, multi-constraint, prognosis analysis},
	pages = {1--14},
	annote = {citing Deep learning with multimodal representation for pancancer prognosis prediction},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\UA8FP3RM\\9556512.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\6AWFA7YG\\Ning et al. - 2021 - Multi-Constraint Latent Representation Learning fo.pdf:application/pdf},
}

@article{lu_integrating_2021,
	title = {Integrating pathomics with radiomics and genomics for cancer prognosis: {A} brief review},
	volume = {33},
	issn = {1000-9604},
	shorttitle = {Integrating pathomics with radiomics and genomics for cancer prognosis},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8580801/},
	doi = {10.21147/j.issn.1000-9604.2021.05.03},
	abstract = {In the last decade, the focus of computational pathology research community has shifted from replicating the pathological examination for diagnosis done by pathologists to unlocking and discovering {\textquotedblleft}sub-visual{\textquotedblright} prognostic image cues from the histopathological image. While we are getting more knowledge and experience in digital pathology, the emerging goal is to integrate other-omics or modalities that will contribute for building a better prognostic assay. In this paper, we provide a brief review of representative works that focus on integrating pathomics with radiomics and genomics for cancer prognosis. It includes: correlation of pathomics and genomics; fusion of pathomics and genomics; fusion of pathomics and radiomics. We also present challenges, potential opportunities, and avenues for future work.},
	number = {5},
	urldate = {2022-02-07},
	journal = {Chinese Journal of Cancer Research},
	author = {Lu, Cheng and Shiradkar, Rakesh and Liu, Zaiyi},
	month = oct,
	year = {2021},
	pmid = {34815630},
	pmcid = {PMC8580801},
	pages = {563--573},
	annote = {citing Deep learning with multimodal representation for pancancer prognosis prediction},
	file = {PubMed Central Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\LMPT2LC8\\Lu et al. - 2021 - Integrating pathomics with radiomics and genomics .pdf:application/pdf},
}

@article{sato_development_nodate,
	title = {Development of novel deep multimodal representation learning-based model for the differentiation of liver tumors on {B}-mode ultrasound images},
	volume = {n/a},
	issn = {1440-1746},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jgh.15763},
	doi = {10.1111/jgh.15763},
	abstract = {Background and Aim Recently, multimodal representation learning for images and other information such as numbers or language has gained much attention. The aim of the current study was to analyze the diagnostic performance of deep multimodal representation model-based integration of tumor image, patient background, and blood biomarkers for the differentiation of liver tumors observed using B-mode ultrasonography (US). Method First, we applied supervised learning with a convolutional neural network (CNN) to 972 liver nodules in the training and development sets to develop a predictive model using segmented B-mode tumor images. Additionally, we also applied a deep multimodal representation model to integrate information about patient background or blood biomarkers to B-mode images. We then investigated the performance of the models in an independent test set of 108 liver nodules. Results Using only the segmented B-mode images, the diagnostic accuracy and area under the curve (AUC) values were 68.52\% and 0.721, respectively. As the information about patient background and blood biomarkers was integrated, the diagnostic performance increased in a stepwise manner. The diagnostic accuracy and AUC value of the multimodal DL model (which integrated B-mode tumor image, patient age, sex, aspartate aminotransferase, alanine aminotransferase, platelet count, and albumin data) reached 96.30\% and 0.994, respectively. Conclusion Integration of patient background and blood biomarkers in addition to US image using multimodal representation learning outperformed the CNN model using US images. We expect that the deep multimodal representation model could be a feasible and acceptable tool for the definitive diagnosis of liver tumors using B-mode US.},
	language = {en},
	number = {n/a},
	urldate = {2022-02-07},
	journal = {Journal of Gastroenterology and Hepatology},
	author = {Sato, Masaya and Kobayashi, Tamaki and Soroida, Yoko and Tanaka, Takashi and Nakatsuka, Takuma and Nakagawa, Hayato and Nakamura, Ayaka and Kurihara, Makiko and Endo, Momoe and Hikita, Hiromi and Sato, Mamiko and Gotoh, Hiroaki and Iwai, Tomomi and Tateishi, Ryosuke and Koike, Kazuhiko and Yatomi, Yutaka},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jgh.15763},
	keywords = {machine learning, B-mode, convolutional neural network, deep multimodal representation learning, liver tumor},
	annote = {citing Deep learning with multimodal representation for pancancer prognosis prediction},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\C38RMUVN\\Sato et al. - Development of novel deep multimodal representatio.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\RB772CK8\\jgh.html:text/html},
}

@inproceedings{msabah_survey_2021,
	title = {A {Survey} on {Deep} {Learning} {Methods} for {Cancer} {Diagnosis} {Using} {Multimodal} {Data} {Fusion}},
	doi = {10.1109/EHB52898.2021.9657722},
	abstract = {Recent advances in data collecting methods have enabled life scientists to collect multimodal data from a variety of biological application fields. Multimodal deep learning is becoming more widespread as the capacity of multimodal data streams and deep learning algorithms grows. This leads to the creation of models capable of consistently processing and interpreting multimodal data. The fusion of multimodal data, such as genetic data and histopathological images, is important to better understand cancer heterogeneity and complexity for specific therapies, as well as improving predictions for cancer research. In this brief overview, we present several pioneering deep learning models where we focus on different architectures that has seen use in the medical field, shedding some light on the importance of deep learning in cancer diagnosis considering its immense results in recent years. We particularly set our eyes on multimodal deep learning approaches fusing histopathological images and genomic data to achieve better results than a single modality.},
	booktitle = {2021 {International} {Conference} on e-{Health} and {Bioengineering} ({EHB})},
	author = {M{\textquoteright}Sabah, Chems Eddine Louahem and Bouziane, Ahmed and Ferdi, Youcef},
	month = nov,
	year = {2021},
	note = {ISSN: 2575-5145},
	keywords = {Data models, deep learning, cancer, Deep learning, Data integration, Genomics, artificial Intelligence, diagnosis, Medical treatment, multimodal, Prediction algorithms, Streaming media},
	pages = {1--4},
	annote = {citing Deep learning with multimodal representation for pancancer prognosis prediction},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\88TKHDKP\\9657722.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\SJ2ZQIBN\\M{\textquoteright}Sabah et al. - 2021 - A Survey on Deep Learning Methods for Cancer Diagn.pdf:application/pdf},
}

@article{vale-silva_long-term_2021,
	title = {Long-term cancer survival prediction using multimodal deep learning},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-92799-4},
	doi = {10.1038/s41598-021-92799-4},
	abstract = {The age of precision medicine demands powerful computational techniques to handle high-dimensional patient data. We present MultiSurv, a multimodal deep learning method for long-term pan-cancer survival prediction. MultiSurv uses dedicated submodels to establish feature representations of clinical, imaging, and different high-dimensional omics data modalities. A data fusion layer aggregates the multimodal representations, and a prediction submodel generates conditional survival probabilities for follow-up time intervals spanning several decades. MultiSurv is the first non-linear and non-proportional survival prediction method that leverages multimodal data. In addition, MultiSurv can handle missing data, including single values and complete data modalities. MultiSurv was applied to data from 33 different cancer types and yields accurate pan-cancer patient survival curves. A quantitative comparison with previous methods showed that Multisurv achieves the best results according to different time-dependent metrics. We also generated visualizations of the learned multimodal representation of MultiSurv, which revealed insights on cancer characteristics and heterogeneity.},
	language = {en},
	number = {1},
	urldate = {2022-02-07},
	journal = {Scientific Reports},
	author = {Vale-Silva, Lu{\'i}s A. and Rohr, Karl},
	month = jun,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Cancer, Data integration, Cancer genomics, Cancer imaging, Cancer models, Computational biology and bioinformatics, Computational models},
	pages = {13505},
	annote = {citing Pan-Cancer Prognosis Prediction Using Multimodal Deep Learning},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\UCCSWLEM\\Vale-Silva und Rohr - 2021 - Long-term cancer survival prediction using multimo.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\2J7PGCJC\\s41598-021-92799-4.html:text/html},
}

@inproceedings{guan_predicting_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Predicting {Esophageal} {Fistula} {Risks} {Using} a {Multimodal} {Self}-attention {Network}},
	isbn = {978-3-030-87240-3},
	doi = {10.1007/978-3-030-87240-3_69},
	abstract = {Radiotherapy plays a vital role in treating patients with esophageal cancer (EC), whereas potential complications such as esophageal fistula (EF) can be devastating and even life-threatening. Therefore, predicting EF risks prior to radiotherapies for EC patients is crucial for their clinical treatment and quality of life. We propose a novel method of combining thoracic Computerized Tomography (CT) scans and clinical tabular data to improve the prediction of EF risks in EC patients. The multimodal network includes encoders to extract salient features from images and clinical data, respectively. In addition, we devise a self-attention module, named VisText, to uncover the complex relationships and correlations among different features. The associated multimodal features are integrated with clinical features by aggregation to further enhance prediction accuracy. Experimental results indicate that our method classifies EF status for EC patients with an accuracy of 0.8366, F1 score of 0.7337, specificity of 0.9312 and AUC of 0.9119, outperforming other methods in comparison.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} {\textendash} {MICCAI} 2021},
	publisher = {Springer International Publishing},
	author = {Guan, Yulu and Cui, Hui and Xu, Yiyue and Jin, Qiangguo and Feng, Tian and Tu, Huawei and Xuan, Ping and Li, Wanlong and Wang, Linlin and Duh, Been-Lirn},
	editor = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
	year = {2021},
	keywords = {Esophageal fistula prediction, Multimodal attention, Self attention},
	pages = {721--730},
	annote = {citing Pan-Cancer Prognosis Prediction Using Multimodal Deep Learning},
	file = {Springer Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\DQQN27WH\\Guan et al. - 2021 - Predicting Esophageal Fistula Risks Using a Multim.pdf:application/pdf},
}

@article{schulz_multimodal_2021,
	title = {Multimodal {Deep} {Learning} for {Prognosis} {Prediction} in {Renal} {Cancer}.},
	volume = {11},
	issn = {2234-943X},
	url = {https://europepmc.org/articles/PMC8651560/},
	doi = {10.3389/fonc.2021.788740},
	abstract = {Clear-cell renal cell carcinoma (ccRCC) is common and associated with substantial mortality. TNM stage and histopathological grading have been the sole determinants of a patient{\textquoteright}s prognosis for decades and there are few prognostic biomarkers used ...},
	language = {eng},
	urldate = {2022-02-07},
	journal = {Frontiers in oncology},
	author = {Schulz, S. and Woerl, A. C. and Jungmann, F. and Glasner, C. and Stenzel, P. and Strobl, S. and Fernandez, A. and Wagner, D. C. and Haferkamp, A. and Mildenberger, P. and Roth, W. and Foersch, S.},
	year = {2021},
	pmid = {34900744},
	pages = {788740--788740},
	annote = {citing Predicting cancer outcomes from histology and genomics using convolutional networks},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\2E8LEH7G\\Schulz et al. - 2021 - Multimodal Deep Learning for Prognosis Prediction .pdf:application/pdf},
}

@article{yan_richer_2021,
	title = {Richer fusion network for breast cancer classification based on multimodal data},
	volume = {21},
	issn = {1472-6947},
	url = {https://doi.org/10.1186/s12911-020-01340-6},
	doi = {10.1186/s12911-020-01340-6},
	abstract = {Deep learning algorithms significantly improve the accuracy of pathological image classification, but the accuracy of breast cancer classification using only single-mode pathological images still cannot meet the needs of clinical practice. Inspired by the real scenario of pathologists reading pathological images for diagnosis, we integrate pathological images and structured data extracted from clinical electronic medical record (EMR) to further improve the accuracy of breast cancer classification.},
	number = {1},
	urldate = {2022-02-07},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Yan, Rui and Zhang, Fa and Rao, Xiaosong and Lv, Zhilong and Li, Jintao and Zhang, Lingling and Liang, Shuang and Li, Yilin and Ren, Fei and Zheng, Chunhou and Liang, Jun},
	month = apr,
	year = {2021},
	keywords = {Multimodal fusion, Breast cancer classification, Convolutional neural network, Electronic medical record, Pathological image},
	pages = {134},
	annote = {citing Predicting cancer outcomes from histology and genomics using convolutional networks},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\YNH7MU6K\\Yan et al. - 2021 - Richer fusion network for breast cancer classifica.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\TSFIK89H\\s12911-020-01340-6.html:text/html},
}

@inproceedings{subramanian_multimodal_2020,
	title = {Multimodal {Fusion} of {Imaging} and {Genomics} for {Lung} {Cancer} {Recurrence} {Prediction}},
	doi = {10.1109/ISBI45749.2020.9098545},
	abstract = {Lung cancer has a high rate of recurrence in early-stage patients. Predicting the post-surgical recurrence in lung cancer patients has traditionally been approached using single modality information of genomics or radiology images. We investigate the potential of multimodal fusion for this task. By combining computed tomography (CT) images and genomics, we demonstrate improved prediction of recurrence using linear Cox proportional hazards models with elastic net regularization. We work on a recent non-small cell lung cancer (NSCLC) radiogenomics dataset of 130 patients and observe an increase in concordance-index values of up to 10\%. Employing non-linear methods from the neural network literature, such as multi -layer perceptrons and visual-question answering fusion modules, did not improve performance consistently. This indicates the need for larger multimodal datasets and fusion techniques better adapted to this biological setting.},
	booktitle = {2020 {IEEE} 17th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Subramanian, Vaishnavi and Do, Minh N. and Syeda-Mahmood, Tanveer},
	month = apr,
	year = {2020},
	note = {ISSN: 1945-8452},
	keywords = {Bioinformatics, Cancer, Hazards, Genomics, Computed tomography, Lung},
	pages = {804--808},
	annote = {cited by Multimodal Fusion Using Sparse Cca For Breast Cancer Survival Prediction},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\6LYE2DSX\\9098545.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\S6FVMKZJ\\Subramanian et al. - 2020 - Multimodal Fusion of Imaging and Genomics for Lung.pdf:application/pdf},
}

@article{yap_multimodal_2018,
	title = {Multimodal skin lesion classification using deep learning},
	volume = {27},
	issn = {1600-0625},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exd.13777},
	doi = {10.1111/exd.13777},
	abstract = {While convolutional neural networks (CNNs) have successfully been applied for skin lesion classification, previous studies have generally considered only a single clinical/macroscopic image and output a binary decision. In this work, we have presented a method which combines multiple imaging modalities together with patient metadata to improve the performance of automated skin lesion diagnosis. We evaluated our method on a binary classification task for comparison with previous studies as well as a five class classification task representative of a real-world clinical scenario. We showed that our multimodal classifier outperforms a baseline classifier that only uses a single macroscopic image in both binary melanoma detection (AUC 0.866 vs 0.784) and in multiclass classification (mAP 0.729 vs 0.598). In addition, we have quantitatively showed the automated diagnosis of skin lesions using dermatoscopic images obtains a higher performance when compared to using macroscopic images. We performed experiments on a new data set of 2917 cases where each case contains a dermatoscopic image, macroscopic image and patient metadata.},
	language = {en},
	number = {11},
	urldate = {2022-02-07},
	journal = {Experimental Dermatology},
	author = {Yap, Jordan and Yolland, William and Tschandl, Philipp},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/exd.13777},
	keywords = {deep learning, multimodal, dermatology, dermatoscopy, feature fusion},
	pages = {1261--1267},
	annote = {cited by Predicting Esophageal Fistula Risks Using a Multimodal Self-attention Network},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\4PSVN9AI\\Yap et al. - 2018 - Multimodal skin lesion classification using deep l.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\5BJJA8A8\\exd.html:text/html},
}

@inproceedings{xu_multimodal_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multimodal {Deep} {Learning} for {Cervical} {Dysplasia} {Diagnosis}},
	isbn = {978-3-319-46723-8},
	doi = {10.1007/978-3-319-46723-8_14},
	abstract = {To improve the diagnostic accuracy of cervical dysplasia, it is important to fuse multimodal information collected during a patient{\textquoteright}s screening visit. However, current multimodal frameworks suffer from low sensitivity at high specificity levels, due to their limitations in learning correlations among highly heterogeneous modalities. In this paper, we design a deep learning framework for cervical dysplasia diagnosis by leveraging multimodal information. We first employ the convolutional neural network (CNN) to convert the low-level image data into a feature vector fusible with other non-image modalities. We then jointly learn the non-linear correlations among all modalities in a deep neural network. Our multimodal framework is an end-to-end deep network which can learn better complementary features from the image and non-image modalities. It automatically gives the final diagnosis for cervical dysplasia with 87.83 \% sensitivity at 90 \% specificity on a large dataset, which significantly outperforms methods using any single source of information alone and previous multimodal frameworks.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} {\textendash} {MICCAI} 2016},
	publisher = {Springer International Publishing},
	author = {Xu, Tao and Zhang, Han and Huang, Xiaolei and Zhang, Shaoting and Metaxas, Dimitris N.},
	editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R. and Unal, Gozde and Wells, William},
	year = {2016},
	keywords = {Cervical Dysplasia, Cervical Intraepithelial Neoplasia, Convolutional Neural Network, Deep Neural Network, Late Fusion},
	pages = {115--123},
	annote = {cited by Predicting Esophageal Fistula Risks Using a Multimodal Self-attention Network},
	file = {Springer Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\V83DVK7H\\Xu et al. - 2016 - Multimodal Deep Learning for Cervical Dysplasia Di.pdf:application/pdf},
}

@article{guo_deep_2019,
	title = {Deep {Multimodal} {Representation} {Learning}: {A} {Survey}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {Deep {Multimodal} {Representation} {Learning}},
	doi = {10.1109/ACCESS.2019.2916887},
	abstract = {Multimodal representation learning, which aims to narrow the heterogeneity gap among different modalities, plays an indispensable role in the utilization of ubiquitous multimodal data. Due to the powerful representation ability with multiple levels of abstraction, deep learning-based multimodal representation learning has attracted much attention in recent years. In this paper, we provided a comprehensive survey on deep multimodal representation learning which has never been concentrated entirely. To facilitate the discussion on how the heterogeneity gap is narrowed, according to the underlying structures in which different modalities are integrated, we category deep multimodal representation learning methods into three frameworks: joint representation, coordinated representation, and encoder-decoder. Additionally, we review some typical models in this area ranging from conventional models to newly developed technologies. This paper highlights on the key issues of newly developed technologies, such as encoder-decoder model, generative adversarial networks, and attention mechanism in a multimodal representation learning perspective, which, to the best of our knowledge, have never been reviewed previously, even though they have become the major focuses of much contemporary research. For each framework or model, we discuss its basic structure, learning objective, application scenes, key issues, advantages, and disadvantages, such that both novel and experienced researchers can benefit from this survey. Finally, we suggest some important directions for future work.},
	journal = {IEEE Access},
	author = {Guo, Wenzhong and Wang, Jianwen and Wang, Shiping},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Feature extraction, Deep learning, Task analysis, Data mining, Decoding, deep multimodal fusion, multimodal adversarial learning, multimodal deep learning, Multimodal representation learning, multimodal translation, Semantics, Speech recognition},
	pages = {63373--63394},
	annote = {cited by Long-term cancer survival prediction using multimodal deep learning},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\DQAWUPTE\\8715409.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\EMLDUM5I\\Guo et al. - 2019 - Deep Multimodal Representation Learning A Survey.pdf:application/pdf},
}

@inproceedings{wang_ammasurv_2021,
	title = {{AMMASurv}: {Asymmetrical} {Multi}-{Modal} {Attention} for {Accurate} {Survival} {Analysis} with {Whole} {Slide} {Images} and {Gene} {Expression} {Data}},
	shorttitle = {{AMMASurv}},
	doi = {10.1109/BIBM52615.2021.9669382},
	abstract = {The use of multi-modal data such as the combination of whole slide images (WSIs) and gene expression data for survival analysis can lead to more accurate survival predictions. Previous multi-modal survival models are not able to efficiently excavate the intrinsic information within each modality. Moreover, previous methods regard the information from different modalities as similarly important so they cannot flexibly utilize the potential connection between the modalities. To address the above problems, we propose a new asymmetrical multi-modal method, termed as AMMASurv. Different from previous works, AMMASurv can effectively utilize the intrinsic information within every modality and flexibly adapts to the modalities of different importance. Encouraging experimental results demonstrate the superiority of our method over other state-of-the-art methods.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Wang, Ruoqi and Huang, Ziwang and Wang, Haitao and Wu, Hejun},
	month = dec,
	year = {2021},
	keywords = {Bioinformatics, Gene expression, Survival Analysis, WSI, Transformer, Conferences, Gene Expression, MultiModal Learning},
	pages = {757--760},
	annote = {citing Long-term cancer survival prediction using multimodal deep learning},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\3Q2Y3PBT\\9669382.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\4XY3MPVE\\Wang et al. - 2021 - AMMASurv Asymmetrical Multi-Modal Attention for A.pdf:application/pdf},
}

@article{kawahara_seven-point_2019,
	title = {Seven-{Point} {Checklist} and {Skin} {Lesion} {Classification} {Using} {Multitask} {Multimodal} {Neural} {Nets}},
	volume = {23},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2018.2824327},
	abstract = {We propose a multitask deep convolutional neural network, trained on multimodal data (clinical and dermoscopic images, and patient metadata), to classify the 7-point melanoma checklist criteria and perform skin lesion diagnosis. Our neural network is trained using several multitask loss functions, where each loss considers different combinations of the input modalities, which allows our model to be robust to missing data at inference time. Our final model classifies the 7-point checklist and skin condition diagnosis, produces multimodal feature vectors suitable for image retrieval, and localizes clinically discriminant regions. We benchmark our approach using 1011 lesion cases, and report comprehensive results over all 7-point criteria and diagnosis. We also make our dataset (images and metadata) publicly available online at http://derm.cs.sfu.ca.},
	number = {2},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Kawahara, Jeremy and Daneshvar, Sara and Argenziano, Giuseppe and Hamarneh, Ghassan},
	month = mar,
	year = {2019},
	note = {Conference Name: IEEE Journal of Biomedical and Health Informatics},
	keywords = {Convolutional neural networks, Feature extraction, deep learning, dermatology, 7-point checklist, Classification, convolutional neural networks, Lesions, Malignant tumors, melanoma, Pattern analysis, skin, Skin},
	pages = {538--546},
	annote = {cited by Skin lesion classification using ensembles of multi-resolution EfficientNets with meta data},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\ZXTTDSX3\\Kawahara et al. - 2019 - Seven-Point Checklist and Skin Lesion Classificati.pdf:application/pdf},
}

@article{hohn_combining_2021,
	title = {Combining {CNN}-based histologic whole slide image analysis and patient data to improve skin cancer classification},
	volume = {149},
	issn = {0959-8049},
	url = {https://www.sciencedirect.com/science/article/pii/S0959804921001404},
	doi = {10.1016/j.ejca.2021.02.032},
	abstract = {Background
Clinicians and pathologists traditionally use patient data in addition to clinical examination to support their diagnoses.
Objectives
We investigated whether a combination of histologic whole slides image (WSI) analysis based on convolutional neural networks (CNNs) and commonly available patient data (age, sex and anatomical site of the lesion) in a binary melanoma/nevus classification task could increase the performance compared with CNNs alone.
Methods
We used 431 WSIs from two different laboratories and analysed the performance of classifiers that used the image or patient data individually or three common fusion techniques. Furthermore, we tested a naive combination of patient data and an image classifier: for cases interpreted as {\textquoteleft}uncertain{\textquoteright} (CNN output score {\textless}0.7), the decision of the CNN was replaced by the decision of the patient data classifier.
Results
The CNN on its own achieved the best performance (mean~{\textpm}~standard deviation of five individual runs) with AUROC of 92.30\%~{\textpm}~0.23\% and balanced accuracy of 83.17\%~{\textpm}~0.38\%. While the classification performance was not significantly improved in general by any of the tested fusions, naive strategy of replacing the image classifier with the patient data classifier on slides with low output scores improved balanced accuracy to 86.72\%~{\textpm}~0.36\%.
Conclusion
In most cases, the CNN on its own was so accurate that patient data integration did not provide any benefit. However, incorporating patient data for lesions that were classified by the CNN with low {\textquoteleft}confidence{\textquoteright} improved balanced accuracy.},
	language = {en},
	urldate = {2022-02-08},
	journal = {European Journal of Cancer},
	author = {H{\"o}hn, Julia and Krieghoff-Henning, Eva and Jutzi, Tanja B. and von Kalle, Christof and Utikal, Jochen S. and Meier, Friedegund and Gellrich, Frank F. and Hobelsberger, Sarah and Hauschild, Axel and Schlager, Justin G. and French, Lars and Heinzerling, Lucie and Schlaak, Max and Ghoreschi, Kamran and Hilke, Franz J. and Poch, Gabriela and Kutzner, Heinz and Heppt, Markus V. and Haferkamp, Sebastian and Sondermann, Wiebke and Schadendorf, Dirk and Schilling, Bastian and Goebeler, Matthias and Hekler, Achim and Fr{\"o}hling, Stefan and Lipka, Daniel B. and Kather, Jakob N. and Krahl, Dieter and Ferrara, Gerardo and Haggenm{\"u}ller, Sarah and Brinker, Titus J.},
	month = may,
	year = {2021},
	keywords = {Convolutional neural networks, Data fusion, Histologic whole slide images, Patient data, Skin cancer classification},
	pages = {94--101},
	annote = {citing Skin lesion classification using ensembles of multi-resolution EfficientNets with meta data},
	file = {ScienceDirect Snapshot:C\:\\Users\\roman\\Zotero\\storage\\9X3VE5JH\\S0959804921001404.html:text/html;Volltext:C\:\\Users\\roman\\Zotero\\storage\\3U5RL5C6\\H{\"o}hn et al. - 2021 - Combining CNN-based histologic whole slide image a.pdf:application/pdf},
}

@article{hohn_integrating_2021,
	title = {Integrating {Patient} {Data} {Into} {Skin} {Cancer} {Classification} {Using} {Convolutional} {Neural} {Networks}: {Systematic} {Review}},
	volume = {23},
	shorttitle = {Integrating {Patient} {Data} {Into} {Skin} {Cancer} {Classification} {Using} {Convolutional} {Neural} {Networks}},
	url = {https://www.jmir.org/2021/7/e20708},
	doi = {10.2196/20708},
	abstract = {Background: Recent years have been witnessing a substantial improvement in the accuracy of skin cancer classification using convolutional neural networks (CNNs). CNNs perform on par with or better than dermatologists with respect to the classification tasks of single images. However, in clinical practice, dermatologists also use other patient data beyond the visual aspects present in a digitized image, further increasing their diagnostic accuracy. Several pilot studies have recently investigated the effects of integrating different subtypes of patient data into CNN-based skin cancer classifiers.
Objective: This systematic review focuses on the current research investigating the impact of merging information from image features and patient data on the performance of CNN-based skin cancer image classification. This study aims to explore the potential in this field of research by evaluating the types of patient data used, the ways in which the nonimage data are encoded and merged with the image features, and the impact of the integration on the classifier performance.
Methods: Google Scholar, PubMed, MEDLINE, and ScienceDirect were screened for peer-reviewed studies published in English that dealt with the integration of patient data within a CNN-based skin cancer classification. The search terms skin cancer classification, convolutional neural network(s), deep learning, lesions, melanoma, metadata, clinical information, and patient data were combined.
Results: A total of 11 publications fulfilled the inclusion criteria. All of them reported an overall improvement in different skin lesion classification tasks with patient data integration. The most commonly used patient data were age, sex, and lesion location. The patient data were mostly one-hot encoded. There were differences in the complexity that the encoded patient data were processed with regarding deep learning methods before and after fusing them with the image features for a combined classifier.
Conclusions: This study indicates the potential benefits of integrating patient data into CNN-based diagnostic algorithms. However, how exactly the individual patient data enhance classification performance, especially in the case of multiclass classification problems, is still unclear. Moreover, a substantial fraction of patient data used by dermatologists remains to be analyzed in the context of CNN-based skin cancer classification. Further exploratory analyses in this promising field may optimize patient data integration into CNN-based skin cancer diagnostics for patients{\textquoteright} benefits.},
	language = {EN},
	number = {7},
	urldate = {2022-02-08},
	journal = {Journal of Medical Internet Research},
	author = {H{\"o}hn, Julia and Hekler, Achim and Krieghoff-Henning, Eva and Kather, Jakob Nikolas and Utikal, Jochen Sven and Meier, Friedegund and Gellrich, Frank Friedrich and Hauschild, Axel and French, Lars and Schlager, Justin Gabriel and Ghoreschi, Kamran and Wilhelm, Tabea and Kutzner, Heinz and Heppt, Markus and Haferkamp, Sebastian and Sondermann, Wiebke and Schadendorf, Dirk and Schilling, Bastian and Maron, Roman C. and Schmitt, Max and Jutzi, Tanja and Fr{\"o}hling, Stefan and Lipka, Daniel B. and Brinker, Titus Josef},
	month = jul,
	year = {2021},
	note = {Company: Journal of Medical Internet Research
Distributor: Journal of Medical Internet Research
Institution: Journal of Medical Internet Research
Label: Journal of Medical Internet Research
Publisher: JMIR Publications Inc., Toronto, Canada},
	pages = {e20708},
	annote = {citing Skin lesion classification using ensembles of multi-resolution EfficientNets with meta data},
	file = {Snapshot:C\:\\Users\\roman\\Zotero\\storage\\Q7TQ52XP\\e20708.html:text/html;Volltext:C\:\\Users\\roman\\Zotero\\storage\\XQJAY38C\\H{\"o}hn et al. - 2021 - Integrating Patient Data Into Skin Cancer Classifi.pdf:application/pdf},
}

@article{pacheco_attention-based_2021,
	title = {An {Attention}-{Based} {Mechanism} to {Combine} {Images} and {Metadata} in {Deep} {Learning} {Models} {Applied} to {Skin} {Cancer} {Classification}},
	volume = {25},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2021.3062002},
	abstract = {Computer-aided skin cancer classification systems built with deep neural networks usually yield predictions based only on images of skin lesions. Despite presenting promising results, it is possible to achieve higher performance by taking into account patient demographics, which are important clues that human experts consider during skin lesion screening. In this article, we deal with the problem of combining images and metadata features using deep learning models applied to skin cancer classification. We propose the Metadata Processing Block (MetaBlock), a novel algorithm that uses metadata to support data classification by enhancing the most relevant features extracted from the images throughout the classification pipeline. We compared the proposed method with two other combination approaches: the MetaNet and one based on features concatenation. Results obtained for two different skin lesion datasets show that our method improves classification for all tested models and performs better than the other combination approaches in 6 out of 10 scenarios.},
	number = {9},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Pacheco, Andre G. C. and Krohling, Renato A.},
	month = sep,
	year = {2021},
	note = {Conference Name: IEEE Journal of Biomedical and Health Informatics},
	keywords = {Convolutional neural networks, Feature extraction, deep learning, Data mining, Lesions, Skin, data aggregation, Logic gates, Metadata, Skin cancer, skin cancer classification},
	pages = {3554--3563},
	annote = {citing Skin lesion classification using ensembles of multi-resolution EfficientNets with meta data},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\5HVA4U2Q\\9364366.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\9SHD2JK6\\Pacheco und Krohling - 2021 - An Attention-Based Mechanism to Combine Images and.pdf:application/pdf},
}

@article{sun_skin_2021,
	title = {Skin {Lesion} {Classification} {Using} {Additional} {Patient} {Information}},
	volume = {2021},
	issn = {2314-6133},
	url = {https://www.hindawi.com/journals/bmri/2021/6673852/},
	doi = {10.1155/2021/6673852},
	abstract = {In this paper, we describe our method for skin lesion classification. The goal is to classify skin lesions based on dermoscopic images to several diagnoses{\textquoteright} classes presented in the HAM (Human Against Machine) dataset: melanoma (MEL), melanocytic nevus (NV), basal cell carcinoma (BCC), actinic keratosis (AK), benign keratosis (BKL), dermatofibroma (DF), and vascular lesion (VASC). We propose a simplified solution which has a better accuracy than previous methods, but only predicted on a single model that is practical for a real-world scenario. Our results show that using a network with additional metadata as input achieves a better classification performance. This metadata includes both the patient information and the extra information during the data augmentation process. On the international skin imaging collaboration (ISIC) 2018 skin lesion classification challenge test set, our algorithm yields a balanced multiclass accuracy of 88.7\% on a single model and 89.5\% for the embedding solution, which makes it the currently first ranked algorithm on the live leaderboard. To improve the inference accuracy. Test time augmentation (TTA) is applied. We also demonstrate how Grad-CAM is applied in TTA. Therefore, TTA and Grad-CAM can be integrated in heat map generation, which can be very helpful to assist the clinician for diagnosis.},
	language = {en},
	urldate = {2022-02-08},
	journal = {BioMed Research International},
	author = {Sun, Qilin and Huang, Chao and Chen, Minjie and Xu, Hui and Yang, Yali},
	month = apr,
	year = {2021},
	note = {Publisher: Hindawi},
	pages = {e6673852},
	annote = {citing Skin lesion classification using ensembles of multi-resolution EfficientNets with meta data},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\F5LU7BW4\\Sun et al. - 2021 - Skin Lesion Classification Using Additional Patien.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\ASD2XLBF\\6673852.html:text/html},
}

@article{chieregato_hybrid_2021,
	title = {A hybrid machine learning/deep learning {COVID}-19 severity predictive model from {CT} images and clinical data},
	url = {http://arxiv.org/abs/2105.06141},
	abstract = {COVID-19 clinical presentation and prognosis are highly variable, ranging from asymptomatic and paucisymptomatic cases to acute respiratory distress syndrome and multi-organ involvement. We developed a hybrid machine learning/deep learning model to classify patients in two outcome categories, non-ICU and ICU (intensive care admission or death), using 558 patients admitted in a northern Italy hospital in February/May of 2020. A fully 3D patient-level CNN classifier on baseline CT images is used as feature extractor. Features extracted, alongside with laboratory and clinical data, are fed for selection in a Boruta algorithm with SHAP game theoretical values. A classifier is built on the reduced feature space using CatBoost gradient boosting algorithm and reaching a probabilistic AUC of 0.949 on holdout test set. The model aims to provide clinical decision support to medical doctors, with the probability score of belonging to an outcome class and with case-based SHAP interpretation of features importance.},
	urldate = {2022-02-08},
	journal = {arXiv:2105.06141 [physics, q-bio]},
	author = {Chieregato, Matteo and Frangiamore, Fabio and Morassi, Mauro and Baresi, Claudia and Nici, Stefania and Bassetti, Chiara and Bn{\`a}, Claudio and Galelli, Marco},
	month = may,
	year = {2021},
	note = {arXiv: 2105.06141},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Physics - Medical Physics, Quantitative Biology - Quantitative Methods},
	annote = {citing Skin lesion classification using ensembles of multi-resolution EfficientNets with meta data},
	file = {arXiv Fulltext PDF:C\:\\Users\\roman\\Zotero\\storage\\HAIYH8SX\\Chieregato et al. - 2021 - A hybrid machine learningdeep learning COVID-19 s.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\roman\\Zotero\\storage\\5BPJH8VH\\2105.html:text/html},
}

@inproceedings{grant_deep_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Deep {Learning} {Classification} of {Cardiomegaly} {Using} {Combined} {Imaging} and {Non}-imaging {ICU} {Data}},
	isbn = {978-3-030-80432-9},
	doi = {10.1007/978-3-030-80432-9_40},
	abstract = {In this paper, we investigate the classification of cardiomegaly using multimodal data, combining imaging data from chest radiography with routinely collected Intensive Care Unit (ICU) data comprising vital sign values, laboratory measurements, and admission metadata. In practice a clinician would assess for the presence of cardiomegaly using a synthesis of multiple sources of data, however, prior machine learning approaches to this task have focused on chest radiographs only. We show that non-imaging ICU data can be used for cardiomegaly classification and propose a novel multimodal network trained simultaneously on both chest radiographs and ICU data. We compare the predictive power of both single-mode approaches with the joint network. We use a subset of data from the publicly available MIMIC-CXR and MIMIC-IV datasets, which contain both chest radiographs and non-imaging ICU data for the same patients. The approach from non-imaging ICU data alone achieves an AUC of 0.684 and the standard chest radiography approach an AUC of 0.840. Our joint model achieves an AUC of 0.880. We conclude that non-imaging ICU data have predictive value for cardiomegaly, and that combining chest radiographs with non-imaging ICU data has the potential to improve model performance for the same subset of patients, with further work required to demonstrate a significant improvement.},
	language = {en},
	booktitle = {Medical {Image} {Understanding} and {Analysis}},
	publisher = {Springer International Publishing},
	author = {Grant, Declan and Papie{\.z}, Bart{\l }omiej W. and Parsons, Guy and Tarassenko, Lionel and Mahdi, Adam},
	editor = {Papie{\.z}, Bart{\l }omiej W. and Yaqub, Mohammad and Jiao, Jianbo and Namburete, Ana I. L. and Noble, J. Alison},
	year = {2021},
	keywords = {Deep learning, Cardiomegaly, Chest X-ray, Multimodal approach},
	pages = {547--558},
	annote = {citing Skin lesion classification using ensembles of multi-resolution EfficientNets with meta data},
	file = {Springer Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\DDUPH2XM\\Grant et al. - 2021 - Deep Learning Classification of Cardiomegaly Using.pdf:application/pdf},
}

@article{huang_multimodal_2020,
	title = {Multimodal fusion with deep neural networks for leveraging {CT} imaging and electronic health record: a case-study in pulmonary embolism detection},
	volume = {10},
	copyright = {2020 The Author(s)},
	issn = {2045-2322},
	shorttitle = {Multimodal fusion with deep neural networks for leveraging {CT} imaging and electronic health record},
	url = {https://www.nature.com/articles/s41598-020-78888-w},
	doi = {10.1038/s41598-020-78888-w},
	abstract = {Recent advancements in deep learning have led to a resurgence of medical imaging and Electronic Medical Record (EMR) models for a variety of applications, including clinical decision support, automated workflow triage, clinical prediction and more. However, very few models have been developed to integrate both clinical and imaging data, despite that in routine practice clinicians rely on EMR to provide context in medical imaging interpretation. In this study, we developed and compared different multimodal fusion model architectures that are capable of utilizing both pixel data from volumetric Computed Tomography Pulmonary Angiography scans and clinical patient data from the EMR to automatically classify Pulmonary Embolism (PE) cases. The best performing multimodality model is a late fusion model that achieves an AUROC of 0.947 [95\% CI: 0.946{\textendash}0.948] on the entire held-out test~set, outperforming imaging-only and EMR-only single modality models.},
	language = {en},
	number = {1},
	urldate = {2022-02-08},
	journal = {Scientific Reports},
	author = {Huang, Shih-Cheng and Pareek, Anuj and Zamanian, Roham and Banerjee, Imon and Lungren, Matthew P.},
	month = dec,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Data integration, Embolism},
	pages = {22147},
	annote = {citing Seven-Point Checklist and Skin Lesion Classification Using Multitask Multimodal Neural Nets},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\WJMPTC5Z\\Huang et al. - 2020 - Multimodal fusion with deep neural networks for le.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\LZ87U2YZ\\s41598-020-78888-w.html:text/html},
}

@inproceedings{nunnari_study_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Study} on the {Fusion} of {Pixels} and {Patient} {Metadata} in {CNN}-{Based} {Classification} of {Skin} {Lesion} {Images}},
	isbn = {978-3-030-57321-8},
	doi = {10.1007/978-3-030-57321-8_11},
	abstract = {We present a study on the fusion of pixel data and patient metadata (age, gender, and body location) for improving the classification of skin lesion images. The experiments have been conducted with the ISIC 2019 skin lesion classification challenge data set. Taking two plain convolutional neural networks (CNNs) as a baseline, metadata are merged using either non-neural machine learning methods (tree-based and support vector machines) or shallow neural networks. Results show that shallow neural networks outperform other approaches in all overall evaluation measures. However, despite the increase in the classification accuracy (up to +19.1\%), interestingly, the average per-class sensitivity decreases in three out of four cases for CNNs, thus suggesting that using metadata penalizes the prediction accuracy for lower represented classes. A study on the patient metadata shows that age is the most useful metadatum as a decision criterion, followed by body location and gender.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Extraction}},
	publisher = {Springer International Publishing},
	author = {Nunnari, Fabrizio and Bhuvaneshwara, Chirag and Ezema, Abraham Obinwanne and Sonntag, Daniel},
	editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
	year = {2020},
	keywords = {Machine learning, Data fusion, Convolutional neural network, Patient metadata, Skin lesion classification},
	pages = {191--208},
	annote = {citing Seven-Point Checklist and Skin Lesion Classification Using Multitask Multimodal Neural Nets},
	file = {Springer Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\RITACCQC\\Nunnari et al. - 2020 - A Study on the Fusion of Pixels and Patient Metada.pdf:application/pdf},
}

@article{wang_knowledge-aware_2021,
	title = {Knowledge-aware deep framework for collaborative skin lesion segmentation and melanoma recognition},
	volume = {120},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320321002624},
	doi = {10.1016/j.patcog.2021.108075},
	abstract = {Deep learning techniques have shown their superior performance in dermatologist clinical inspection. Nevertheless, melanoma diagnosis is still a challenging task due to the difficulty of incorporating the useful dermatologist clinical knowledge into the learning process. In this paper, we propose a novel knowledge-aware deep framework that incorporates some clinical knowledge into collaborative learning of two important melanoma diagnosis tasks, i.e., skin lesion segmentation and melanoma recognition. Specifically, to exploit the knowledge of morphological expressions of the lesion region and also the periphery region for melanoma identification, a lesion-based pooling and shape extraction (LPSE) scheme is designed, which transfers the structure information obtained from skin lesion segmentation into melanoma recognition. Meanwhile, to pass the skin lesion diagnosis knowledge from melanoma recognition to skin lesion segmentation, an effective diagnosis guided feature fusion (DGFF) strategy is designed. Moreover, we propose a recursive mutual learning mechanism that further promotes the inter-task cooperation, and thus iteratively improves the joint learning capability of the model for both skin lesion segmentation and melanoma recognition. Experimental results on two publicly available skin lesion datasets show the effectiveness of the proposed method for melanoma analysis.},
	language = {en},
	urldate = {2022-02-08},
	journal = {Pattern Recognition},
	author = {Wang, Xiaohong and Jiang, Xudong and Ding, Henghui and Zhao, Yuqian and Liu, Jun},
	month = dec,
	year = {2021},
	keywords = {Diagnosis guided feature fusion, Knowledge-aware deep framework, Lesion-based pooling and shape extraction, Melanoma diagnosis, Recursive mutual learning},
	pages = {108075},
	annote = {citing Seven-Point Checklist and Skin Lesion Classification Using Multitask Multimodal Neural Nets},
	file = {Eingereichte Version:C\:\\Users\\roman\\Zotero\\storage\\AUFY3G63\\Wang et al. - 2021 - Knowledge-aware deep framework for collaborative s.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\roman\\Zotero\\storage\\EGBHXRZQ\\S0031320321002624.html:text/html},
}

@inproceedings{yildirim-yayilgan_pre-trained_2021,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Pre-trained {CNN} {Based} {Deep} {Features} with {Hand}-{Crafted} {Features} and {Patient} {Data} for {Skin} {Lesion} {Classification}},
	isbn = {978-3-030-71711-7},
	doi = {10.1007/978-3-030-71711-7_13},
	abstract = {Skin cancer is a major public health problem, with millions newly diagnosed cases each year. Melanoma is the deadliest form of skin cancer, responsible for the most over 6500 deaths each year in the US, and the rates have been rising rapidly over years. Because of this, a lot of research is being done in automated image-based systems for skin lesion classification. In our paper we propose an automated melanoma and seborrheic keratosis recognition system, which is based on pre-trained deep network combined with structural features. We compare using different pre-trained deep networks, analyze the impact of using patient data in our approach, and evaluate our system performance with different datasets. Our results shown us that patient data has impact on characteristic curve metric value with around 2{\textendash}6\% and different algorithm in final classification layer has impact with around 1{\textendash}4\%.},
	language = {en},
	booktitle = {Intelligent {Technologies} and {Applications}},
	publisher = {Springer International Publishing},
	author = {Yildirim-Yayilgan, Sule and Arifaj, Blend and Rahimpour, Masoomeh and Hardeberg, Jon Yngve and Ahmedi, Lule},
	editor = {Yildirim Yayilgan, Sule and Bajwa, Imran Sarwar and Sanfilippo, Filippo},
	year = {2021},
	keywords = {CNN, Deep networks, Handcrafted features, Image processing, Skin lesion classification and segmentation},
	pages = {151--162},
	annote = {citing Seven-Point Checklist and Skin Lesion Classification Using Multitask Multimodal Neural Nets},
	file = {Eingereichte Version:C\:\\Users\\roman\\Zotero\\storage\\5UQ6DIVW\\Yildirim-Yayilgan et al. - 2021 - Pre-trained CNN Based Deep Features with Hand-Craf.pdf:application/pdf},
}

@inproceedings{nedelcu_multi-dataset_2020,
	title = {Multi-{Dataset} {Training} for {Skin} {Lesion} {Classification} on {Multimodal} and {Multitask} {Deep} {Learning}},
	url = {https://avestia.com/EECSS2020_Proceedings/files/paper/ICBES/ICBES_120.pdf},
	doi = {10.11159/icbes20.120},
	abstract = {According to the World Health Organization, skin cancer represents approximately one third of every diagnosed cancer, reaching over 3 million cases over the world, annually. Similar to other types of cancer, though, early diagnosis is key for a good outcome, and computer-aided diagnosis has shown great promise in such task. In this paper we improve the results of previous work on skin lesion diagnosis by using a deep convolutional neural network trained on multimodal data, namely macroscopic and dermoscopic image and metadata. For a deep learning approach is important to have a large number of samples, which EDRA dataset does not present. We have improved the results of previous work in the field of multimodal and multitasking for skin lesion classification by performing transfer learning using similar datasets, which are predicting different skin conditions. By pre-training on datasets which belong to a similar domain, the network learns useful features which enhances the performances of the network.},
	language = {en},
	urldate = {2022-02-08},
	author = {Nedelcu, Tudor and Vasconcelos, Maria and Carreiro, Andr{\'e}},
	month = aug,
	year = {2020},
	annote = {citing Seven-Point Checklist and Skin Lesion Classification Using Multitask Multimodal Neural Nets},
	file = {Nedelcu et al. - 2020 - Multi-Dataset Training for Skin Lesion Classificat.pdf:C\:\\Users\\roman\\Zotero\\storage\\FCQ5BF3Y\\Nedelcu et al. - 2020 - Multi-Dataset Training for Skin Lesion Classificat.pdf:application/pdf},
}

@article{artzi_classification_2021,
	title = {Classification of {Pediatric} {Posterior} {Fossa} {Tumors} {Using} {Convolutional} {Neural} {Network} and {Tabular} {Data}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3085771},
	abstract = {Posterior fossa tumors (PFT) are the most common tumors in children. Differentiation between the various PFT types is critical, as different tumors have diverse treatment approaches. This study proposes the use of fused architecture comprising two neural networks, a pre-trained ResNet-50 Convolutional Neural Network (CNN) and a tabular based network for the classification of PFT. The study included data for 158 MRI scans of 22 healthy controls and 136 pediatric patients with newly diagnosed PFT (63 Pilocytic Astrocytoma, 57 Medulloblastoma and 16 Ependymoma). The input data for classification were from magnetic resonance imaging: post contrast T1-weighted, fluid attenuated inversion recovery and diffusion Trace images, and tabular data: subject's age. Evaluation of the model was performed in a stratified 5-fold cross-validation manner, based on accuracy, precision, recall and F1 score metrics. Model explanation was performed in terms of visual explanation of the CNN by Gradient-weighted Class Activation Mapping (Grad-CAM) and by testing the contribution to the classification results of the different imaging input data sets and the proposed fused architectures relative to CNN only and tabular only architectures. The best classification results were obtained with the fused CNN + tabular data architecture, and based on diffusion Trace images, achieving mean cross-validation accuracy of 0.88 {\textpm} 0.04 for the validation and 0.87 {\textpm} 0.02 for the test dataset. Overall, the proposed architecture achieved improvement in accuracy and F1 score compared to CNN method for this dataset. The source code is available on the GitHub repository: https://github.com/artzimy/CNNTabular.},
	journal = {IEEE Access},
	author = {Artzi, Moran and Redmard, Erez and Tzemach, Oron and Zeltser, Jonathan and Gropper, Omri and Roth, Jonathan and Shofty, Ben and Kozyrev, Danil A. and Constantini, Shlomi and Ben-Sira, Liat},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Feature extraction, Training, Deep learning, Tumors, Medical diagnostic imaging, convolutional neural networks, Magnetic resonance imaging, posterior fossa tumors, Standards, tabular data, Visualization},
	pages = {91966--91973},
	annote = {citing Seven-Point Checklist and Skin Lesion Classification Using Multitask Multimodal Neural Nets},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\3TJAD64T\\9446147.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\K8VFTG44\\Artzi et al. - 2021 - Classification of Pediatric Posterior Fossa Tumors.pdf:application/pdf},
}

@article{wang_interpretability-based_2021,
	title = {Interpretability-{Based} {Multimodal} {Convolutional} {Neural} {Networks} for {Skin} {Lesion} {Diagnosis}},
	issn = {2168-2275},
	doi = {10.1109/TCYB.2021.3069920},
	abstract = {Skin lesion diagnosis is a key step for skin cancer screening, which requires high accuracy and interpretability. Though many computer-aided methods, especially deep learning methods, have made remarkable achievements in skin lesion diagnosis, their generalization and interpretability are still a challenge. To solve this issue, we propose an interpretability-based multimodal convolutional neural network (IM-CNN), which is a multiclass classification model with skin lesion images and metadata of patients as input for skin lesion diagnosis. The structure of IM-CNN consists of three main paths to deal with metadata, features extracted from segmented skin lesion with domain knowledge, and skin lesion images, respectively. We add interpretable visual modules to provide explanations for both images and metadata. In addition to area under the ROC curve (AUC), sensitivity, and specificity, we introduce a new indicator, an AUC curve with a sensitivity larger than 80\% (AUC\_SEN\_80) for performance evaluation. Extensive experimental studies are conducted on the popular HAM10000 dataset, and the results indicate that the proposed model has overwhelming advantages compared with popular deep learning models, such as DenseNet, ResNet, and other state-of-the-art models for melanoma diagnosis. The proposed multimodal model also achieves on average 72\% and 21\% improvement in terms of sensitivity and AUC\_SEN\_80, respectively, compared with the single-modal model. The visual explanations can also help gain trust from dermatologists and realize man-machine collaborations, effectively reducing the limitation of black-box models in supporting medical decision making.{\textbackslash}enlargethispage-8pt},
	journal = {IEEE Transactions on Cybernetics},
	author = {Wang, Sutong and Yin, Yunqiang and Wang, Dujuan and Wang, Yanzhang and Jin, Yaochu},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Cybernetics},
	keywords = {Convolutional neural networks, Cancer, Feature extraction, Deep learning, Medical diagnostic imaging, Lesions, Skin, interpretability, multimodal convolutional neural network, skin lesion diagnosis.},
	pages = {1--15},
	annote = {citing Seven-Point Checklist and Skin Lesion Classification Using Multitask Multimodal Neural Nets},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\ZVMJRNQ4\\9543523.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\YAACNTZU\\Wang et al. - 2021 - Interpretability-Based Multimodal Convolutional Ne.pdf:application/pdf},
}

@inproceedings{carvalho_multimodal_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multimodal {Multi}-tasking for~{Skin} {Lesion} {Classification} {Using} {Deep} {Neural} {Networks}},
	isbn = {978-3-030-90439-5},
	doi = {10.1007/978-3-030-90439-5_3},
	abstract = {Skin cancer is one of the most common types of cancer and, with its increasing incidence, accurate early diagnosis is crucial to improve prognosis of patients. In the process of visual inspection, dermatologists follow specific dermoscopic algorithms and identify important features to provide a diagnosis. This process can be automated as such characteristics can be extracted by computer vision techniques. Although deep neural networks can extract useful features from digital images for skin lesion classification, performance can be improved by providing additional information. The extracted pseudo-features can be used as input (multimodal) or output (multi-tasking) to train a robust deep learning model. This work investigates the multimodal and multi-tasking techniques for more efficient training, given the single optimization of several related tasks in the latter, and generation of better diagnosis predictions. Additionally, the role of lesion segmentation is also studied. Results show that multi-tasking improves learning of beneficial features which lead to better predictions, and pseudo-features inspired by the ABCD rule provide readily available helpful information about the skin lesion.},
	language = {en},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer International Publishing},
	author = {Carvalho, Rafaela and Pedrosa, Jo{\~a}o and Nedelcu, Tudor},
	editor = {Bebis, George and Athitsos, Vassilis and Yan, Tong and Lau, Manfred and Li, Frederick and Shi, Conglei and Yuan, Xiaoru and Mousas, Christos and Bruder, Gerd},
	year = {2021},
	keywords = {Multi-task learning, Multimodal learning, Skin lesions},
	pages = {27--38},
	annote = {citing Seven-Point Checklist and Skin Lesion Classification Using Multitask Multimodal Neural Nets},
	file = {Springer Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\VUSWA82A\\Carvalho et al. - 2021 - Multimodal Multi-tasking for~Skin Lesion Classific.pdf:application/pdf},
}

@article{tang_fusionm4net_2022,
	title = {{FusionM4Net}: {A} multi-stage multi-modal learning algorithm for multi-label skin lesion classification},
	volume = {76},
	issn = {1361-8415},
	shorttitle = {{FusionM4Net}},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521003522},
	doi = {10.1016/j.media.2021.102307},
	abstract = {Skin disease is one of the most common diseases in the world. Deep learning-based methods have achieved excellent skin lesion recognition performance, most of which are based on only dermoscopy images. In recent works that use multi-modality data (patient{\textquoteright}s meta-data, clinical images, and dermoscopy images), the methods adopt a one-stage fusion approach and only optimize the information fusion at the feature level. These methods do not use information fusion at the decision level and thus cannot fully use the data of all modalities. This work proposes a novel two-stage multi-modal learning algorithm (FusionM4Net) for multi-label skin diseases classification. At the first stage, we construct a FusionNet, which exploits and integrates the representation of clinical and dermoscopy images at the feature level, and then uses a Fusion Scheme 1 to conduct the information fusion at the decision level. At the second stage, to further incorporate the patient{\textquoteright}s meta-data, we propose a Fusion Scheme 2, which integrates the multi-label predictive information from the first stage and patient{\textquoteright}s meta-data information to train an SVM cluster. The final diagnosis is formed by the fusion of the predictions from the first and second stages. Our algorithm was evaluated on the seven-point checklist dataset, a well-established multi-modality multi-label skin disease dataset. Without using the patient{\textquoteright}s meta-data, the proposed FusionM4Net{\textquoteright}s first stage (FusionM4Net-FS) achieved an average accuracy of 75.7\% for multi-classification tasks and 74.9\% for diagnostic tasks, which is more accurate than other state-of-the-art methods. By further fusing the patient{\textquoteright}s meta-data at FusionM4Net{\textquoteright}s second stage (FusionM4Net-SS), the entire FusionM4Net finally boosts the average accuracy to 77.0\% and the diagnostic accuracy to 78.5\%, which indicates its robust and excellent classification performance on the label-imbalanced dataset. The corresponding code is available at: https://github.com/pixixiaonaogou/MLSDR.},
	language = {en},
	urldate = {2022-02-08},
	journal = {Medical Image Analysis},
	author = {Tang, Peng and Yan, Xintong and Nan, Yang and Xiang, Shao and Krammer, Sebastian and Lasser, Tobias},
	month = feb,
	year = {2022},
	keywords = {Multi-label classification, Multi-modal learning, Multi-stage information fusion, Seven-points checklist criteria, Skin disease recognition},
	pages = {102307},
	annote = {citing Seven-Point Checklist and Skin Lesion Classification Using Multitask Multimodal Neural Nets},
	file = {ScienceDirect Snapshot:C\:\\Users\\roman\\Zotero\\storage\\U6TW6A5N\\S1361841521003522.html:text/html},
}

@article{jiang_fusion_2021,
	title = {Fusion of medical imaging and electronic health records with attention and multi-head machanisms},
	url = {http://arxiv.org/abs/2112.11710},
	abstract = {Doctors often make diagonostic decisions based on patient's image scans, such as magnetic resonance imaging (MRI), and patient's electronic health records (EHR) such as age, gender, blood pressure and so on. Despite a lot of automatic methods have been proposed for either image or text analysis in computer vision or natural language research areas, much fewer studies have been developed for the fusion of medical image and EHR data for medical problems. Among existing early or intermediate fusion methods, concatenation of features from both modalities is still a mainstream. For a better exploiting of image and EHR data, we propose a multi-modal attention module which use EHR data to help the selection of important regions during image feature extraction process conducted by traditional CNN. Moreover, we propose to incorporate multi-head machnism to gated multimodal unit (GMU) to make it able to parallelly fuse image and EHR features in different subspaces. With the help of the two modules, existing CNN architecture can be enhanced using both modalities. Experiments on predicting Glasgow outcome scale (GOS) of intracerebral hemorrhage patients and classifying Alzheimer's Disease showed the proposed method can automatically focus on task-related areas and achieve better results by making better use of image and EHR features.},
	urldate = {2022-02-08},
	journal = {arXiv:2112.11710 [cs]},
	author = {Jiang, Cheng and Chen, Yihao and Chang, Jianbo and Feng, Ming and Wang, Renzhi and Yao, Jianhua},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.11710},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {citing Seven-Point Checklist and Skin Lesion Classification Using Multitask Multimodal Neural Nets},
	file = {arXiv Fulltext PDF:C\:\\Users\\roman\\Zotero\\storage\\SZBF8VXU\\Jiang et al. - 2021 - Fusion of medical imaging and electronic health re.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\roman\\Zotero\\storage\\TEHT6G3T\\2112.html:text/html},
}

@inproceedings{bonechi_fusion_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Fusion of {Visual} and {Anamnestic} {Data} for the {Classification} of {Skin} {Lesions} with {Deep} {Learning}},
	isbn = {978-3-030-30754-7},
	doi = {10.1007/978-3-030-30754-7_21},
	abstract = {Early diagnosis of skin lesions is essential for the positive outcome of the disease, which can only be resolved with surgical treatment. In this manuscript, a deep learning method is proposed for the classification of cutaneous lesions based on their visual appearance and on the patient{\textquoteright}s anamnestic data. These include age and gender of the patient and position of the lesion. The classifier discriminates between benign and malignant lesions, mimicking a typical procedure in dermatological diagnostics. Good preliminary results on the ISIC Dataset demonstrate the importance of the information fusion process, which significantly improves the classification accuracy.},
	language = {en},
	booktitle = {New {Trends} in {Image} {Analysis} and {Processing} {\textendash} {ICIAP} 2019},
	publisher = {Springer International Publishing},
	author = {Bonechi, Simone and Bianchini, Monica and Bongini, Pietro and Ciano, Giorgio and Giacomini, Giorgia and Rosai, Riccardo and Tognetti, Linda and Rossi, Alberto and Andreini, Paolo},
	editor = {Cristani, Marco and Prati, Andrea and Lanz, Oswald and Messelodi, Stefano and Sebe, Nicu},
	year = {2019},
	pages = {211--219},
	annote = {cited by Integrating Patient Data Into Skin Cancer Classification Using Convolutional Neural Networks: Systematic Review},
	file = {Springer Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\97PVD445\\Bonechi et al. - 2019 - Fusion of Visual and Anamnestic Data for the Class.pdf:application/pdf},
}

@inproceedings{li_fusing_2020,
	title = {Fusing {Metadata} and {Dermoscopy} {Images} for {Skin} {Disease} {Diagnosis}},
	doi = {10.1109/ISBI45749.2020.9098645},
	abstract = {To date, it is still difficult and challenging to automatically classify dermoscopy images. Although the state-of-the-art convolutional networks were applied to solve the classification problem and achieved overall decent prediction results, there is still room for performance improvement, especially for rare disease categories. Considering that human dermatologists often make use of other information (e.g., body locations of skin lesions) to help diagnose, we propose using both dermoscopy images and non-image metadata for intelligent diagnosis of skin diseases. Specifically, the metadata information is innovatively applied to control the importance of different types of visual information during diagnosis. Comprehensive experiments with various deep learning model architectures demonstrated the superior performance of the proposed fusion approach especially for relatively rare diseases. All our codes will be made publicly available.},
	booktitle = {2020 {IEEE} 17th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Li, Weipeng and Zhuang, Jiaxin and Wang, Ruixuan and Zhang, Jianguo and Zheng, Wei-Shi},
	month = apr,
	year = {2020},
	note = {ISSN: 1945-8452},
	keywords = {Feature extraction, Data integration, Task analysis, Skin, Metadata, Visualization, data fusion, Diseases, metadata, Skin disease classification},
	pages = {1996--2000},
	annote = {cited by Integrating Patient Data Into Skin Cancer Classification Using Convolutional Neural Networks: Systematic Review},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\U4ZTDIAN\\9098645.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\D9PBM2AN\\Li et al. - 2020 - Fusing Metadata and Dermoscopy Images for Skin Dis.pdf:application/pdf},
}

@article{pacheco_impact_2020,
	title = {The impact of patient clinical information on automated skin cancer detection},
	volume = {116},
	issn = {0010-4825},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482519304019},
	doi = {10.1016/j.compbiomed.2019.103545},
	abstract = {Skin cancer is one of the most common types of cancer worldwide. Over the past few years, different approaches have been proposed to deal with automated skin cancer detection. Nonetheless, most of them are based only on dermoscopic images and do not take into account the patient clinical information, an important clue towards clinical diagnosis. In this work, we present an approach to fill this gap. First, we introduce a new dataset composed of clinical images, collected using smartphones, and clinical data related to the patient. Next, we propose a straightforward method that includes an aggregation mechanism in well-known deep learning models to combine features from images and clinical data. Last, we carry out experiments to compare the models{\textquoteright} performance with and without using this mechanism. The results present an improvement of approximately 7\% in balanced accuracy when the aggregation method is applied. Overall, the impact of clinical data on models{\textquoteright} performance is significant and shows the importance of including these features on automated skin cancer detection.},
	language = {en},
	urldate = {2022-02-11},
	journal = {Computers in Biology and Medicine},
	author = {Pacheco, Andre G. C. and Krohling, Renato A.},
	month = jan,
	year = {2020},
	keywords = {Deep learning, Clinical images, Clinical information, Data aggregation, Skin cancer detection},
	pages = {103545},
	annote = {cited by Integrating Patient Data Into Skin Cancer Classification Using Convolutional Neural Networks: Systematic Review},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\6GWXL57L\\Pacheco und Krohling - 2020 - The impact of patient clinical information on auto.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\roman\\Zotero\\storage\\964NTKJX\\S0010482519304019.html:text/html},
}

@inproceedings{ruiz-castilla_cnn_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{CNN} and {Metadata} for {Classification} of {Benign} and {Malignant} {Melanomas}},
	isbn = {978-3-030-26969-2},
	doi = {10.1007/978-3-030-26969-2_54},
	abstract = {Skin cancer is detected in skin lesions. The most common skin cancer is melanoma. Skin cancer is increasing in several parts of the world. Due to the above, it is important to work on the classification of melanomas, in order to support the possible detection of malignant melanomas that cause skin cancer. We use Convolutional Neural Networks (CNN) for the classification of melanomas. We use images available from International Skin Imaging Collaboration (ISIC). We created a repository of 1000 images and did training with a sequential CNN to obtain two categories: benign and malignant melanomas. In the first instance we obtained results of 94.89\% accuracy and 82.25\% in validation. In the second instance we created another repository of 600 images for the method that we propose that consists in adding metadata within the same pixel matrix of the image in each RGB layer. The image was shown with a band of colors at the bottom. We made training with the CNN using images with metadata and achieved the results: 98.39\% of accuracy and 79\% of validation. Therefore, we conclude that adding the metadata repeatedly to the pixel matrix of the image improves the results of the classification.},
	language = {en},
	booktitle = {Intelligent {Computing} {Theories} and {Application}},
	publisher = {Springer International Publishing},
	author = {Ruiz-Castilla, Jos{\'e}-Sergio and Rangel-Cortes, Juan-Jos{\'e} and Garc{\'i}a-Lamont, Farid and Trueba-Espinosa, Adri{\'a}n},
	editor = {Huang, De-Shuang and Jo, Kang-Hyun and Huang, Zhi-Kai},
	year = {2019},
	keywords = {Convolutional neural networks, Classification, Metadata, Melanomas, Prediction},
	pages = {569--579},
	annote = {cited by Integrating Patient Data Into Skin Cancer Classification Using Convolutional Neural Networks: Systematic Review},
	file = {Springer Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\TABU86ZC\\Ruiz-Castilla et al. - 2019 - CNN and Metadata for Classification of Benign and .pdf:application/pdf},
}

@article{sriwong_dermatological_2019,
	title = {Dermatological {Classification} {Using} {Deep} {Learning} of {Skin} {Image} and {Patient} {Background} {Knowledge}},
	volume = {9},
	issn = {20103700},
	url = {http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=103&id=1034},
	doi = {10.18178/ijmlc.2019.9.6.884},
	abstract = {Skin cancer is one of the most common human malignancies. It is a kind of skin diseases caused by abnormal growth of skin cells. Clinically, dermatological disease including skin cancer can be divided into many types. Treatment options for each type are varying depending on the prognosis of a disease. Type of skin disease or dermatological classification is an initial process of clinical screening. Traditional method of initial clinical screening requires a visual diagnosing by specialized expertise. In case the disease is classified as a type of skin cancers, it is a serious case of dermatological disease that should be treated promptly. Therefore, an automatic approach applied for this classification task is very useful. In this work, we propose an automatic method for skin disease classification using deep learning model of convolution neural network, or CNN. In order to increase the classification performance of CNN, we employ both image data and background knowledge of the patient in the modeling process. The experimental results performed on a public dataset show that the CNN model can classify skin diseases with 79.29\% accuracy, while our proposed method to incorporate background knowledge of patient in the modeling phase can improve the accuracy up to 80.39\%.},
	language = {en},
	number = {6},
	urldate = {2022-02-11},
	journal = {International Journal of Machine Learning and Computing},
	author = {Sriwong, Kittipat and Bunrit, Supaporn and Kerdprasop, Kittisak and Kerdprasop, Nittaya},
	month = dec,
	year = {2019},
	pages = {862--867},
	annote = {cited by Integrating Patient Data Into Skin Cancer Classification Using Convolutional Neural Networks: Systematic Review},
	file = {School of Computer Engineering, SUT, 111 University Avenue, Muang, Nakhon Ratchasima 30000, Thailand et al. - 2019 - Dermatological Classification Using Deep Learning .pdf:C\:\\Users\\roman\\Zotero\\storage\\P33EPQMR\\School of Computer Engineering, SUT, 111 University Avenue, Muang, Nakhon Ratchasima 30000, Thailand et al. - 2019 - Dermatological Classification Using Deep Learning .pdf:application/pdf},
}

@inproceedings{samak_prediction_2020,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Prediction of {Thrombectomy} {Functional} {Outcomes} {Using} {Multimodal} {Data}},
	isbn = {978-3-030-52791-4},
	doi = {10.1007/978-3-030-52791-4_21},
	abstract = {Recent randomised clinical trials have shown that patients with ischaemic stroke due to occlusion of a large intracranial blood vessel benefit from endovascular thrombectomy. However, predicting outcome of treatment in an individual patient remains a challenge. We propose a novel deep learning approach to directly exploit multimodal data (clinical metadata information, imaging data, and imaging biomarkers extracted from images) to estimate the success of endovascular treatment. We incorporate an attention mechanism in our architecture to model global feature inter-dependencies, both channel-wise and spatially. We perform comparative experiments using unimodal and multimodal data, to predict functional outcome (modified Rankin Scale score, mRS) and achieve 0.75 AUC for dichotomised mRS scores and 0.35 classification accuracy for individual mRS scores.},
	language = {en},
	booktitle = {Medical {Image} {Understanding} and {Analysis}},
	publisher = {Springer International Publishing},
	author = {Samak, Zeynel A. and Clatworthy, Philip and Mirmehdi, Majid},
	editor = {Papie{\.z}, Bart{\l }omiej W. and Namburete, Ana I. L. and Yaqub, Mohammad and Noble, J. Alison},
	year = {2020},
	keywords = {Deep learning, CNN, NCCT, Prognosis, Stroke, Thrombectomy},
	pages = {267--279},
	annote = {cited by Deep Learning Classification of Cardiomegaly Using Combined Imaging and Non-imaging ICU Data},
	file = {Springer Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\9Q4NBW6N\\Samak et al. - 2020 - Prediction of Thrombectomy Functional Outcomes Usi.pdf:application/pdf},
}

@article{liu_joint_2019,
	title = {Joint {Classification} and {Regression} via {Deep} {Multi}-{Task} {Multi}-{Channel} {Learning} for {Alzheimer}'s {Disease} {Diagnosis}},
	volume = {66},
	issn = {1558-2531},
	doi = {10.1109/TBME.2018.2869989},
	abstract = {In the field of computer-aided Alzheimer's disease (AD) diagnosis, jointly identifying brain diseases and predicting clinical scores using magnetic resonance imaging (MRI) have attracted increasing attention since these two tasks are highly correlated. Most of existing joint learning approaches require hand-crafted feature representations for MR images. Since hand-crafted features of MRI and classification/regression models may not coordinate well with each other, conventional methods may lead to sub-optimal learning performance. Also, demographic information (e.g., age, gender, and education) of subjects may also be related to brain status, and thus can help improve the diagnostic performance. However, conventional joint learning methods seldom incorporate such demographic information into the learning models. To this end, we propose a deep multi-task multi-channel learning (DM2L) framework for simultaneous brain disease classification and clinical score regression, using MRI data and demographic information of subjects. Specifically, we first identify the discriminative anatomical landmarks from MR images in a data-driven manner, and then extract multiple image patches around these detected landmarks. We then propose a deep multi-task multi-channel convolutional neural network for joint classification and regression. Our DM2L framework can not only automatically learn discriminative features for MR images, but also explicitly incorporate the demographic information of subjects into the learning process. We evaluate the proposed method on four large multi-center cohorts with 1984 subjects, and the experimental results demonstrate that DM2 L is superior to several state-of-the-art joint learning methods in both the tasks of disease classification and clinical score regression.},
	number = {5},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Liu, Mingxia and Zhang, Jun and Adeli, Ehsan and Shen, Dinggang},
	month = may,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Biomedical Engineering},
	keywords = {Convolutional neural networks, Feature extraction, Task analysis, Magnetic resonance imaging, Anatomical landmark, brain disease diagnosis, classification, convolutional neural network (CNN), Dementia, Education, regression},
	pages = {1195--1206},
	annote = {cited by A hybrid machine learning/deep learning COVID-19 severity predictive model from CT images and clinical data},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\VAQLJP73\\8463559.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\QDPNMZ62\\Liu et al. - 2019 - Joint Classification and Regression via Deep Multi.pdf:application/pdf},
}

@techreport{xu_accurately_2020,
	title = {Accurately {Differentiating} {COVID}-19, {Other} {Viral} {Infection}, and {Healthy} {Individuals} {Using} {Multimodal} {Features} via {Late} {Fusion} {Learning}},
	copyright = {{\textcopyright} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.medrxiv.org/content/10.1101/2020.08.18.20176776v1},
	abstract = {Effectively identifying COVID-19 patients using non-PCR clinical data is critical for the optimal clinical outcomes. Currently, there is a lack of comprehensive understanding of various biomedical features and appropriate technical approaches to accurately detecting COVID-19 patients. In this study, we recruited 214 confirmed COVID-19 patients in non-severe (NS) and 148 in severe (S) clinical type, 198 non-infected healthy (H) participants and 129 non-COVID viral pneumonia (V) patients. The participants{\textquoteright} clinical information (23 features), lab testing results (10 features), and thoracic CT scans upon admission were acquired as three input feature modalities. To enable late fusion of multimodality data, we developed a deep learning model to extract a 10-feature high-level representation of the CT scans. Exploratory analyses showed substantial differences of all features among the four classes. Three machine learning models (k-nearest neighbor kNN, random forest RF, and support vector machine SVM) were developed based on the 43 features combined from all three modalities to differentiate four classes (NS, S, V, and H) at once. All three models had high accuracy to differentiate the overall four classes (95.4\%-97.7\%) and each individual class (90.6\%-99.9\%). Multimodal features provided substantial performance gain from using any single feature modality. Compared to existing binary classification benchmarks often focusing on single feature modality, this study provided a novel and effective breakthrough for clinical applications. Findings and the analytical workflow can be used as clinical decision support for current COVID-19 and other clinical applications with high-dimensional multimodal biomedical features.
One sentence summary We trained and validated late fusion deep learning-machine learning models to predict non-severe COVID-19, severe COVID-19, non-COVID viral infection, and healthy classes from clinical, lab testing, and CT scan features extracted from convolutional neural network and achieved predictive accuracy of {\textgreater} 96\% to differentiate all four classes at once based on a large dataset of 689 participants.},
	language = {en},
	urldate = {2022-02-11},
	institution = {medRxiv},
	author = {Xu, Ming and Ouyang, Liu and Gao, Yan and Chen, Yuanfang and Yu, Tingting and Li, Qian and Sun, Kai and Bao, Forrest Sheng and Safarnejad, Lida and Wen, Jing and Jiang, Chao and Chen, Tianyang and Han, Lei and Zhang, Hengdong and Gao, Yue and Yu, Zhengmin and Liu, Xiaowen and Yan, Tianyu and Li, Hebi and Robinson, Patrick and Zhu, Baoli and Liu, Jie and Liu, Yang and Zhang, Zengli and Ge, Yaorong and Chen, Shi},
	month = aug,
	year = {2020},
	doi = {10.1101/2020.08.18.20176776},
	note = {Type: article},
	pages = {2020.08.18.20176776},
	annote = {cited by A hybrid machine learning/deep learning COVID-19 severity predictive model from CT images and clinical data},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\IA7TDNEC\\Xu et al. - 2020 - Accurately Differentiating COVID-19, Other Viral I.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\NZFW39K6\\2020.08.18.html:text/html},
}

@article{chao_integrative_2021,
	title = {Integrative analysis for {COVID}-19 patient outcome prediction},
	volume = {67},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841520302085},
	doi = {10.1016/j.media.2020.101844},
	abstract = {While image analysis of chest computed tomography (CT) for COVID-19 diagnosis has been intensively studied, little work has been performed for image-based patient outcome prediction. Management of high-risk patients with early intervention is a key to lower the fatality rate of COVID-19 pneumonia, as a majority of patients recover naturally. Therefore, an accurate prediction of disease progression with baseline imaging at the time of the initial presentation can help in patient management. In lieu of only size and volume information of pulmonary abnormalities and features through deep learning based image segmentation, here we combine radiomics of lung opacities and non-imaging features from demographic data, vital signs, and laboratory findings to predict need for intensive care unit (ICU) admission. To our knowledge, this is the first study that uses holistic information of a patient including both imaging and non-imaging data for outcome prediction. The proposed methods were thoroughly evaluated on datasets separately collected from three hospitals, one in the United States, one in Iran, and another in Italy, with a total 295 patients with reverse transcription polymerase chain reaction (RT-PCR) assay positive COVID-19 pneumonia. Our experimental results demonstrate that adding non-imaging features can significantly improve the performance of prediction to achieve AUC up to 0.884 and sensitivity as high as 96.1\%, which can be valuable to provide clinical decision support in managing COVID-19 patients. Our methods may also be applied to other lung diseases including but not limited to community acquired pneumonia. The source code of our work is available at https://github.com/DIAL-RPI/COVID19-ICUPrediction.},
	language = {en},
	urldate = {2022-02-11},
	journal = {Medical Image Analysis},
	author = {Chao, Hanqing and Fang, Xi and Zhang, Jiajin and Homayounieh, Fatemeh and Arru, Chiara D. and Digumarthy, Subba R. and Babaei, Rosa and Mobin, Hadi K. and Mohseni, Iman and Saba, Luca and Carriero, Alessandro and Falaschi, Zeno and Pasche, Alessio and Wang, Ge and Kalra, Mannudeep K. and Yan, Pingkun},
	month = jan,
	year = {2021},
	keywords = {Artificial intelligence, Chest CT, COVID-19, Outcome prediction},
	pages = {101844},
	annote = {cited by A hybrid machine learning/deep learning COVID-19 severity predictive model from CT images and clinical data},
	file = {ScienceDirect Snapshot:C\:\\Users\\roman\\Zotero\\storage\\WIYKZEHI\\S1361841520302085.html:text/html;Volltext:C\:\\Users\\roman\\Zotero\\storage\\VT4ZKCWF\\Chao et al. - 2021 - Integrative analysis for COVID-19 patient outcome .pdf:application/pdf},
}

@inproceedings{li_multi-modal_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multi-modal {Multi}-instance {Learning} {Using} {Weakly} {Correlated} {Histopathological} {Images} and {Tabular} {Clinical} {Information}},
	isbn = {978-3-030-87237-3},
	doi = {10.1007/978-3-030-87237-3_51},
	abstract = {The fusion of heterogeneous medical data is essential in precision medicine to assist medical experts in treatment decision-making. However, there is often little explicit correlation between data from different modalities such as histopathological images and tabular clinical data. Besides, attention-based multi-instance learning (MIL) often lacks sufficient supervision to assign appropriate attention weights for informative image patches and thus generates a good global representation for the whole image. In this paper, we propose a novel multi-modal multi-instance joint learning method, which fuses different modalities and magnification scales as a cross-modal representation to capture the potential complementary information and recalibrate the features in each modality. Furthermore, we leverage the information from tabular clinical data to optimize the MIL bag representation in the imaging modality. The proposed method is evaluated on a challenging medical task, i.e., lymph node metastasis (LNM) prediction of breast cancer, and achieves the state-of-the-art performance with AUC of 0.8844, outperforming the AUC of 0.7111 using histopathological images or the AUC of 0.8312 using tabular clinical data alone. An open-source implementation of our approach can be found at https://github.com/yfzon/Multi-modal-Multi-instance-Learning.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} {\textendash} {MICCAI} 2021},
	publisher = {Springer International Publishing},
	author = {Li, Hang and Yang, Fan and Xing, Xiaohan and Zhao, Yu and Zhang, Jun and Liu, Yueping and Han, Mengxue and Huang, Junzhou and Wang, Liansheng and Yao, Jianhua},
	editor = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
	year = {2021},
	keywords = {Histopathological image analysis, Multi-modal multi-instance learning, Multi-scale},
	pages = {529--539},
	annote = {searching multimodal learning image and tabular data},
	file = {Springer Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\DDPKEZFK\\Li et al. - 2021 - Multi-modal Multi-instance Learning Using Weakly C.pdf:application/pdf},
}

@inproceedings{hamdi_marl_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{MARL}: {Multimodal} {Attentional} {Representation} {Learning} for {Disease} {Prediction}},
	isbn = {978-3-030-87156-7},
	shorttitle = {{MARL}},
	doi = {10.1007/978-3-030-87156-7_2},
	abstract = {Existing learning models often utilise CT-scan images to predict lung diseases. These models are posed by high uncertainties that affect lung segmentation and visual feature learning. We introduce MARL, a novel Multimodal Attentional Representation Learning model architecture that learns useful features from multimodal data under uncertainty. We feed the proposed model with both the lung CT-scan images and their perspective historical patients{\textquoteright} biological records collected over times. Such rich data offers to analyse both spatial and temporal aspects of the disease. MARL employs Fuzzy-based image spatial segmentation to overcome uncertainties in CT-scan images. We then utilise a pre-trained Convolutional Neural Network (CNN) to learn visual representation vectors from images. We augment patients{\textquoteright} data with statistical features from the segmented images. We develop a Long Short-Term Memory (LSTM) network to represent the augmented data and learn sequential patterns of disease progressions. Finally, we inject both CNN and LSTM feature vectors to an attention layer to help focus on the best learning features. We evaluated MARL on regression of lung disease progression and status classification. MARL outperforms state-of-the-art CNN architectures, such as EfficientNet and DenseNet, and baseline prediction models. It achieves a 91\%91\%91{\textbackslash}\% R2R2R{\textasciicircum}2 score, which is higher than the other models by a range of 8\%8\%8{\textbackslash}\% to 27\%27\%27{\textbackslash}\%. Also, MARL achieves 97\%97\%97{\textbackslash}\% and 92\%92\%92{\textbackslash}\% accuracy for binary and multi-class classification, respectively. MARL improves the accuracy of state-of-the-art CNN models with a range of 19\%19\%19{\textbackslash}\% to 57\%57\%57{\textbackslash}\%. The results show that combining spatial and sequential temporal features produces better discriminative feature.},
	language = {en},
	booktitle = {Computer {Vision} {Systems}},
	publisher = {Springer International Publishing},
	author = {Hamdi, Ali and Aboeleneen, Amr and Shaban, Khaled},
	editor = {Vincze, Markus and Patten, Timothy and Christensen, Henrik I. and Nalpantidis, Lazaros and Liu, Ming},
	year = {2021},
	keywords = {Multimodal representation learning, Deep architecture, Lung disease prediction, Visual uncertainty},
	pages = {14--27},
	annote = {searching multimodal learning image and tabular data},
	file = {Springer Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\2JLSLFAV\\Hamdi et al. - 2021 - MARL Multimodal Attentional Representation Learni.pdf:application/pdf},
}

@article{mehta_automated_2021,
	title = {Automated {Detection} of {Glaucoma} {With} {Interpretable} {Machine} {Learning} {Using} {Clinical} {Data} and {Multimodal} {Retinal} {Images}},
	volume = {231},
	issn = {0002-9394},
	url = {https://www.sciencedirect.com/science/article/pii/S0002939421002208},
	doi = {10.1016/j.ajo.2021.04.021},
	abstract = {Purpose
To develop a multimodal model to automate glaucoma detection
Design
Development of a machine-learning glaucoma detection model
Methods
We selected a study cohort from the UK Biobank data set with 1193 eyes of 863 healthy subjects and 1283 eyes of 771 subjects with glaucoma. We trained a multimodal model that combines multiple deep neural nets, trained on macular optical coherence tomography volumes and color fundus photographs, with demographic and clinical data. We performed an interpretability analysis to identify features the model relied on to detect glaucoma. We determined the importance of different features in detecting glaucoma using interpretable machine learning methods. We also evaluated the model on subjects who did not have a diagnosis of glaucoma on the day of imaging but were later diagnosed (progress-to-glaucoma [PTG]).
Results
Results show that a multimodal model that combines imaging with demographic and clinical features is highly accurate (area under the curve 0.97). Interpretation of this model highlights biological features known to be related to the disease, such as age, intraocular pressure, and optic disc morphology. Our model also points to previously unknown or disputed features, such as pulmonary function and retinal outer layers. Accurate prediction in PTG highlights variables that change with progression to glaucoma{\textemdash}age and pulmonary function.
Conclusions
The accuracy of our model suggests distinct sources of information in each imaging modality and in the different clinical and demographic variables. Interpretable machine learning methods elucidate subject-level prediction and help uncover the factors that lead to accurate predictions, pointing to potential disease mechanisms or variables related to the disease.},
	language = {en},
	urldate = {2022-02-12},
	journal = {American Journal of Ophthalmology},
	author = {Mehta, Parmita and Petersen, Christine A. and Wen, Joanne C. and Banitt, Michael R. and Chen, Philip P. and Bojikian, Karine D. and Egan, Catherine and Lee, Su-In and Balazinska, Magdalena and Lee, Aaron Y. and Rokem, Ariel},
	month = nov,
	year = {2021},
	pages = {154--169},
	annote = {searching multimodal learning image and tabular data},
	file = {Eingereichte Version:C\:\\Users\\roman\\Zotero\\storage\\KRILP7Q7\\Mehta et al. - 2021 - Automated Detection of Glaucoma With Interpretable.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\roman\\Zotero\\storage\\HKW9ESXK\\S0002939421002208.html:text/html},
}

@article{kang_prediction_2017,
	title = {Prediction of crime occurrence from multi-modal data using deep learning},
	volume = {12},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0176244},
	doi = {10.1371/journal.pone.0176244},
	abstract = {In recent years, various studies have been conducted on the prediction of crime occurrences. This predictive capability is intended to assist in crime prevention by facilitating effective implementation of police patrols. Previous studies have used data from multiple domains such as demographics, economics, and education. Their prediction models treat data from different domains equally. These methods have problems in crime occurrence prediction, such as difficulty in discovering highly nonlinear relationships, redundancies, and dependencies between multiple datasets. In order to enhance crime prediction models, we consider environmental context information, such as broken windows theory and crime prevention through environmental design. In this paper, we propose a feature-level data fusion method with environmental context based on a deep neural network (DNN). Our dataset consists of data collected from various online databases of crime statistics, demographic and meteorological data, and images in Chicago, Illinois. Prior to generating training data, we select crime-related data by conducting statistical analyses. Finally, we train our DNN, which consists of the following four kinds of layers: spatial, temporal, environmental context, and joint feature representation layers. Coupled with crucial data extracted from various domains, our fusion DNN is a product of an efficient decision-making process that statistically analyzes data redundancy. Experimental performance results show that our DNN model is more accurate in predicting crime occurrence than other prediction models.},
	language = {en},
	number = {4},
	urldate = {2022-02-12},
	journal = {PLOS ONE},
	author = {Kang, Hyeon-Woo and Kang, Hang-Bong},
	month = apr,
	year = {2017},
	note = {Publisher: Public Library of Science},
	keywords = {Deep learning, Census, Crime, Economics of training and education, Forecasting, Police, Statistical data, Violent crime},
	pages = {e0176244},
	annote = {searching multimodal learning image and tabular data deep learning},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\ER2JL29X\\Kang und Kang - 2017 - Prediction of crime occurrence from multi-modal da.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\RAJVPGAW\\article.html:text/html},
}

@article{lee_predicting_2019,
	title = {Predicting {Alzheimer}{\textquoteright}s disease progression using multi-modal deep learning approach},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-37769-z},
	doi = {10.1038/s41598-018-37769-z},
	abstract = {Alzheimer{\textquoteright}s disease (AD) is a progressive neurodegenerative condition marked by a decline in cognitive functions with no validated disease modifying treatment. It is critical for timely treatment to detect AD in its earlier stage before clinical manifestation. Mild cognitive impairment (MCI) is an intermediate stage between cognitively normal older adults and AD. To predict conversion from MCI to probable AD, we applied a deep learning approach, multimodal recurrent neural network. We developed an integrative framework that combines not only cross-sectional neuroimaging biomarkers at baseline but also longitudinal cerebrospinal fluid (CSF) and cognitive performance biomarkers obtained from the Alzheimer{\textquoteright}s Disease Neuroimaging Initiative cohort (ADNI). The proposed framework integrated longitudinal multi-domain data. Our results showed that 1) our prediction model for MCI conversion to AD yielded up to 75\% accuracy (area under the curve (AUC) = 0.83) when using only single modality of data separately; and 2) our prediction model achieved the best performance with 81\% accuracy (AUC = 0.86) when incorporating longitudinal multi-domain data. A multi-modal deep learning approach has potential to identify persons at risk of developing AD who might benefit most from a clinical trial or as a stratification approach within clinical trials.},
	language = {en},
	number = {1},
	urldate = {2022-02-12},
	journal = {Scientific Reports},
	author = {Lee, Garam and Nho, Kwangsik and Kang, Byungkon and Sohn, Kyung-Ah and Kim, Dokyoon},
	month = feb,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Data integration},
	pages = {1952},
	annote = {searching multimodal learning image and tabular data deep learning},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\K85PGGT5\\Lee et al. - 2019 - Predicting Alzheimer{\textquoteright}s disease progression using m.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\RMWC24LR\\s41598-018-37769-z.html:text/html},
}

@article{li_mining_2017,
	title = {Mining {Fashion} {Outfit} {Composition} {Using} an {End}-to-{End} {Deep} {Learning} {Approach} on {Set} {Data}},
	volume = {19},
	issn = {1941-0077},
	doi = {10.1109/TMM.2017.2690144},
	abstract = {Composing fashion outfits involves deep under-standing of fashion standards while incorporating creativity for choosing multiple fashion items (e.g., jewelry, bag, pants, dress). In fashion websites, popular or high-quality fashion outfits are usually designed by fashion experts and followed by large audiences. In this paper, we propose a machine learning system to compose fashion outfits automatically. The core of the proposed automatic composition system is to score fashion outfit candidates based on the appearances and metadata. We propose to leverage outfit popularity on fashion-oriented websites to supervise the scoring component. The scoring component is a multimodal multiinstance deep learning system that evaluates instance aesthetics and set compatibility simultaneously. In order to train and evaluate the proposed composition system, we have collected a large-scale fashion outfit dataset with 195K outfits and 368K fashion items from Polyvore. Although the fashion outfit scoring and composition is rather challenging, we have achieved an AUC of 85\% for the scoring component, and an accuracy of 77\% for a constrained composition task.},
	number = {8},
	journal = {IEEE Transactions on Multimedia},
	author = {Li, Yuncheng and Cao, Liangliang and Zhu, Jiang and Luo, Jiebo},
	month = aug,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Multimedia},
	keywords = {Machine learning, Visualization, Big data applications, Context, Footwear, Image coding, multilayer neural network, multimedia computing, Silicon},
	pages = {1946--1955},
	annote = {searching multimodal learning image and tabular data deep learning},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\NFYYZB5H\\7890387.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\78TL89UH\\Li et al. - 2017 - Mining Fashion Outfit Composition Using an End-to-.pdf:application/pdf},
}

@article{badgeley_deep_2019,
	title = {Deep learning predicts hip fracture using confounding patient and healthcare variables},
	volume = {2},
	copyright = {2019 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-019-0105-1},
	doi = {10.1038/s41746-019-0105-1},
	abstract = {Hip fractures are a leading cause of death and disability among older adults. Hip fractures are also the most commonly missed diagnosis on pelvic radiographs, and delayed diagnosis leads to higher cost and worse outcomes. Computer-aided diagnosis (CAD) algorithms have shown promise for helping radiologists detect fractures, but the image features underpinning their predictions are notoriously difficult to understand. In this study, we trained deep-learning models on 17,587 radiographs to classify fracture, 5 patient traits, and 14 hospital process variables. All 20 variables could be individually predicted from a radiograph, with the best performances on scanner model (AUC = 1.00), scanner brand (AUC = 0.98), and whether the order was marked {\textquotedblleft}priority{\textquotedblright} (AUC = 0.79). Fracture was predicted moderately well from the image (AUC = 0.78) and better when combining image features with patient data (AUC = 0.86, DeLong paired AUC comparison, p = 2e-9) or patient data plus hospital process features (AUC = 0.91, p = 1e-21). Fracture prediction on a test set that balanced fracture risk across patient variables was significantly lower than a random test set (AUC = 0.67, DeLong unpaired AUC comparison, p = 0.003); and on a test set with fracture risk balanced across patient and hospital process variables, the model performed randomly (AUC = 0.52, 95\% CI 0.46{\textendash}0.58), indicating that these variables were the main source of the model{\textquoteright}s fracture predictions. A single model that directly combines image features, patient, and hospital process data outperforms a Naive Bayes ensemble of an image-only model prediction, patient, and hospital process data. If CAD algorithms are inexplicably leveraging patient and process variables in their predictions, it is unclear how radiologists should interpret their predictions in the context of other known patient data. Further research is needed to illuminate deep-learning decision processes so that computers and clinicians can effectively cooperate.},
	language = {en},
	number = {1},
	urldate = {2022-02-12},
	journal = {npj Digital Medicine},
	author = {Badgeley, Marcus A. and Zech, John R. and Oakden-Rayner, Luke and Glicksberg, Benjamin S. and Liu, Manway and Gale, William and McConnell, Michael V. and Percha, Bethany and Snyder, Thomas M. and Dudley, Joel T.},
	month = apr,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computer science, Radiography, Statistics},
	pages = {1--10},
	annote = {searching multimodal learning image and tabular data deep learning},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\FUFPNKAF\\Badgeley et al. - 2019 - Deep learning predicts hip fracture using confound.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\9D578PWF\\s41746-019-0105-1.html:text/html},
}

@article{heiliger_beyond_2022,
	title = {Beyond {Medical} {Imaging} - {A} {Review} of {Multimodal} {Deep} {Learning} in {Radiology}},
	url = {https://www.techrxiv.org/articles/preprint/Beyond_Medical_Imaging_-_A_Review_of_Multimodal_Deep_Learning_in_Radiology/19103432/1},
	doi = {10.36227/techrxiv.19103432.v1},
	abstract = {Healthcare data are inherently multimodal. Almost all data generated and acquired during a patient{\textquoteright}s life can be hypothesized to contain information relevant to providing optimal personalized healthcare. Data sources such as ECGs, doctor{\textquoteright}s notes, histopathological and radiological images all contribute to inform a physician{\textquoteright}s treatment decision. However, most machine learning methods in healthcare focus on single-modality data. This becomes particularly apparent within the field of radiology, which, due to its information density, accessibility, and computational interpretability, constitutes a central pillar in the healthcare data landscape and traditionally has been one of the key target areas of medically-focused machine learning. Computer-assisted diagnostic systems of the future should be capable of simultaneously processing multimodal data, thereby mimicking physicians, who also consider a multitude of resources when treating patients. Before this background, this review offers a comprehensive assessment of multimodal machine learning methods that combine data from radiology and other medical disciplines. It establishes a modality-based taxonomy, discusses common architectures and design principles, evaluation approaches, challenges, and future directions. This work will enable researchers and clinicians to understand the topography of the domain, describe the state-of-the-art, and detect research gaps for future research in multimodal medical machine learning.},
	language = {en},
	urldate = {2022-02-12},
	author = {Heiliger, Lars and Sekuboyina, Anjany and Menze, Bjoern and Egger, Jan and Kleesiek, Jens},
	month = feb,
	year = {2022},
	note = {Publisher: TechRxiv},
	annote = {searching multimodal learning image and tabular data deep learning},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\52JNHXU9\\Heiliger et al. - 2022 - Beyond Medical Imaging - A Review of Multimodal De.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\ZQKE6SZF\\19103432.html:text/html},
}

@article{gomez_giraldo_melanoma_2021,
	title = {Melanoma {Classification}},
	copyright = {info:eu-repo/semantics/openAccess},
	url = {http://bibliotecadigital.udea.edu.co/handle/10495/24997},
	abstract = {ABSTRACT : We presented our solution for the SIIM-ISIC melanoma classification challenge. This is a multi-class multi-modal classification model using images and metadata and, we tested both binary and multi-class image-only models and a binary multi-modal model. The keys to success for our solution were the selection of the target variable, using the available metadata, and the data augmentation strategy. Achieving AUC values of 0.95 and F1 of 0.71 for the validation data.},
	language = {eng},
	urldate = {2022-02-14},
	author = {G{\'o}mez Giraldo, Oscar Nicol{\'a}s and Arbel{\'a}ez L{\'o}pez, N{\'e}stor Iv{\'a}n},
	year = {2021},
	note = {Accepted: 2021-12-13T18:05:33Z
Publisher: Medell{\'i}n},
	annote = {searching metadata image and tabular data deep learning},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\JW8TCC6I\\G{\'o}mez Giraldo und Arbel{\'a}ez L{\'o}pez - 2021 - Melanoma Classification.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\KY7Q6VLH\\24997.html:text/html},
}

@article{venugopalan_multimodal_2021,
	title = {Multimodal deep learning models for early detection of {Alzheimer}{\textquoteright}s disease stage},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-020-74399-w},
	doi = {10.1038/s41598-020-74399-w},
	abstract = {Most current Alzheimer{\textquoteright}s disease (AD) and mild cognitive disorders (MCI) studies use single data modality to make predictions such as AD stages. The fusion of multiple data modalities can provide a holistic view of AD staging analysis. Thus, we use deep learning~(DL) to integrally analyze imaging (magnetic resonance imaging~(MRI)), genetic (single nucleotide polymorphisms (SNPs)), and clinical test data to classify patients into AD, MCI, and controls~(CN). We use stacked denoising auto-encoders to extract features from clinical and genetic data, and use 3D-convolutional neural networks (CNNs) for imaging data.~We also develop a novel data interpretation method to identify top-performing features learned by the deep-models with clustering and perturbation analysis. Using Alzheimer{\textquoteright}s disease neuroimaging initiative (ADNI) dataset, we demonstrate that deep models outperform shallow models, including support vector machines, decision trees, random forests, and k-nearest neighbors. In addition, we demonstrate that integrating multi-modality data outperforms single modality models in terms of accuracy, precision, recall, and meanF1 scores. Our models have identified hippocampus, amygdala brain areas, and the~Rey Auditory Verbal Learning Test (RAVLT) as top distinguished features, which are consistent with the known AD literature.},
	language = {en},
	number = {1},
	urldate = {2022-02-15},
	journal = {Scientific Reports},
	author = {Venugopalan, Janani and Tong, Li and Hassanzadeh, Hamid Reza and Wang, May D.},
	month = feb,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Data integration, Data mining},
	pages = {3254},
	annote = {searching multimodal deep learning},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\YWFMNVMF\\Venugopalan et al. - 2021 - Multimodal deep learning models for early detectio.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\AG8J6Q5K\\s41598-020-74399-w.html:text/html},
}

@inproceedings{polsterl_combining_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Combining {3D} {Image} and {Tabular} {Data} via the {Dynamic} {Affine} {Feature} {Map} {Transform}},
	isbn = {978-3-030-87240-3},
	doi = {10.1007/978-3-030-87240-3_66},
	abstract = {Prior work on diagnosing Alzheimer{\textquoteright}s disease from magnetic resonance images of the brain established that convolutional neural networks (CNNs) can leverage the high-dimensional image information for classifying patients. However, little research focused on how these models can utilize the usually low-dimensional tabular information, such as patient demographics or laboratory measurements. We introduce the Dynamic Affine Feature Map Transform (DAFT), a general-purpose module for CNNs that dynamically rescales and shifts the feature maps of a convolutional layer, conditional on a patient{\textquoteright}s tabular clinical information. We show that DAFT is highly effective in combining 3D image and tabular information for diagnosis and time-to-dementia prediction, where it outperforms competing CNNs with a mean balanced accuracy of 0.622 and mean c-index of 0.748, respectively. Our extensive ablation study provides valuable insights into the architectural properties of DAFT. Our implementation is available at https://github.com/ai-med/DAFT.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} {\textendash} {MICCAI} 2021},
	publisher = {Springer International Publishing},
	author = {P{\"o}lsterl, Sebastian and Wolf, Tom Nuno and Wachinger, Christian},
	editor = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
	year = {2021},
	pages = {688--698},
	annote = {searching image and tabular data deep learning~},
	file = {Springer Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\24VDGDZ7\\P{\"o}lsterl et al. - 2021 - Combining 3D Image and Tabular Data via the Dynami.pdf:application/pdf},
}

@article{cao_multi-modality_2021,
	title = {Multi-modality fusion learning for the automatic diagnosis of optic neuropathy},
	volume = {142},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865520304402},
	doi = {10.1016/j.patrec.2020.12.009},
	abstract = {Optic neuropathy is kind of common eye diseases, which usually causes irreversible vision loss. Early diagnosis is key to saving patients{\textquoteright} vision. Due to the similar early clinical manifestations of common optic neuropathy, it may cause misdiagnosis and delays in treatment. Worse, most diagnoses rely on experienced doctors. In this paper, we proposed a novel deep learning architecture GroupFusionNet (GFN) to diagnose five normal optic neuropathy diseases, including Anterior Ischemic Optic Neuropathy (AION), papilledema, papillitis, Optic Disc Vasculitis (ODV), and optic atrophy (OA). Specifically, we combined multi-modalities in clinic examination such as fundus image, visual field tests and age of each patient. GFN utilized two ResNet pathways to extract and fuse both features of fundus image and visual field tests, and the information of structured data was embedded in the end. Experimental results demonstrate that multi-modality feature aggregation is effective for optic neuropathy diseases diagnosis, and GFN achieved a five-classes classification accuracy of 87.82\% on the test dataset.},
	language = {en},
	urldate = {2022-02-18},
	journal = {Pattern Recognition Letters},
	author = {Cao, Zheng and Sun, Chuanbin and Wang, Wenzhe and Zheng, Xiangshang and Wu, Jian and Gao, Honghao},
	month = feb,
	year = {2021},
	keywords = {Computer-aided diagnosis, Deep learning, Multi-modality, Optic neuropathy},
	pages = {58--64},
	annote = {searching multi-modality fusion image and tabular data deep learning},
	file = {ScienceDirect Snapshot:C\:\\Users\\roman\\Zotero\\storage\\FLZTWV9X\\S0167865520304402.html:text/html},
}

@inproceedings{chen_multimodal_2021,
	title = {Multimodal {Co}-{Attention} {Transformer} for {Survival} {Prediction} in {Gigapixel} {Whole} {Slide} {Images}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Multimodal_Co-Attention_Transformer_for_Survival_Prediction_in_Gigapixel_Whole_Slide_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-02-23},
	author = {Chen, Richard J. and Lu, Ming Y. and Weng, Wei-Hung and Chen, Tiffany Y. and Williamson, Drew F. K. and Manz, Trevor and Shady, Maha and Mahmood, Faisal},
	year = {2021},
	pages = {4015--4025},
	annote = {searching multimodal attention image and tabular data deep learning},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\KJZ84ZGD\\Chen et al. - 2021 - Multimodal Co-Attention Transformer for Survival P.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\P97QFYHG\\Chen_Multimodal_Co-Attention_Transformer_for_Survival_Prediction_in_Gigapixel_Whole_Slide_ICCV_.html:text/html},
}

@article{antonelli_integrating_2019,
	title = {Integrating imaging and omics data: {A} review},
	volume = {52},
	issn = {1746-8094},
	shorttitle = {Integrating imaging and omics data},
	url = {https://www.sciencedirect.com/science/article/pii/S1746809419301326},
	doi = {10.1016/j.bspc.2019.04.032},
	abstract = {We refer to omics imaging as an emerging interdisciplinary field concerned with the integration of data collected from biomedical images and omics analyses. Bringing together information coming from different sources, it permits to reveal hidden genotype{\textendash}phenotype relationships, with the aim of better understanding the onset and progression of many diseases, and identifying new diagnostic and prognostic biomarkers. More in detail, biomedical images, generated by anatomical or functional techniques, are processed to extract hundreds of numerical features describing visual aspects {\textendash} as in solid cancer imaging {\textendash} or functional elements {\textendash} as in neuroimaging. These imaging features are then complemented and integrated with genotypic and phenotypic information, such as DNA mutations, RNA expression levels, and protein abundances. Apart from the difficulties arising from imaging and omics analyses alone, the process of integrating, combining, processing, and making sense of the omics imaging data is quite challenging, owed to the heterogeneity of the sources, the high dimensionality of the resulting feature space, and the reduced availability of freely accessible, large, and well-curated datasets containing both images and omics data for each sample. In this review, we present the state of the art of omics imaging, with the aim of providing the interested reader a unique source of information, with links for further detailed information. Based on the existing literature, we describe both the omics and imaging data that have been adopted, provide a list of curated databases of integrated resources, discuss the types of adopted features, give hints on the used data analysis methods, and overview current research in this field.},
	language = {en},
	urldate = {2022-03-01},
	journal = {Biomedical Signal Processing and Control},
	author = {Antonelli, Laura and Guarracino, Mario Rosario and Maddalena, Lucia and Sangiovanni, Mara},
	month = jul,
	year = {2019},
	keywords = {Biomedical imaging, Radiogenomics, Imaging genomics, Omics data, Omics imaging},
	pages = {264--280},
	annote = {searching omics image and tabular data deep learning},
	file = {ScienceDirect Snapshot:C\:\\Users\\roman\\Zotero\\storage\\VRTW59ZN\\S1746809419301326.html:text/html},
}

@inproceedings{smedley_using_2018,
	title = {Using deep neural networks for radiogenomic analysis},
	doi = {10.1109/ISBI.2018.8363864},
	abstract = {Radiogenomic studies have suggested that biological heterogeneity of tumors is reflected radiographically through visible features on magnetic resonance (MR) images. We apply deep learning techniques to map between tumor gene expression profiles and tumor morphology in pre-operative MR studies of glioblastoma patients. A deep autoencoder was trained on 528 patients, each with 12,042 gene expressions. Then, the autoencoder's weights were used to initialize a supervised deep neural network. The supervised model was trained using a subset of 109 patients with both gene and MR data. For each patient, 20 morphological image features were extracted from contrast-enhancing and peritumoral edema regions. We found that neural network pre-trained with an autoencoder and dropout had lower errors than linear regression in predicting tumor morphology features by an average of 16.98\% mean absolute percent error and 0.0114 mean absolute error, where several features were significantly different (adjusted p-value {\textless}; 0.05). These results indicate neural networks, which can incorporate nonlinear, hierarchical relationships between gene expressions, may have the representational power to find more predictive radiogenomic associations than pairwise or linear methods.},
	booktitle = {2018 {IEEE} 15th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2018)},
	author = {Smedley, Nova F. and Hsu, William},
	month = apr,
	year = {2018},
	note = {ISSN: 1945-8452},
	keywords = {Gene expression, Tumors, Imaging, Biological neural networks, deep neural networks, gene expression, glioblastoma, Image segmentation, Linear regression, magnetic resonance imaging, radiogenomics},
	pages = {1529--1533},
	annote = {cited by Integrating Imaging and omics data: A revie},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\L8VXX2NL\\8363864.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\B74CXL3S\\Smedley und Hsu - 2018 - Using deep neural networks for radiogenomic analys.pdf:application/pdf},
}

@article{holzinger_why_2019,
	title = {Why imaging data alone is not enough: {AI}-based integration of imaging, omics, and clinical data},
	volume = {46},
	issn = {1619-7089},
	shorttitle = {Why imaging data alone is not enough},
	url = {https://doi.org/10.1007/s00259-019-04382-9},
	doi = {10.1007/s00259-019-04382-9},
	abstract = {Artificial intelligence (AI) is currently regaining enormous interest due to the success of machine learning (ML), and in particular deep learning (DL). Image analysis, and thus radiomics, strongly benefits from this research. However, effectively and efficiently integrating diverse clinical, imaging, and molecular profile data is necessary to understand complex diseases, and to achieve accurate diagnosis in order to provide the best possible treatment. In addition to the need for sufficient computing resources, suitable algorithms, models, and data infrastructure, three important aspects are often neglected: (1) the need for multiple independent, sufficiently large and, above all, high-quality data sets; (2) the need for domain knowledge and ontologies; and (3) the requirement for multiple networks that provide relevant relationships among biological entities. While one will always get results out of high-dimensional data, all three aspects are essential to provide robust training and validation of ML models, to provide explainable hypotheses and results, and to achieve the necessary trust in AI and confidence for clinical applications.},
	language = {en},
	number = {13},
	urldate = {2022-03-01},
	journal = {European Journal of Nuclear Medicine and Molecular Imaging},
	author = {Holzinger, Andreas and Haibe-Kains, Benjamin and Jurisica, Igor},
	month = dec,
	year = {2019},
	pages = {2722--2730},
	annote = {searching imaging-omics fusion},
	file = {Springer Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\KQJRWDKI\\Holzinger et al. - 2019 - Why imaging data alone is not enough AI-based int.pdf:application/pdf},
}

@article{holzinger_towards_2021,
	title = {Towards multi-modal causability with {Graph} {Neural} {Networks} enabling information fusion for explainable {AI}},
	volume = {71},
	issn = {1566-2535},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253521000142},
	doi = {10.1016/j.inffus.2021.01.008},
	abstract = {AI is remarkably successful and outperforms human experts in certain tasks, even in complex domains such as medicine. Humans on the other hand are experts at multi-modal thinking and can embed new inputs almost instantly into a conceptual knowledge space shaped by experience. In many fields the aim is to build systems capable of explaining themselves, engaging in interactive what-if questions. Such questions, called counterfactuals, are becoming important in the rising field of explainable AI (xAI). Our central hypothesis is that using conceptual knowledge as a guiding model of reality will help to train more explainable, more robust and less biased machine learning models, ideally able to learn from fewer data. One important aspect in the medical domain is that various modalities contribute to one single result. Our main question is {\textquotedblleft}How can we construct a multi-modal feature representation space (spanning images, text, genomics data) using knowledge bases as an initial connector for the development of novel explanation interface techniques?{\textquotedblright}. In this paper we argue for using Graph Neural Networks as a method-of-choice, enabling information fusion for multi-modal causability (causability {\textendash} not to confuse with causality {\textendash} is the measurable extent to which an explanation to a human expert achieves a specified level of causal understanding). The aim of this paper is to motivate the international xAI community to further work into the fields of multi-modal embeddings and interactive explainability, to lay the foundations for effective future human{\textendash}AI interfaces. We emphasize that Graph Neural Networks play a major role for multi-modal causability, since causal links between features can be defined directly using graph structures.},
	language = {en},
	urldate = {2022-03-02},
	journal = {Information Fusion},
	author = {Holzinger, Andreas and Malle, Bernd and Saranti, Anna and Pfeifer, Bastian},
	month = jul,
	year = {2021},
	keywords = {Counterfactuals, Explainable AI, Graph Neural Networks, Information fusion, Knowledge graphs, Multi-modal causability, xAI},
	pages = {28--37},
	annote = {searching imaging-omics fusion~},
	file = {ScienceDirect Snapshot:C\:\\Users\\roman\\Zotero\\storage\\TMVRXGU5\\S1566253521000142.html:text/html},
}

@article{lu_deep-learningbased_2020,
	title = {Deep-{Learning}{\textendash}{Based} {Characterization} of {Tumor}-{Infiltrating} {Lymphocytes} in {Breast} {Cancers} {From} {Histopathology} {Images} and {Multiomics} {Data}},
	url = {https://ascopubs.org/doi/full/10.1200/CCI.19.00126},
	doi = {10.1200/CCI.19.00126},
	abstract = {PURPOSE

Tumor-infiltrating lymphocytes (TILs) and their spatial characterizations on whole-slide images (WSIs) of histopathology sections have become crucial in diagnosis, prognosis, and treatment response prediction for different cancers. However, fully automatic assessment of TILs on WSIs currently remains a great challenge because of the heterogeneity and large size of WSIs. We present an automatic pipeline based on a cascade-training U-net to generate high-resolution TIL maps on WSIs.

METHODS

We present global cell-level TIL maps and 43 quantitative TIL spatial image features for 1,000 WSIs of The Cancer Genome Atlas patients with breast cancer. For more specific analysis, all the patients were divided into three subtypes, namely, estrogen receptor (ER){\textendash}positive, ER-negative, and triple-negative groups. The associations between TIL scores and gene expression and somatic mutation were examined separately in three breast cancer subtypes. Both univariate and multivariate survival analyses were performed on 43 TIL image features to examine the prognostic value of TIL spatial patterns in different breast cancer subtypes.

RESULTS

The TIL score was in strong association with immune response pathway and genes (eg, programmed death-1 and CLTA4). Different breast cancer subtypes showed TIL score in association with mutations from different genes suggesting that different genetic alterations may lead to similar phenotypes. Spatial TIL features that represent density and distribution of TIL clusters were important indicators of the patient outcomes.

CONCLUSION

Our pipeline can facilitate computational pathology-based discovery in cancer immunology and research on immunotherapy. Our analysis results are available for the research community to generate new hypotheses and insights on breast cancer immunology and development.},
	number = {4},
	urldate = {2022-03-08},
	journal = {JCO Clinical Cancer Informatics},
	author = {Lu, Zixiao and Xu, Siwen and Shao, Wei and Wu, Yi and Zhang, Jie and Han, Zhi and Feng, Qianjin and Huang, Kun},
	month = nov,
	year = {2020},
	note = {Publisher: Wolters Kluwer},
	pages = {480--490},
	annote = {searching omics image and tabular data deep learning~
~},
	file = {Volltext:C\:\\Users\\roman\\Zotero\\storage\\J35PIQVN\\Lu et al. - 2020 - Deep-Learning{\textendash}Based Characterization of Tumor-Infi.pdf:application/pdf},
}

@inproceedings{polsterl_combining_2021-1,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Combining {3D} {Image} and {Tabular} {Data} via the {Dynamic} {Affine} {Feature} {Map} {Transform}},
	isbn = {978-3-030-87240-3},
	doi = {10.1007/978-3-030-87240-3_66},
	abstract = {Prior work on diagnosing Alzheimer{\textquoteright}s disease from magnetic resonance images of the brain established that convolutional neural networks (CNNs) can leverage the high-dimensional image information for classifying patients. However, little research focused on how these models can utilize the usually low-dimensional tabular information, such as patient demographics or laboratory measurements. We introduce the Dynamic Affine Feature Map Transform (DAFT), a general-purpose module for CNNs that dynamically rescales and shifts the feature maps of a convolutional layer, conditional on a patient{\textquoteright}s tabular clinical information. We show that DAFT is highly effective in combining 3D image and tabular information for diagnosis and time-to-dementia prediction, where it outperforms competing CNNs with a mean balanced accuracy of 0.622 and mean c-index of 0.748, respectively. Our extensive ablation study provides valuable insights into the architectural properties of DAFT. Our implementation is available at https://github.com/ai-med/DAFT.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} {\textendash} {MICCAI} 2021},
	publisher = {Springer International Publishing},
	author = {P{\"o}lsterl, Sebastian and Wolf, Tom Nuno and Wachinger, Christian},
	editor = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
	year = {2021},
	pages = {688--698},
	file = {Springer Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\QITEZ6MQ\\P{\"o}lsterl et al. - 2021 - Combining 3D Image and Tabular Data via the Dynami.pdf:application/pdf},
}

@article{bori_artificial_2021,
	title = {An artificial intelligence model based on the proteomic profile of euploid embryos and blastocyst morphology: a preliminary study},
	volume = {42},
	issn = {1472-6491},
	shorttitle = {An artificial intelligence model based on the proteomic profile of euploid embryos and blastocyst morphology},
	doi = {10.1016/j.rbmo.2020.09.031},
	abstract = {RESEARCH QUESTION: The study aimed to develop an artificial intelligence model based on artificial neural networks (ANNs) to predict the likelihood of achieving a live birth using the proteomic profile of spent culture media and blastocyst morphology.
DESIGN: This retrospective cohort study included 212 patients who underwent single blastocyst transfer at IVI Valencia. A single image of each of 186 embryos was studied, and the protein profile was analysed in 81 samples of spent embryo culture medium from patients included in the preimplantation genetic testing programme. The information extracted from the analyses was used as input data for the ANN. The multilayer perceptron and the back-propagation learning method were used to train the ANN. Finally, predictive power was measured using the area under the curve (AUC) of the receiver operating characteristic curve.
RESULTS: Three ANN architectures classified most of the embryos correctly as leading (LB+) or not leading (LB-) to a live birth: 100.0\% for ANN1 (morphological variables and two proteins), 85.7\% for ANN2 (morphological variables and seven proteins), and 83.3\% for ANN3 (morphological variables and 25 proteins). The artificial intelligence model using information extracted from blastocyst image analysis and concentrations of interleukin-6 and matrix metalloproteinase-1 was able to predict live birth with an AUC of 1.0.
CONCLUSIONS: The model proposed in this preliminary report may provide a promising tool to select the embryo most likely to lead to a live birth in a euploid cohort. The accuracy of prediction demonstrated by this software may improve the efficacy of an assisted reproduction treatment by reducing the number of transfers per patient. Prospective studies are, however, needed.},
	language = {eng},
	number = {2},
	journal = {Reproductive Biomedicine Online},
	author = {Bori, Lorena and Dominguez, Francisco and Fernandez, Eleonora Inacio and Del Gallego, Raquel and Alegre, Lucia and Hickman, Cristina and Qui{\~n}onero, Alicia and Nogueira, Marcelo Fabio Gouveia and Rocha, Jose Celso and Meseguer, Marcos},
	month = feb,
	year = {2021},
	pmid = {33279421},
	keywords = {Artificial intelligence, Adult, Artificial neural network, Blastocyst, Blastocyst morphology, Female, Humans, Live birth, Live Birth, Neural Networks, Computer, Pregnancy, Proteome, Proteomics, Retrospective Studies},
	pages = {340--350},
	annote = {searching genomics image and tabular data deep learning~},
}

@techreport{treeck_deepmed_2021,
	title = {{DeepMed}: {A} unified, modular pipeline for end-to-end deep learning in computational pathology},
	copyright = {{\textcopyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {{DeepMed}},
	url = {https://www.biorxiv.org/content/10.1101/2021.12.19.473344v1},
	abstract = {The interpretation of digitized histopathology images has been transformed thanks to artificial intelligence (AI). End-to-end AI algorithms can infer high-level features directly from raw image data, extending the capabilities of human experts. In particular, AI can predict tumor subtypes, genetic mutations and gene expression directly from hematoxylin and eosin (H\&E) stained pathology slides. However, existing end-to-end AI workflows are poorly standardized and not easily adaptable to new tasks. Here, we introduce DeepMed, a Python library for predicting any high-level attribute directly from histopathological whole slide images alone, or from images coupled with additional meta-data (https://github.com/KatherLab/deepmed). Unlike earlier computational pipelines, DeepMed is highly developer-friendly: its structure is modular and separates preprocessing, training, deployment, statistics, and visualization in such a way that any one of these processes can be altered without affecting the others. Also, DeepMed scales easily from local use on laptop computers to multi-GPU clusters in cloud computing services and therefore can be used for teaching, prototyping and for large-scale applications. Finally, DeepMed is user-friendly and allows researchers to easily test multiple hypotheses in a single dataset (via cross-validation) or in multiple datasets (via external validation). Here, we demonstrate and document DeepMed{\textquoteright}s abilities to predict molecular alterations, histopathological subtypes and molecular features from routine histopathology images, using a large benchmark dataset which we release publicly. In summary, DeepMed is a fully integrated and broadly applicable end-to-end AI pipeline for the biomedical research community.},
	language = {en},
	urldate = {2022-03-15},
	institution = {bioRxiv},
	author = {Treeck, Marko van and Cifci, Didem and Laleh, Narmin Ghaffari and Saldanha, Oliver Lester and Loeffler, Chiara M. L. and Hewitt, Katherine J. and Muti, Hannah Sophie and Echle, Amelie and Seibel, Tobias and Seraphin, Tobias Paul and Trautwein, Christian and Foersch, Sebastian and Luedde, Tom and Truhn, Daniel and Kather, Jakob Nikolas},
	month = dec,
	year = {2021},
	doi = {10.1101/2021.12.19.473344},
	note = {Section: New Results
Type: article},
	pages = {2021.12.19.473344},
	annote = {searching gene expression image and tabular data deep learning~},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\KYKKX88F\\Treeck et al. - 2021 - DeepMed A unified, modular pipeline for end-to-en.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\6673QXKX\\2021.12.19.473344v1.html:text/html},
}

@misc{noauthor_neural_nodate,
	title = {Neural {Networks} - {A} {Systematic} {Introduction}},
	url = {https://page.mi.fu-berlin.de/rojas/neural/},
	urldate = {2022-03-27},
	file = {Neural Networks - A Systematic Introduction:C\:\\Users\\roman\\Zotero\\storage\\8UJEBCGE\\neural.html:text/html},
}

@book{rojas_neural_1996,
	address = {Berlin},
	title = {Neural {Networks} - {A} {Systematic} {Introduction}},
	language = {en},
	publisher = {Springer},
	author = {Rojas, Ra{\'u}l},
	year = {1996},
	file = {London - 1996 - A Systematic Introduction.pdf:C\:\\Users\\roman\\Zotero\\storage\\R8JVAWE6\\London - 1996 - A Systematic Introduction.pdf:application/pdf},
}

@article{gallant_perceptron-based_1990,
	title = {Perceptron-based learning algorithms},
	volume = {1},
	issn = {1941-0093},
	doi = {10.1109/72.80230},
	abstract = {A key task for connectionist research is the development and analysis of learning algorithms. An examination is made of several supervised learning algorithms for single-cell and network models. The heart of these algorithms is the pocket algorithm, a modification of perceptron learning that makes perceptron learning well-behaved with nonseparable training data, even if the data are noisy and contradictory. Features of these algorithms include speed algorithms fast enough to handle large sets of training data; network scaling properties, i.e. network methods scale up almost as well as single-cell models when the number of inputs is increased; analytic tractability, i.e. upper bounds on classification error are derivable; online learning, i.e. some variants can learn continually, without referring to previous data; and winner-take-all groups or choice groups, i.e. algorithms can be adapted to select one out of a number of possible classifications. These learning algorithms are suitable for applications in machine learning, pattern recognition, and connectionist expert systems.{\textless}{\textgreater}},
	number = {2},
	journal = {IEEE Transactions on Neural Networks},
	author = {Gallant, S.I.},
	month = jun,
	year = {1990},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {Machine learning, Algorithm design and analysis, Classification algorithms, Heart, Hybrid intelligent systems, Machine learning algorithms, Pattern recognition, Supervised learning, Training data, Upper bound},
	pages = {179--191},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\EECMFNDJ\\80230.html:text/html},
}

@article{han_augmented_2020,
	title = {Augmented {Intelligence} {Dermatology}: {Deep} {Neural} {Networks} {Empower} {Medical} {Professionals} in {Diagnosing} {Skin} {Cancer} and~{Predicting} {Treatment} {Options} for 134 {Skin}~{Disorders}},
	volume = {140},
	issn = {1523-1747},
	shorttitle = {Augmented {Intelligence} {Dermatology}},
	doi = {10.1016/j.jid.2020.01.019},
	abstract = {Although deep learning algorithms have demonstrated expert-level performance, previous efforts were mostly binary classifications of limited disorders. We trained an algorithm with 220,680 images of 174 disorders and validated it using Edinburgh (1,300 images; 10 disorders) and SNU datasets (2,201 images; 134 disorders). The algorithm could accurately predict malignancy, suggest primary treatment options, render multi-class classification among 134 disorders, and improve the performance of medical professionals. The area under the curves for malignancy detection were 0.928 {\textpm} 0.002 (Edinburgh) and 0.937 {\textpm} 0.004 (SNU). The area under the curves of primary treatment suggestion (SNU) were 0.828 {\textpm} 0.012, 0.885 {\textpm} 0.006, 0.885 {\textpm} 0.006, and 0.918 {\textpm} 0.006 for steroids, antibiotics, antivirals, and antifungals, respectively. For multi-class classification, the mean top-1 and top-5 accuracies were 56.7 {\textpm} 1.6\% and 92.0 {\textpm} 1.1\% (Edinburgh) and 44.8 {\textpm} 1.2\% and 78.1 {\textpm} 0.3\% (SNU), respectively. With the assistance of our algorithm, the sensitivity and specificity of 47 clinicians (21 dermatologists and 26 dermatology residents) for malignancy prediction (SNU; 240 images) were improved by 12.1\% (P {\textless} 0.0001) and 1.1\% (P {\textless} 0.0001), respectively. The malignancy prediction sensitivity of 23 non-medical professionals was significantly increased by 83.8\% (P {\textless} 0.0001). The top-1 and top-3 accuracies of four doctors in the multi-class classification of 134 diseases (SNU; 2,201 images) were increased by 7.0\% (P~= 0.045) and 10.1\% (P~= 0.0020), respectively. The results suggest that our algorithm may serve as augmented intelligence that can empower medical professionals in diagnostic dermatology.},
	language = {eng},
	number = {9},
	journal = {The Journal of Investigative Dermatology},
	author = {Han, Seung Seog and Park, Ilwoo and Eun Chang, Sung and Lim, Woohyung and Kim, Myoung Shin and Park, Gyeong Hun and Chae, Je Byeong and Huh, Chang Hun and Na, Jung-Im},
	month = sep,
	year = {2020},
	pmid = {32243882},
	keywords = {Deep Learning, Skin, Adult, Female, Humans, Adolescent, Aged, Anti-Bacterial Agents, Antifungal Agents, Antiviral Agents, Clinical Competence, Datasets as Topic, Dermatologists, Dermatology, Dermoscopy, Drug Therapy, Computer-Assisted, Feasibility Studies, Glucocorticoids, Image Interpretation, Computer-Assisted, Internship and Residency, Male, Middle Aged, Photography, ROC Curve, Skin Diseases, Skin Neoplasms, Young Adult},
	pages = {1753--1761},
	file = {Volltext:C\:\\Users\\roman\\Zotero\\storage\\IEI93MRT\\Han et al. - 2020 - Augmented Intelligence Dermatology Deep Neural Ne.pdf:application/pdf},
}

@article{maron_systematic_2019,
	title = {Systematic outperformance of 112 dermatologists in multiclass skin cancer image classification by convolutional neural networks},
	volume = {119},
	issn = {1879-0852},
	doi = {10.1016/j.ejca.2019.06.013},
	abstract = {BACKGROUND: Recently, convolutional neural networks (CNNs) systematically outperformed dermatologists in distinguishing dermoscopic melanoma and nevi images. However, such a binary classification does not reflect the clinical reality of skin cancer screenings in which multiple diagnoses need to be taken into account.
METHODS: Using 11,444 dermoscopic images, which covered dermatologic diagnoses comprising the majority of commonly pigmented skin lesions commonly faced in skin cancer screenings, a CNN was trained through novel deep learning techniques. A test~set of 300 biopsy-verified images was used to compare the classifier's performance with that of 112 dermatologists from 13 German university hospitals. The primary end-point was the correct classification of the different lesions into benign and malignant. The secondary end-point was the correct classification of the images into one of the five diagnostic categories.
FINDINGS: Sensitivity and specificity of dermatologists for the primary end-point were 74.4\% (95\% confidence interval [CI]: 67.0-81.8\%) and 59.8\% (95\% CI: 49.8-69.8\%), respectively. At equal sensitivity, the algorithm achieved a specificity of 91.3\% (95\% CI: 85.5-97.1\%). For the secondary end-point, the mean sensitivity and specificity of the dermatologists were at 56.5\% (95\% CI: 42.8-70.2\%) and 89.2\% (95\% CI: 85.0-93.3\%), respectively. At equal sensitivity, the algorithm achieved a specificity of 98.8\%. Two-sided McNemar tests revealed significance for the primary end-point (p~{\textless}~0.001). For the secondary end-point, outperformance (p~{\textless}~0.001) was achieved except for basal cell carcinoma (on-par performance).
INTERPRETATION: Our findings show that automated classification of dermoscopic melanoma and nevi images is extendable to a multiclass classification problem, thus better reflecting clinical differential diagnoses, while still outperforming dermatologists at a significant level (p~{\textless}~0.001).},
	language = {eng},
	journal = {European Journal of Cancer (Oxford, England: 1990)},
	author = {Maron, Roman C. and Weichenthal, Michael and Utikal, Jochen S. and Hekler, Achim and Berking, Carola and Hauschild, Axel and Enk, Alexander H. and Haferkamp, Sebastian and Klode, Joachim and Schadendorf, Dirk and Jansen, Philipp and Holland-Letz, Tim and Schilling, Bastian and von Kalle, Christof and Fr{\"o}hling, Stefan and Gaiser, Maria R. and Hartmann, Daniela and Gesierich, Anja and K{\"a}hler, Katharina C. and Wehkamp, Ulrike and Karoglan, Ante and B{\"a}r, Claudia and Brinker, Titus J. and {Collabrators}},
	month = sep,
	year = {2019},
	pmid = {31419752},
	keywords = {Skin cancer, Artificial intelligence, Female, Humans, Neural Networks, Computer, Dermatologists, Dermoscopy, Male, Skin Neoplasms, Algorithms, Biopsy, Diagnosis, Differential, Hospitals, University, Melanoma, Nevus, Sensitivity and Specificity, Skin cancer screening, Surveys and Questionnaires},
	pages = {57--65},
	file = {Volltext:C\:\\Users\\roman\\Zotero\\storage\\QTQMSSLD\\Maron et al. - 2019 - Systematic outperformance of 112 dermatologists in.pdf:application/pdf},
}

@article{brinker_deep_2019,
	title = {Deep learning outperformed 136 of 157 dermatologists in a head-to-head dermoscopic melanoma image classification task},
	volume = {113},
	issn = {1879-0852},
	doi = {10.1016/j.ejca.2019.04.001},
	abstract = {BACKGROUND: Recent studies have successfully demonstrated the use of deep-learning algorithms for dermatologist-level classification of suspicious lesions by the use of excessive proprietary image databases and limited numbers of dermatologists. For the first time, the performance of a deep-learning algorithm trained by open-source images exclusively is compared to a large number of dermatologists covering all levels within the clinical hierarchy.
METHODS: We used methods from enhanced deep learning to train a convolutional neural network (CNN) with 12,378 open-source dermoscopic images. We used 100 images to compare the performance of the CNN to that of the 157 dermatologists from 12 university hospitals in Germany. Outperformance of dermatologists by the deep neural network was measured in terms of sensitivity, specificity~and receiver operating characteristics.
FINDINGS: The mean sensitivity and specificity achieved by the dermatologists with dermoscopic images was 74.1\% (range 40.0\%-100\%) and 60\% (range 21.3\%-91.3\%), respectively. At a mean sensitivity of 74.1\%, the CNN exhibited a mean specificity of 86.5\% (range 70.8\%-91.3\%). At a mean specificity of 60\%, a mean sensitivity of 87.5\% (range 80\%-95\%) was achieved by our algorithm. Among the dermatologists, the chief physicians showed the highest mean specificity of 69.2\% at a mean sensitivity of 73.3\%. With the same high specificity of 69.2\%, the CNN had a mean sensitivity of 84.5\%.
INTERPRETATION: A CNN trained by open-source images exclusively outperformed 136 of the 157 dermatologists and all the different levels of experience (from junior to chief physicians) in terms of average specificity and sensitivity.},
	language = {eng},
	journal = {European Journal of Cancer (Oxford, England: 1990)},
	author = {Brinker, Titus J. and Hekler, Achim and Enk, Alexander H. and Klode, Joachim and Hauschild, Axel and Berking, Carola and Schilling, Bastian and Haferkamp, Sebastian and Schadendorf, Dirk and Holland-Letz, Tim and Utikal, Jochen S. and von Kalle, Christof and {Collaborators}},
	month = may,
	year = {2019},
	pmid = {30981091},
	keywords = {Deep Learning, Skin cancer, Artificial intelligence, Humans, Dermatologists, Dermoscopy, Skin Neoplasms, Hospitals, University, Melanoma, Nevus, Sensitivity and Specificity, Germany},
	pages = {47--54},
	file = {Volltext:C\:\\Users\\roman\\Zotero\\storage\\BM72EPG5\\Brinker et al. - 2019 - Deep learning outperformed 136 of 157 dermatologis.pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	note = {{\textbackslash}url\{http://www.deeplearningbook.org\}},
}

@book{goodfellow_deep_2016-1,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	annote = {http://www.deeplearningbook.org},
}

@misc{noauthor_springer_nodate,
	title = {springer cite ebook without page number - {Google} {Search}},
	url = {https://www.google.com/search?client=firefox-b-d&q=springer+cite+ebook+without+page+number},
	urldate = {2022-04-25},
	file = {springer cite ebook without page number - Google Search:C\:\\Users\\roman\\Zotero\\storage\\FYDRSKLD\\search.html:text/html},
}

@article{noauthor_image_2020,
	title = {Image {Classification} using {HOG} and {LBP} {Feature} {Descriptors} with {SVM} and {CNN}},
	volume = {8},
	abstract = {Image recognition and classification plays an important role in many applications, like driverless cars and online shopping. We present the classification of FashionMNIST (F-MNIST) dataset using two important classifiers SVM (Support Vector Machine) and CNN (Convolutional Neural Networks). In the first model two feature descriptors HOG (Histogram of Oriented Gradient) and Local Binary Pattern (LBP) with multiclass SVM. In this paper we explore the impact of various feature descriptors and classifiers on Fashion products classification tasks. We have used one of the most simple and effective single feature descriptor HOG. The multiclass SVM which is one of the best machine learning classifier algorithms is used in this method to train the images. In computer vision Convolutional Neural Networks (CNN or ConvNet) are the default deep learning model used for image classification problems. Selecting appropriate technique for feature extraction and choosing a best classifier algorithm remains a big challenging task for attaining good classification accuracy. However, the experimental results show that impressive results on this new benchmarking dataset FMNIST.},
	language = {en},
	number = {04},
	journal = {International Journal of Engineering Research},
	year = {2020},
	pages = {5},
	file = {2020 - Image Classification using HOG and LBP Feature Des.pdf:C\:\\Users\\roman\\Zotero\\storage\\CCPVW85F\\2020 - Image Classification using HOG and LBP Feature Des.pdf:application/pdf},
}

@article{noauthor_image_2020-1,
	title = {Image {Classification} using {HOG} and {LBP} {Feature} {Descriptors} with {SVM} and {CNN}},
	volume = {8},
	abstract = {Image recognition and classification plays an important role in many applications, like driverless cars and online shopping. We present the classification of FashionMNIST (F-MNIST) dataset using two important classifiers SVM (Support Vector Machine) and CNN (Convolutional Neural Networks). In the first model two feature descriptors HOG (Histogram of Oriented Gradient) and Local Binary Pattern (LBP) with multiclass SVM. In this paper we explore the impact of various feature descriptors and classifiers on Fashion products classification tasks. We have used one of the most simple and effective single feature descriptor HOG. The multiclass SVM which is one of the best machine learning classifier algorithms is used in this method to train the images. In computer vision Convolutional Neural Networks (CNN or ConvNet) are the default deep learning model used for image classification problems. Selecting appropriate technique for feature extraction and choosing a best classifier algorithm remains a big challenging task for attaining good classification accuracy. However, the experimental results show that impressive results on this new benchmarking dataset FMNIST.},
	language = {en},
	number = {04},
	journal = {International Journal of Engineering Research},
	year = {2020},
	pages = {5},
	file = {2020 - Image Classification using HOG and LBP Feature Des.pdf:C\:\\Users\\roman\\Zotero\\storage\\YFAS7JNY\\2020 - Image Classification using HOG and LBP Feature Des.pdf:application/pdf},
}

@inproceedings{greeshma_image_2020,
	title = {Image classification using {HOG} and {LBP} feature descriptors with {SVM} and {CNN}},
	booktitle = {Int {J} {Eng} {Res} {Technol} ({IJERT}). {NSDARM}-2020 conference proceedings},
	author = {Greeshma, KV and Gripsy, J Viji},
	year = {2020},
}

@article{wang_assessment_2019,
	title = {Assessment of {Deep} {Learning} {Using} {Nonimaging} {Information} and {Sequential} {Medical} {Records} to {Develop} a {Prediction} {Model} for {Nonmelanoma} {Skin} {Cancer}},
	volume = {155},
	issn = {2168-6068},
	url = {https://doi.org/10.1001/jamadermatol.2019.2335},
	doi = {10.1001/jamadermatol.2019.2335},
	abstract = {A prediction model for new-onset nonmelanoma skin cancer could enhance prevention measures, but few patient data{\textendash}driven tools exist for more accurate prediction.To use machine learning to develop a prediction model for incident nonmelanoma skin cancer based on large-scale, multidimensional, nonimaging medical information.This study used a database comprising 2 million randomly sampled patients from the Taiwan National Health Insurance Research Database from January 1, 1999, to December 31, 2013. A total of 1829 patients with nonmelanoma skin cancer as their first diagnosed cancer and 7665 random controls without cancer were included in the analysis. A convolutional neural network, a deep learning approach, was used to develop a risk prediction model. This risk prediction model used 3-year clinical diagnostic information, medical records, and temporal-sequential information to predict the skin cancer risk of a given patient within the next year. Stepwise feature selection was also performed to investigate important and determining factors of the model. Statistical analysis was performed from November 1, 2016, to October 31, 2018.Sensitivity, specificity, and area under the receiver operating characteristic (AUROC) curve were used to evaluate the performance of the models.A total of 1829 patients (923 women [50.5\%] and 906 men [49.5\%]; mean [SD] age, 65.3 [15.7] years) with nonmelanoma skin cancer and 7665 random controls without cancer (3951 women [51.5\%] and 3714 men [48.4\%]; mean [SD] age, 47.5 [17.3] years) were included in the analysis. The 1-year incident nonmelanoma skin cancer risk prediction model using sequential diagnostic information and drug prescription information as a time-incorporated feature matrix could attain an AUROC of 0.89 (95\% CI, 0.87-0.91), with a mean (SD) sensitivity of 83.1\% (3.5\%) and mean (SD) specificity of 82.3\% (4.1\%). Carcinoma in situ of skin (AUROC, 0.867; {\textendash}2.80\% loss) and other chronic comorbidities (eg, degenerative osteopathy [AUROC, 0.872; {\textendash}2.32\% loss], hypertension [AUROC, 0.879; {\textendash}1.53\% loss], and chronic kidney insufficiency [AUROC, 0.879; {\textendash}1.52\% loss]) served as more discriminative factors for the prediction. Medications such as trazodone, acarbose, systemic antifungal agents, statins, nonsteroidal anti-inflammatory drugs, and thiazide diuretics were the top-ranking discriminative features in the model; each led to more than a 1\% decrease of the AUROC when eliminated individually (eg, trazodone AUROC, 0.868; -2.67\% reduction; acarbose AUROC, 0.870; -2.50 reduction; and systemic antifungal agents AUROC, 0.875; -1.99 reduction).The findings of this study suggest that a risk prediction model may have potential predictive factors for nonmelanoma skin cancer. This model may help health care professionals target high-risk populations for more intensive skin cancer preventive methods.},
	number = {11},
	urldate = {2022-04-28},
	journal = {JAMA Dermatology},
	author = {Wang, Hsiao-Han and Wang, Yu-Hsiang and Liang, Chia-Wei and Li, Yu-Chuan},
	month = nov,
	year = {2019},
	pages = {1277--1283},
	file = {Snapshot:C\:\\Users\\roman\\Zotero\\storage\\GN2DAB6B\\2749356.html:text/html;Volltext:C\:\\Users\\roman\\Zotero\\storage\\BY3TAHRP\\Wang et al. - 2019 - Assessment of Deep Learning Using Nonimaging Infor.pdf:application/pdf},
}

@misc{noauthor_predicting_nodate,
	title = {Predicting non-melanoma skin cancer via a multi-parameterized artificial neural network {\textbar} {Scientific} {Reports}},
	url = {https://www.nature.com/articles/s41598-018-19907-9},
	urldate = {2022-04-28},
	file = {Predicting non-melanoma skin cancer via a multi-parameterized artificial neural network | Scientific Reports:C\:\\Users\\roman\\Zotero\\storage\\BUH3QFM2\\s41598-018-19907-9.html:text/html},
}

@article{wang_assessment_2019-1,
	title = {Assessment of {Deep} {Learning} {Using} {Nonimaging} {Information} and {Sequential} {Medical} {Records} to {Develop} a {Prediction} {Model} for {Nonmelanoma} {Skin} {Cancer}},
	volume = {155},
	issn = {2168-6068},
	url = {https://jamanetwork.com/journals/jamadermatology/fullarticle/2749356},
	doi = {10.1001/jamadermatol.2019.2335},
	abstract = {OBJECTIVE To use machine learning to develop a prediction model for incident nonmelanoma skin cancer based on large-scale, multidimensional, nonimaging medical information. DESIGN, SETTING, AND PARTICIPANTS This study used a database comprising 2 million randomly sampled patients from the Taiwan National Health Insurance Research Database from January 1, 1999, to December 31, 2013. A total of 1829 patients with nonmelanoma skin cancer as their first diagnosed cancer and 7665 random controls without cancer were included in the analysis. A convolutional neural network, a deep learning approach, was used to develop a risk prediction model. This risk prediction model used 3-year clinical diagnostic information, medical records, and temporal-sequential information to predict the skin cancer risk of a given patient within the next year. Stepwise feature selection was also performed to investigate important and determining factors of the model. Statistical analysis was performed from November 1, 2016, to October 31, 2018. MAIN OUTCOMES AND MEASURES Sensitivity, specificity, and area under the receiver operating characteristic (AUROC) curve were used to evaluate the performance of the models.
RESULTS A total of 1829 patients (923 women [50.5\%] and 906 men [49.5\%]; mean [SD] age, 65.3 [15.7] years) with nonmelanoma skin cancer and 7665 random controls without cancer (3951 women [51.5\%] and 3714 men [48.4\%]; mean [SD] age, 47.5 [17.3] years) were included in the analysis. The 1-year incident nonmelanoma skin cancer risk prediction model using sequential diagnostic information and drug prescription information as a time-incorporated feature matrix could attain an AUROC of 0.89 (95\% CI, 0.87-0.91), with a mean (SD) sensitivity of 83.1\% (3.5\%) and mean (SD) specificity of 82.3\% (4.1\%). Carcinoma in situ of skin (AUROC, 0.867; {\textendash}2.80\% loss) and other chronic comorbidities (eg, degenerative osteopathy [AUROC, 0.872; {\textendash}2.32\% loss], hypertension [AUROC, 0.879; {\textendash}1.53\% loss], and chronic kidney insufficiency [AUROC, 0.879; {\textendash}1.52\% loss]) served as more discriminative factors for the prediction. Medications such as trazodone, acarbose, systemic antifungal agents, statins, nonsteroidal anti-inflammatory drugs, and thiazide diuretics were the top-ranking discriminative features in the model; each led to more than a 1\% decrease of the AUROC when eliminated individually (eg, trazodone AUROC, 0.868; -2.67\% reduction; acarbose AUROC, 0.870; -2.50 reduction; and systemic antifungal agents AUROC, 0.875; -1.99 reduction).
CONCLUSIONS AND RELEVANCE The findings of this study suggest that a risk prediction model may have potential predictive factors for nonmelanoma skin cancer. This model may help health care professionals target high-risk populations for more intensive skin cancer preventive methods.},
	language = {en},
	number = {11},
	urldate = {2022-04-28},
	journal = {JAMA Dermatology},
	author = {Wang, Hsiao-Han and Wang, Yu-Hsiang and Liang, Chia-Wei and Li, Yu-Chuan},
	month = nov,
	year = {2019},
	pages = {1277},
	file = {Wang et al. - 2019 - Assessment of Deep Learning Using Nonimaging Infor.pdf:C\:\\Users\\roman\\Zotero\\storage\\Q7K97NKA\\Wang et al. - 2019 - Assessment of Deep Learning Using Nonimaging Infor.pdf:application/pdf},
}

@article{leslie_influence_2000,
	title = {The influence of clinical information on the reporting of {CT} by radiologists},
	volume = {73},
	issn = {0007-1285},
	doi = {10.1259/bjr.73.874.11271897},
	abstract = {The aim of the study was to determine whether clinical information alters the CT report. This prospective blinded study consisted of 50 consecutive patients who attended a Department of Radiology for CT. Each study was interpreted by two of three consultant radiologists, before and after knowledge of the clinical information. 19 reports were changed after clinical information was known. Clinical follow-up was available in 15 cases. In ten cases the reports were more accurate after clinical information and in five cases the reports were less accurate. In three of the five cases where accuracy was reduced, the clinical information was incorrect. It was concluded that clinical information affects the CT report. If the information is accurate it has a beneficial effect; if it is inaccurate it has a detrimental effect. The more complex the investigation, the more important the clinical information. There was a correlation between readers regarding the influence of clinical information. Correct clinical information therefore improves the radiology report. It is the responsibility of the clinician to provide this information in an accurate and legible form.},
	language = {eng},
	number = {874},
	journal = {The British Journal of Radiology},
	author = {Leslie, A. and Jones, A. J. and Goddard, P. R.},
	month = oct,
	year = {2000},
	pmid = {11271897},
	keywords = {Adult, Female, Humans, Adolescent, Aged, Male, Middle Aged, Sensitivity and Specificity, Aged, 80 and over, Decision Making, Health Knowledge, Attitudes, Practice, Medical History Taking, Medical Records, Observer Variation, Professional Practice, Prospective Studies, Radiology, Tomography, X-Ray Computed},
	pages = {1052--1055},
}

@misc{noauthor_radiologist_nodate,
	title = {Radiologist {Use} of and {Perceived} {Need} for {Patient} {Data} {Access} - {PMC}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3043710/},
	urldate = {2022-04-28},
}

@article{boonn_radiologist_2009,
	title = {Radiologist use of and perceived need for patient data access},
	volume = {22},
	issn = {1618-727X},
	url = {https://pubmed.ncbi.nlm.nih.gov/18459002},
	doi = {10.1007/s10278-008-9115-2},
	abstract = {Given the increasing volume of radiological exams, the decreasing frequency of direct communication with the referring provider, and the distribution of patient data over many clinical systems, radiologists often do not have adequate clinical information at the time of interpretation. We have performed a survey of radiologists to determine the need and actual utilization of patient data at the time of image interpretation. Our findings demonstrate that most radiologists want more clinical information when interpreting images and that this information would impact their report, but they are discouraged by the time it takes to access this information. In addition, current mechanisms for monitoring necessary patient follow-up are inadequate.},
	language = {eng},
	number = {4},
	journal = {Journal of digital imaging},
	author = {Boonn, William W and Langlotz, Curtis P},
	month = aug,
	year = {2009},
	note = {Edition: 2008/05/06
Publisher: Springer-Verlag},
	keywords = {Humans, Surveys and Questionnaires, *Interdisciplinary Communication, *Medical Records, *Radiology Information Systems},
	pages = {357--362},
}

@article{ramachandram_deep_2017,
	title = {Deep {Multimodal} {Learning}: {A} {Survey} on {Recent} {Advances} and {Trends}},
	volume = {34},
	issn = {1558-0792},
	shorttitle = {Deep {Multimodal} {Learning}},
	doi = {10.1109/MSP.2017.2738401},
	abstract = {The success of deep learning has been a catalyst to solving increasingly complex machine-learning problems, which often involve multiple data modalities. We review recent advances in deep multimodal learning and highlight the state-of the art, as well as gaps and challenges in this active research field. We first classify deep multimodal learning architectures and then discuss methods to fuse learned multimodal representations in deep-learning architectures. We highlight two areas of research-regularization strategies and methods that learn or optimize multimodal fusion structures-as exciting areas for future work.},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Ramachandram, Dhanesh and Taylor, Graham W.},
	month = nov,
	year = {2017},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Machine learning, Training data, Computer architecture, Emotion recognition, Face recognition, Sensors},
	pages = {96--108},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\3ZBK4B55\\8103116.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\IVQQAUZD\\Ramachandram und Taylor - 2017 - Deep Multimodal Learning A Survey on Recent Advan.pdf:application/pdf},
}

@article{kiela_efficient_2018,
	title = {Efficient {Large}-{Scale} {Multi}-{Modal} {Classification}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11945},
	abstract = {While the incipient internet was largely text-based, the modern digital world is becoming increasingly multi-modal. Here, we examine multi-modal classification where one modality is discrete, e.g. text, and the other is continuous, e.g. visual representations transferred from a convolutional neural network. In particular, we focus on scenarios where we have to be able to classify large quantities of data quickly. We investigate various methods for performing multi-modal fusion and analyze their trade-offs in terms of classification accuracy and computational efficiency. Our findings indicate that the inclusion of continuous information improves performance over text-only on a range of multi-modal classification tasks, even with simple fusion methods. In addition, we experiment with discretizing the continuous features in order to speed up and simplify the fusion process even further. Our results show that fusion with discretized features outperforms text-only classification, at a fraction of the computational cost of full multi-modal fusion, with the additional benefit of improved interpretability.},
	language = {en},
	number = {1},
	urldate = {2022-04-30},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Kiela, Douwe and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	month = apr,
	year = {2018},
	note = {Number: 1},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\NRRH6MZ8\\Kiela et al. - 2018 - Efficient Large-Scale Multi-Modal Classification.pdf:application/pdf},
}

@article{karpathy_large-scale_nodate,
	title = {Large-scale {Video} {Classification} with {Convolutional} {Neural} {Networks}},
	abstract = {Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on largescale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3\% to 63.9\%), but only a surprisingly modest improvement compared to single-frame models (59.3\% to 60.9\%). We further study the generalization performance of our best model by retraining the top layers on the UCF101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3\% up from 43.9\%).},
	language = {en},
	author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
	pages = {8},
	file = {Karpathy et al. - Large-scale Video Classification with Convolutional.pdf:C\:\\Users\\roman\\Zotero\\storage\\EGMJJ9KK\\Karpathy et al. - Large-scale Video Classification with Convolutional.pdf:application/pdf},
}

@article{neverova_moddrop_2015,
	title = {{ModDrop}: adaptive multi-modal gesture recognition},
	shorttitle = {{ModDrop}},
	url = {http://arxiv.org/abs/1501.00102},
	abstract = {We present a method for gesture detection and localisation based on multi-scale and multi-modal deep learning. Each visual modality captures spatial information at a particular spatial scale (such as motion of the upper body or a hand), and the whole system operates at three temporal scales. Key to our technique is a training strategy which exploits: i) careful initialization of individual modalities; and ii) gradual fusion involving random dropping of separate channels (dubbed ModDrop) for learning cross-modality correlations while preserving uniqueness of each modality-specific representation. We present experiments on the ChaLearn 2014 Looking at People Challenge gesture recognition track, in which we placed first out of 17 teams. Fusing multiple modalities at several spatial and temporal scales leads to a significant increase in recognition rates, allowing the model to compensate for errors of the individual classifiers as well as noise in the separate channels. Futhermore, the proposed ModDrop training technique ensures robustness of the classifier to missing signals in one or several channels to produce meaningful predictions from any number of available modalities. In addition, we demonstrate the applicability of the proposed fusion scheme to modalities of arbitrary nature by experiments on the same dataset augmented with audio.},
	language = {en},
	urldate = {2022-05-03},
	journal = {arXiv:1501.00102 [cs]},
	author = {Neverova, Natalia and Wolf, Christian and Taylor, Graham W. and Nebout, Florian},
	month = jun,
	year = {2015},
	note = {arXiv: 1501.00102},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction},
	annote = {Comment: 14 pages, 7 figures},
	file = {Neverova et al. - 2015 - ModDrop adaptive multi-modal gesture recognition.pdf:C\:\\Users\\roman\\Zotero\\storage\\WIGK4V6S\\Neverova et al. - 2015 - ModDrop adaptive multi-modal gesture recognition.pdf:application/pdf},
}

@article{thung_multi-stage_2017,
	title = {Multi-stage {Diagnosis} of {Alzheimer}'s {Disease} with {Incomplete} {Multimodal} {Data} via {Multi}-task {Deep} {Learning}},
	volume = {10553},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5666687/},
	doi = {10.1007/978-3-319-67558-9_19},
	abstract = {Utilization of biomedical data from multiple modalities improves the diagnostic accuracy of neurodegenerative diseases. However, multi-modality data are often incomplete because not all data can be collected for every individual. When using such incomplete data for diagnosis, current approaches for addressing the problem of missing data, such as imputation, matrix completion and multi-task learning, implicitly assume linear data-to-label relationship, therefore limiting their performances. We thus propose multi-task deep learning for incomplete data, where prediction tasks that are associated with different modality combinations are learnt jointly to improve the performance of each task. Specifically, we devise a multi-input multi-output deep learning framework, and train our deep network subnet-wise, partially updating its weights based on the availability of modality data. The experimental results using the ADNI dataset show that our method outperforms the state-of-the-art methods.},
	urldate = {2022-05-04},
	journal = {Deep learning in medical image analysis and multimodal learning for clinical decision support : third International Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, held in conjunction with MICCAI 2017, Quebec City, QC.},
	author = {Thung, Kim-Han and Yap, Pew-Thian and Shen, Dinggang},
	month = sep,
	year = {2017},
	pmid = {29104963},
	pmcid = {PMC5666687},
	pages = {160--168},
	file = {Akzeptierte Version:C\:\\Users\\roman\\Zotero\\storage\\TVLCQ9FL\\Thung et al. - 2017 - Multi-stage Diagnosis of Alzheimer's Disease with .pdf:application/pdf},
}

@article{hyun_machine-learning_2019,
	title = {A {Machine}-{Learning} {Approach} {Using} {PET}-{Based} {Radiomics} to {Predict} the {Histological} {Subtypes} of {Lung} {Cancer}},
	volume = {44},
	issn = {1536-0229},
	doi = {10.1097/RLU.0000000000002810},
	abstract = {PURPOSE: We sought to distinguish lung adenocarcinoma (ADC) from squamous cell carcinoma using a machine-learning algorithm with PET-based radiomic features.
METHODS: A total of 396 patients with 210 ADCs and 186 squamous cell carcinomas who underwent FDG PET/CT prior to treatment were retrospectively analyzed. Four clinical features (age, sex, tumor size, and smoking status) and 40 radiomic features were investigated in terms of lung ADC subtype prediction. Radiomic features were extracted from the PET images of segmented tumors using the LIFEx package. The clinical and radiomic features were ranked, and a subset of useful features was selected based on Gini coefficient scores in terms of associations with histological class. The areas under the receiver operating characteristic curves (AUCs) of classifications afforded by several machine-learning algorithms (random forest, neural network, naive Bayes, logistic regression, and a support vector machine) were compared and validated via random sampling.
RESULTS: We developed and validated a PET-based radiomic model predicting the histological subtypes of lung cancer. Sex, SUVmax, gray-level zone length nonuniformity, gray-level nonuniformity for zone, and total lesion glycolysis were the 5 best predictors of lung ADC. The logistic regression model outperformed all other classifiers (AUC = 0.859, accuracy = 0.769, F1 score = 0.774, precision = 0.804, recall = 0.746) followed by the neural network model (AUC = 0.854, accuracy = 0.772, F1 score = 0.777, precision = 0.807, recall = 0.750).
CONCLUSIONS: A machine-learning approach successfully identified the histological subtypes of lung cancer. A PET-based radiomic features may help clinicians improve the histopathologic diagnosis in a noninvasive manner.},
	language = {eng},
	number = {12},
	journal = {Clinical Nuclear Medicine},
	author = {Hyun, Seung Hyup and Ahn, Mi Sun and Koh, Young Wha and Lee, Su Jin},
	month = dec,
	year = {2019},
	pmid = {31689276},
	keywords = {Female, Humans, Retrospective Studies, Aged, Male, Middle Aged, ROC Curve, Area Under Curve, Bayes Theorem, Carcinoma, Squamous Cell, Image Processing, Computer-Assisted, Lung Neoplasms, Machine Learning, Positron Emission Tomography Computed Tomography},
	pages = {956--960},
}

@article{liu_prediction_2018,
	title = {Prediction of rupture risk in anterior communicating artery aneurysms with a feed-forward artificial neural network},
	volume = {28},
	issn = {1432-1084},
	doi = {10.1007/s00330-017-5300-3},
	abstract = {OBJECTIVES: Anterior communicating artery (ACOM) aneurysms are the most common intracranial aneurysms, and predicting their rupture risk is challenging. We aimed to predict this risk using a two-layer feed-forward artificial neural network (ANN).
MATERIALS AND METHOD: 594 ACOM aneurysms, 54 unruptured and 540 ruptured, were reviewed. A two-layer feed-forward ANN was designed for ACOM aneurysm rupture-risk analysis. To improve ANN efficiency, an adaptive synthetic (ADASYN) sampling approach was applied to generate more synthetic data for unruptured aneurysms. Seventeen parameters (13 morphological parameters of ACOM aneurysm measured from these patients' CT angiography (CTA) images, two demographic factors, and hypertension and smoking histories) were adopted as ANN input.
RESULTS: Age, vessel size, aneurysm height, perpendicular height, aneurysm neck size, aspect ratio, size ratio, aneurysm angle, vessel angle, aneurysm projection, A1 segment configuration, aneurysm lobulations and hypertension were significantly different between the ruptured and unruptured groups. Areas under the ROC curve for training, validating, testing and overall data sets were 0.953, 0.937, 0.928 and 0.950, respectively. Overall prediction accuracy for raw 594 samples was 94.8 \%.
CONCLUSION: This ANN presents good performance and offers a valuable tool for prediction of rupture risk in ACOM aneurysms, which may facilitate management of unruptured ACOM aneurysms.
KEY POINTS: {\textbullet} A feed-forward ANN was designed for the prediction of rupture risk in ACOM aneurysms. {\textbullet} Two demographic parameters, 13 morphological aneurysm parameters, and hypertension/smoking history were acquired. {\textbullet} An ADASYN sampling approach was used to improve ANN quality. {\textbullet} Overall prediction accuracy of 94.8 \% for the raw samples was achieved.},
	language = {eng},
	number = {8},
	journal = {European Radiology},
	author = {Liu, Jinjin and Chen, Yongchun and Lan, Li and Lin, Boli and Chen, Weijian and Wang, Meihao and Li, Rui and Yang, Yunjun and Zhao, Bing and Hu, Zilong and Duan, Yuxia},
	month = aug,
	year = {2018},
	pmid = {29476219},
	keywords = {Machine learning, Adult, Female, Humans, Neural Networks, Computer, Aged, Male, Middle Aged, ROC Curve, Young Adult, Aged, 80 and over, Decision Making, Age Factors, Aneurysm, Aneurysm, Ruptured, Angiography, Anterior Cerebral Artery, Cerebral Angiography, Computed Tomography Angiography, Intracranial Aneurysm, Models, Biological, Predictive Value of Tests, Risk, Risk Factors, Rupture},
	pages = {3268--3275},
}

@misc{noauthor_machine_nodate,
	title = {Machine {Learning} {Glossary}},
	url = {https://developers.google.com/machine-learning/glossary},
	abstract = {Compilation of key machine-learning and TensorFlow terms, with beginner-friendly definitions.},
	language = {en},
	urldate = {2022-05-04},
	journal = {Google Developers},
	file = {Snapshot:C\:\\Users\\roman\\Zotero\\storage\\Z6T3CEQW\\glossary.html:text/html},
}

@misc{noauthor_what_nodate,
	title = {What is {Data} {Fusion}? - {Definition} from {Techopedia}},
	shorttitle = {What is {Data} {Fusion}?},
	url = {http://www.techopedia.com/definition/32735/data-fusion},
	abstract = {This definition explains the meaning of Data Fusion and why it matters.},
	language = {en},
	urldate = {2022-05-04},
	journal = {Techopedia.com},
	file = {Snapshot:C\:\\Users\\roman\\Zotero\\storage\\CR6K7IBH\\data-fusion.html:text/html},
}

@article{roffman_predicting_2018,
	title = {Predicting non-melanoma skin cancer via a multi-parameterized artificial neural network},
	volume = {8},
	issn = {2045-2322},
	doi = {10.1038/s41598-018-19907-9},
	abstract = {Ultraviolet radiation (UVR) exposure and family history are major associated risk factors for the development of non-melanoma skin cancer (NMSC). The objective of this study was to develop and validate a multi-parameterized artificial neural network based on available personal health information for early detection of NMSC with high sensitivity and specificity, even in the absence of known UVR exposure and family history. The 1997-2015 NHIS adult survey data used to train and validate our neural network (NN) comprised of 2,056 NMSC and 460,574 non-cancer cases. We extracted 13 parameters for our NN: gender, age, BMI, diabetic status, smoking status, emphysema, asthma, race, Hispanic ethnicity, hypertension, heart diseases, vigorous exercise habits, and history of stroke. This study yielded an area under the ROC curve of 0.81 and 0.81 for training and validation, respectively. Our results (training sensitivity 88.5\% and specificity 62.2\%, validation sensitivity 86.2\% and specificity 62.7\%) were comparable to a previous study of basal and squamous cell carcinoma prediction that also included UVR exposure and family history information. These results indicate that our NN is robust enough to make predictions, suggesting that we have identified novel associations and potential predictive parameters of NMSC.},
	language = {eng},
	number = {1},
	journal = {Scientific Reports},
	author = {Roffman, David and Hart, Gregory and Girardi, Michael and Ko, Christine J. and Deng, Jun},
	month = jan,
	year = {2018},
	pmid = {29374196},
	pmcid = {PMC5786038},
	keywords = {Female, Humans, Neural Networks, Computer, Male, Middle Aged, ROC Curve, Skin Neoplasms, Sensitivity and Specificity, Decision Support Techniques, Diagnostic Tests, Routine, Early Diagnosis},
	pages = {1701},
	file = {Volltext:C\:\\Users\\roman\\Zotero\\storage\\ES3PK8A9\\Roffman et al. - 2018 - Predicting non-melanoma skin cancer via a multi-pa.pdf:application/pdf},
}

@article{tschandl_ham10000_2018,
	title = {The {HAM10000} dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions},
	volume = {5},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/sdata2018161},
	doi = {10.1038/sdata.2018.161},
	language = {en},
	number = {1},
	urldate = {2022-06-05},
	journal = {Scientific Data},
	author = {Tschandl, Philipp and Rosendahl, Cliff and Kittler, Harald},
	month = dec,
	year = {2018},
	pages = {180161},
	file = {Tschandl et al. - 2018 - The HAM10000 dataset, a large collection of multi-.pdf:C\:\\Users\\roman\\Zotero\\storage\\QS4T7GPK\\Tschandl et al. - 2018 - The HAM10000 dataset, a large collection of multi-.pdf:application/pdf},
}

@misc{noauthor_isic_nodate,
	title = {{ISIC} {Challenge}},
	url = {https://challenge.isic-archive.com/landing/2019/},
	urldate = {2022-06-05},
	file = {ISIC Challenge:C\:\\Users\\roman\\Zotero\\storage\\ZB4ILLZG\\2019.html:text/html},
}

@article{nie_multi-channel_2019,
	title = {Multi-{Channel} {3D} {Deep} {Feature} {Learning} for {Survival} {Time} {Prediction} of {Brain} {Tumor} {Patients} {Using} {Multi}-{Modal} {Neuroimages}},
	volume = {9},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-018-37387-9},
	doi = {10.1038/s41598-018-37387-9},
	language = {en},
	number = {1},
	urldate = {2022-06-05},
	journal = {Scientific Reports},
	author = {Nie, Dong and Lu, Junfeng and Zhang, Han and Adeli, Ehsan and Wang, Jun and Yu, Zhengda and Liu, LuYan and Wang, Qian and Wu, Jinsong and Shen, Dinggang},
	month = dec,
	year = {2019},
	pages = {1103},
	file = {Nie et al. - 2019 - Multi-Channel 3D Deep Feature Learning for Surviva.pdf:C\:\\Users\\roman\\Zotero\\storage\\X9HWCTNX\\Nie et al. - 2019 - Multi-Channel 3D Deep Feature Learning for Surviva.pdf:application/pdf},
}

@article{maimaitijiang_soybean_2020,
	title = {Soybean yield prediction from {UAV} using multimodal data fusion and deep learning},
	volume = {237},
	issn = {0034-4257},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425719306194},
	doi = {10.1016/j.rse.2019.111599},
	abstract = {Preharvest crop yield prediction is critical for grain policy making and food security. Early estimation of yield at field or plot scale also contributes to high-throughput plant phenotyping and precision agriculture. New developments in Unmanned Aerial Vehicle (UAV) platforms and sensor technology facilitate cost-effective data collection through simultaneous multi-sensor/multimodal data collection at very high spatial and spectral resolutions. The objective of this study is to evaluate the power of UAV-based multimodal data fusion using RGB, multispectral and thermal sensors to estimate soybean (Glycine max) grain yield within the framework of Deep Neural Network (DNN). RGB, multispectral, and thermal images were collected using a low-cost multi-sensory UAV from a test site in Columbia, Missouri, USA. Multimodal information, such as canopy spectral, structure, thermal and texture features, was extracted and combined to predict crop grain yield using Partial Least Squares Regression (PLSR), Random Forest Regression (RFR), Support Vector Regression (SVR), input-level feature fusion based DNN (DNN-F1) and intermediate-level feature fusion based DNN (DNN-F2). The results can be summarized in three messages: (1) multimodal data fusion improves the yield prediction accuracy and is more adaptable to spatial variations; (2) DNN-based models improve yield prediction model accuracy: the highest accuracy was obtained by DNN-F2 with an R2 of 0.720 and a relative root mean square error (RMSE\%) of 15.9\%; (3) DNN-based models were less prone to saturation effects, and exhibited more adaptive performance in predicting grain yields across the Dwight, Pana and AG3432 soybean genotypes in our study. Furthermore, DNN-based models demonstrated consistent performance over space with less spatial dependency and variations. This study indicates that multimodal data fusion using low-cost UAV within a DNN framework can provide a relatively accurate and robust estimation of crop yield, and deliver valuable insight for high-throughput phenotyping and crop field management with high spatial precision.},
	language = {en},
	urldate = {2022-06-06},
	journal = {Remote Sensing of Environment},
	author = {Maimaitijiang, Maitiniyazi and Sagan, Vasit and Sidike, Paheding and Hartling, Sean and Esposito, Flavio and Fritschi, Felix B.},
	month = feb,
	year = {2020},
	keywords = {Deep learning, Data fusion, Multimodality, Phenotyping, Remote sensing, Spatial autocorrelation, Yield prediction},
	pages = {111599},
	file = {ScienceDirect Snapshot:C\:\\Users\\roman\\Zotero\\storage\\3GYHSPIM\\S0034425719306194.html:text/html},
}

@inproceedings{gadiraju_multimodal_2020,
	address = {New York, NY, USA},
	series = {{KDD} '20},
	title = {Multimodal {Deep} {Learning} {Based} {Crop} {Classification} {Using} {Multispectral} and {Multitemporal} {Satellite} {Imagery}},
	isbn = {978-1-4503-7998-4},
	url = {https://doi.org/10.1145/3394486.3403375},
	doi = {10.1145/3394486.3403375},
	abstract = {The Food and Agriculture Organization (FAO) of the United Nations predicts that in order to meet the needs of the expected 3 billion population growth by 2050, food production has to increase by 60\%. Therefore, monitoring and mapping crops accurately is essential for estimating food production during each crop growing season across the globe. Traditionally, multispectral remote sensing imagery has been widely used for mapping crops worldwide. However, single date imagery does not capture temporal characteristics (phenology) of growing crops, leading to imprecise crop maps and food estimates. On the other hand, purely temporal classification approaches also produce inaccurate crop maps as they do not account for spatial autocorrelations. In this paper, we present a multimodal deep learning solution that jointly exploits spatial-spectral and phenological properties to identify major crop types. Using a two stream architecture, spatial characteristics are captured via a spatial stream consisting of very high resolution images (single date, 1m, 3-spectral bands, USDA NAIP) with a CNN and the phenological characteristics via a temporal stream images (biweekly, 250m, MODIS NDVI) with an LSTM. Experimental results show that the proposed multimodal solution reduces prediction error by 60\%.},
	urldate = {2022-06-06},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Gadiraju, Krishna Karthik and Ramachandra, Bharathkumar and Chen, Zexi and Vatsavai, Ranga Raju},
	month = aug,
	year = {2020},
	keywords = {multimodal, crop classification, neural networks, remote sensing},
	pages = {3234--3242},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\U755GSYX\\Gadiraju et al. - 2020 - Multimodal Deep Learning Based Crop Classification.pdf:application/pdf},
}

@article{amdan_analysis_2017,
	title = {An {Analysis} of {Profanity} in {English} {Lyrics}},
	volume = {1},
	copyright = {Copyright (c)},
	issn = {2600-9501},
	url = {https://spaj.ukm.my/jws/index.php/jws/article/view/57},
	abstract = {ABSTRACT
~This paper is designed to present results of an analysis of Profanity in English lyrics. The purpose of this qualitative study is to discover type of profane words used in English songs. According to Commercial Radio Malaysia (CRM), {\textquotedblleft}17.7 million of Malaysia{\textquoteright}s population aged 10 years and above tune in to a radio daily{\textquotedblright} which indicates how music is easily to get access. 100 songs from Top 20 Billboard Hot 100 year 2009 until 2015were selected and analyzed in this paper. The results showed that profane words in the selected songs were falls under obscenity type with the highest frequency of 41\% in the songs throughout the year. The results showed that this type of words generally found in Hip Hop, R\&B and Pop genre compared to others. 
~
Keywords: Profanity, Cultivation Theory, Objectification Theory and Social Cognitive Theory on Gender Development},
	language = {en},
	number = {1},
	urldate = {2022-09-24},
	journal = {Jurnal Wacana Sarjana},
	author = {Amdan, Nur Faatihah Binti and Shaari, Azianura Hani},
	month = dec,
	year = {2017},
	note = {Number: 1},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\QPEEP8WT\\Amdan und Shaari - 2017 - An Analysis of Profanity in English Lyrics.pdf:application/pdf},
}

@misc{log_swear_nodate,
	title = {On {Swear} {Words} and {Rap} {Music}:},
	language = {en},
	author = {Log, Filip Nikitas Metallinos and Lee, Joseph},
	file = {Log und Lee - On Swear Words and Rap Music.pdf:C\:\\Users\\roman\\Zotero\\storage\\GKKTC4JX\\Log und Lee - On Swear Words and Rap Music.pdf:application/pdf},
}

@misc{log_swear_nodate-1,
	title = {On {Swear} {Words} and {Rap} {Music}:},
	language = {en},
	author = {Log, Filip Nikitas Metallinos and Lee, Joseph},
	note = {Student Thesis},
	file = {Log und Lee - On Swear Words and Rap Music.pdf:C\:\\Users\\roman\\Zotero\\storage\\5GDJ6JL9\\Log und Lee - On Swear Words and Rap Music.pdf:application/pdf},
}

@misc{syamsul_bahri_taboo_2016,
	title = {{TABOO} {WORDS} {IN} {POP}, {ROCK}, {AND} {HIP} {HOP} {SONGS} {MEDAN}},
	url = {https://jurnal.unimed.ac.id/2012/index.php/jalu/article/view/6434},
	abstract = {Rahmayanti, Riska Juli, 2113220034. Taboo Words in Pop, Rock, and Hip Hop Songs. A Thesis, English, Department, Faculty of Languages and Arts, State University of Medan, 2016.},
	language = {en},
	urldate = {2022-09-24},
	author = {Syamsul Bahri, Riska Juli Rahmayanti And},
	month = oct,
	year = {2016},
	file = {Syamsul Bahri - 2016 - TABOO WORDS IN POP, ROCK, AND HIP HOP SONGS MEDAN.pdf:C\:\\Users\\roman\\Zotero\\storage\\ITNPL52F\\Syamsul Bahri - 2016 - TABOO WORDS IN POP, ROCK, AND HIP HOP SONGS MEDAN.pdf:application/pdf},
}

@misc{ramadinata_analysis_2013,
	title = {{AN} {ANALYSIS} {OF} {SWEAR} {WORDS} {USED} {IN} "{EMINEM}" {SONG} {LYRICS}},
	url = {https://eprints.umm.ac.id/27102/},
	abstract = {Music is popular language in the world. Music and components, such as sound in rhythm, harmony and lyrics, which are the sound and composition is familiar for people in the world. Everybody who likes music knows that music has power to transfer the ideas and emotions. There are many famous musicians who make a song from their own experiences, so that they produce a good lyric to transfer their ideas and emotions. Several musicians try to make a unique and different from each other's in their song lyrics, for example Eminem that used the swear words to representative his felling when angry, happy, or sad in his song lyric with hip-hop genre. Because by using that word he fell satisfied with the problem that past in his life without considering that he can solve it or not, and in every album that he released also became controversial, especially in his country.

In this research, the researcher analysis; 1) What are the categories of swear words used in Eminem song lyrics? 2) What are the meaning of swear words used in Eminem song lyrics? And purpose of this research are; 1) To analyze the categories of swear words used in Eminem song lyrics. 2) To know the meaning of swear words used in Eminem song lyrics. The researcher used descriptive qualitative design and objective approach. The subject of this research was Eminem song lyrics. The researcher did not use population and sample because this research does not meant the research to be generalized.

The result of this research showed they are seven categories of swear words that Eminem mostly used: copulative terms, excretory terms, human genital terms, sexual irregularities terms, mother in law terms, animal terms, and death. Besides that, in every swear words has different meanings; for example in the word "Pussy", it is classified into human genital terms because "pussy" it refers to organ sex of human being for the reproduction system to the next generation. The word "fuck" categorized into copulative terms and, the meaning of this word was related to the sexual intercourse between men and woman.},
	language = {en},
	urldate = {2022-09-24},
	publisher = {University of Muhammadiyah Malang},
	author = {RAMADINATA, AFRI},
	month = may,
	year = {2013},
	note = {Student Thesis},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\QBVPYPVK\\RAMADINATA - 2013 - AN ANALYSIS OF SWEAR WORDS USED IN EMINEM SONG L.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\IQ46IV3T\\27102.html:text/html},
}

@book{sanden_you_2020,
	title = {You know who pop the most shit? : {A} study of profanity and gender differences in modern pop music},
	shorttitle = {You know who pop the most shit?},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:umu:diva-173998},
	abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
	language = {eng},
	urldate = {2022-09-24},
	author = {Sand{\'e}n, Johanna},
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\FWB7YLNL\\Sand{\'e}n - 2020 - You know who pop the most shit  A study of profa.pdf:application/pdf;Snapshot:C\:\\Users\\roman\\Zotero\\storage\\HXN87Z5U\\record.html:text/html},
}

@misc{sanden_you_2020-1,
	title = {You know who pop the most shit? : {A} study of profanity and gender differences in modern pop music},
	abstract = {Are there any gender differences in how profanity is used in modern pop music in 2019? The purpose of this study was to analyse music lyrics with particular attention paid to frequency and profanity profiles between female and male artists. The corpus used in the investigation contains a total of 34 music lyrics equally divided between the genders and was collected from a Billboard chart called {\textquotedblleft}Hot 100 Songs{\textquotedblright}. Although this sample was small and may not be representative of all modern pop music lyrics, trends in this data show that the female artists sampled from the Billboard chart actually use profanity more frequently than male artists, which is in contrast to previous research. Furthermore, the result shows that male and female artists have distinctive profiles regarding the types of profanity used. Female artists use the swear word bitch more frequently whereas male artists tent to use nigger with a greater frequency.},
	publisher = {Ume{\r a} University, Department of language studies},
	author = {Sand{\'e}n, Johanna},
	year = {2020},
	note = {Backup Publisher: Ume{\r a} University, Department of language studies
Pages: 32},
	keywords = {frequency, gender, music lyrics, Profanity, profanity profiles, swear words},
}

@incollection{woodfield_chapter_2017,
	title = {Chapter 4: {Using} {Twitter} as a {Data} {Source}: {An} {Overview} of {Ethical}, {Legal}, and {Methodological} {Challenges}},
	volume = {2},
	isbn = {978-1-78714-486-6 978-1-78714-485-9},
	shorttitle = {Chapter 4},
	url = {https://www.emerald.com/insight/content/doi/10.1108/S2398-601820180000002004/full/html},
	language = {en},
	urldate = {2023-01-27},
	booktitle = {Advances in {Research} {Ethics} and {Integrity}},
	publisher = {Emerald Publishing Limited},
	author = {Ahmed, Wasim and Bath, Peter A. and Demartini, Gianluca},
	editor = {Woodfield, Kandy},
	month = dec,
	year = {2017},
	doi = {10.1108/S2398-601820180000002004},
	pages = {79--107},
	file = {Ahmed et al. - 2017 - Chapter 4 Using Twitter as a Data Source An Over.pdf:C\:\\Users\\roman\\Zotero\\storage\\K5YXN7UQ\\Ahmed et al. - 2017 - Chapter 4 Using Twitter as a Data Source An Over.pdf:application/pdf},
}

@incollection{ahmed_using_2017,
	series = {Advances in {Research} {Ethics} and {Integrity}},
	title = {Using {Twitter} as a {Data} {Source}: {An} {Overview} of {Ethical}, {Legal}, and {Methodological} {Challenges}},
	volume = {2},
	isbn = {978-1-78714-486-6 978-1-78714-485-9},
	shorttitle = {Using {Twitter} as a {Data} {Source}},
	url = {https://doi.org/10.1108/S2398-601820180000002004},
	abstract = {This chapter provides an overview of the specific legal, ethical, and privacy issues that can arise when conducting research using Twitter data. Existing literature is reviewed to inform those who may be undertaking social media research. We also present a number of industry and academic case studies in order to highlight the challenges that may arise in research projects using social media data. Finally, the chapter provides an overview of the process that was followed to gain ethics approval for a Ph.D. project using Twitter as a primary source of data. By outlining a number of Twitter-specific research case studies, the chapter will be a valuable resource to those considering the ethical implications of their own research projects utilizing social media data. Moreover, the chapter outlines existing work looking at the ethical practicalities of social media data and relates their applicability to researching Twitter.},
	urldate = {2023-01-27},
	booktitle = {The {Ethics} of {Online} {Research}},
	publisher = {Emerald Publishing Limited},
	author = {Ahmed, Wasim and Bath, Peter A. and Demartini, Gianluca},
	editor = {Woodfield, Kandy},
	month = jan,
	year = {2017},
	doi = {10.1108/S2398-601820180000002004},
	keywords = {Ethics, Privacy, Research integrity, Social Media, Twitter},
	pages = {79--107},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\I4DCRISJ\\Ahmed et al. - 2017 - Using Twitter as a Data Source An Overview of Eth.pdf:application/pdf},
}

@inproceedings{shukla_enhanced_2021,
	title = {Enhanced {Twitter} bot detection using ensemble machine learning},
	doi = {10.1109/ICICT50816.2021.9358734},
	abstract = {Social media has been an unavoidable part of our life over the years. As it is getting popular, the number of social media bots are also increasing. Social media bots are the artificial agents who imitate as a human on social media. They are intended to like, retweet the posts which eventually can tamper with the genuineness of the trend. They can also be a menace to democracy as they can falsely influence people. Social media bots can be used for cyberbullying, terrorist activities, gaining fame, spreading wrong information, restricting freedom of speech, spamming. To detect social media bots on Twitter, we utilized metadata of Twitter profiles and applied a unique feature selection method, and also explored the power of ensemble learning to make a robust classifier.},
	booktitle = {2021 6th {International} {Conference} on {Inventive} {Computation} {Technologies} ({ICICT})},
	author = {Shukla, Hrushikesh and Jagtap, Nakshatra and Patil, Balaji},
	month = jan,
	year = {2021},
	keywords = {Feature extraction, Metadata, Machine Learning, Blogs, Encoding, Ensemble Learning, Social networking (online), Terrorism, Twitter bot detection, Twitter profile Metadata, Unsolicited e-mail, Weight of evidance encoding},
	pages = {930--936},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\BQP4WTXF\\9358734.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\FPXWSFJY\\Shukla et al. - 2021 - Enhanced Twitter bot detection using ensemble mach.pdf:application/pdf},
}

@article{kudugunta_deep_2018,
	title = {Deep neural networks for bot detection},
	volume = {467},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025518306248},
	doi = {10.1016/j.ins.2018.08.019},
	abstract = {The problem of detecting bots, automated social media accounts governed by software but disguising as human users, has strong implications. For example, bots have been used to sway political elections by distorting online discourse, to manipulate the stock market, or to push anti-vaccine conspiracy theories that may have caused health epidemics. Most techniques proposed to date detect bots at the account level, by processing large amounts of social media posts, and leveraging information from network structure, temporal dynamics, sentiment analysis, etc. In this paper, we propose a deep neural network based on contextual long short-term memory (LSTM) architecture that exploits both content and metadata to detect bots at the tweet level: contextual features are extracted from user metadata and fed as auxiliary input to LSTM deep nets processing the tweet text. Another contribution that we make is proposing a technique based on synthetic minority oversampling to generate a large labeled dataset, suitable for deep nets training, from a minimal amount of labeled data (roughly 3000 examples of sophisticated Twitter bots). We demonstrate that, from just one single tweet, our architecture can achieve high classification accuracy (AUC  {\textgreater}  96\%) in separating bots from humans. We apply the same architecture to account-level bot detection, achieving nearly perfect classification accuracy (AUC  {\textgreater}  99\%). Our system outperforms previous state of the art while leveraging a small and interpretable set of features, yet requiring minimal training data.},
	language = {en},
	urldate = {2023-01-27},
	journal = {Information Sciences},
	author = {Kudugunta, Sneha and Ferrara, Emilio},
	month = oct,
	year = {2018},
	keywords = {Deep learning, Deep neural networks, Social bots, Social media networks, Web and social media},
	pages = {312--322},
	file = {Eingereichte Version:C\:\\Users\\roman\\Zotero\\storage\\7RS7RAI5\\Kudugunta und Ferrara - 2018 - Deep neural networks for bot detection.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\roman\\Zotero\\storage\\FBH7MEHL\\S0020025518306248.html:text/html},
}

@article{feng_heterogeneity-aware_2022,
	title = {Heterogeneity-{Aware} {Twitter} {Bot} {Detection} with {Relational} {Graph} {Transformers}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20314},
	doi = {10.1609/aaai.v36i4.20314},
	abstract = {Twitter bot detection has become an important and challenging task to combat misinformation and protect the integrity of the online discourse. State-of-the-art approaches generally leverage the topological structure of the Twittersphere, while they neglect the heterogeneity of relations and influence among users. In this paper, we propose a novel bot detection framework to alleviate this problem, which leverages the topological structure of user-formed heterogeneous graphs and models varying influence intensity between users. Specifically, we construct a heterogeneous information network with users as nodes and diversified relations as edges. We then propose relational graph transformers to model heterogeneous influence between users and learn node representations. Finally, we use semantic attention networks to aggregate messages across users and relations and conduct heterogeneity-aware Twitter bot detection. Extensive experiments demonstrate that our proposal outperforms state-of-the-art methods on a comprehensive Twitter bot detection benchmark. Additional studies also bear out the effectiveness of our proposed relational graph transformers, semantic attention networks and the graph-based approach in general.},
	language = {en},
	number = {4},
	urldate = {2023-01-27},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Feng, Shangbin and Tan, Zhaoxuan and Li, Rui and Luo, Minnan},
	month = jun,
	year = {2022},
	note = {Number: 4},
	keywords = {Machine Learning (ML)},
	pages = {3977--3985},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\CNBF9PHR\\Feng et al. - 2022 - Heterogeneity-Aware Twitter Bot Detection with Rel.pdf:application/pdf},
}

@inproceedings{vashisth_gender_2020,
	title = {Gender {Classification} using {Twitter} {Text} {Data}},
	doi = {10.1109/ISSC49989.2020.9180161},
	abstract = {Increasingly content sharing websites such as social media have become very popular in many countries across the world. Classifying the gender of a person based on these short messages is an interesting research area that could benefit legal investigation, forensics, marketing analysis, advertising and recommendation. This research will explore the use of Natural Language Processing (NLP) techniques and tweets in a gender classification system. This investigation will compare multiple techniques such as Bag of Words (Term Frequency - Inverse Document Frequency), Word Embedding (W2Vec, GloVe) and traditional Machine Learning techniques (Logistic Regression, Support Vector Machine and Na{\"i}ve Bayes) in this context. A new dataset has been generated to be used as part of this study comprising of the user gender and associated tweets. This dataset was developed due to the unavailability of any public standard dataset with the volume required to perform this investigation. The results have determined that the traditional Bag of Words model did not provide any significant results in classification. However, word embedding models have significantly performed better using multiple machine learning techniques. Therefore, the word embedding models have been proven to be the most effective technique in classifying gender based on twitter text data.},
	booktitle = {2020 31st {Irish} {Signals} and {Systems} {Conference} ({ISSC})},
	author = {Vashisth, Pradeep and Meehan, Kevin},
	month = jun,
	year = {2020},
	note = {ISSN: 2688-1454},
	keywords = {Feature extraction, Machine learning, Task analysis, Machine Learning, Twitter, Gender Classification, Natural language processing, Natural Language Processing, Support vector machines, Word Embedding},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\roman\\Zotero\\storage\\48IE8H4C\\9180161.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\7UG8I5B4\\Vashisth und Meehan - 2020 - Gender Classification using Twitter Text Data.pdf:application/pdf},
}

@inproceedings{yan_classifying_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Classifying {Twitter} {Users} {Based} on {User} {Profile} and {Followers} {Distribution}},
	isbn = {978-3-642-40285-2},
	doi = {10.1007/978-3-642-40285-2_34},
	abstract = {We propose methods to classify Twitter users into open accounts and closed accounts. Open accounts (shop accounts, etc.) are the accounts who publish information to general public and their intentions is to promotion products, services or themselves. On the other hand, closed accounts tweet information on their daily lives or use Twitter as a communication tool with their friends. To distinguish these two different kinds of Twitter users can help us to search for local and daily information on Twitter. We classify Twitter accounts based on user profiles and followers distributions. The features of profile of open accounts include clue keywords, telephone number, detailed address, and so on. Follower distribution is another notable feature: most open accounts have followers from variety community. The experimental results validate our methods.},
	language = {en},
	booktitle = {Database and {Expert} {Systems} {Applications}},
	publisher = {Springer},
	author = {Yan, Liang and Ma, Qiang and Yoshikawa, Masatoshi},
	editor = {Decker, Hendrik and Lhotsk{\'a}, Lenka and Link, Sebastian and Basl, Josef and Tjoa, A. Min},
	year = {2013},
	keywords = {Open Account, Promotion Product, Receiver Operating Characteristic Curve, Telephone Number, Twitter User},
	pages = {396--403},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\45BDAU72\\Yan et al. - 2013 - Classifying Twitter Users Based on User Profile an.pdf:application/pdf},
}

@misc{li_hybrid_2018,
	title = {A {Hybrid} {Model} for {Role}-related {User} {Classification} on {Twitter}},
	url = {http://arxiv.org/abs/1811.10202},
	doi = {10.48550/arXiv.1811.10202},
	abstract = {To aid a variety of research studies, we propose TWIROLE, a hybrid model for role-related user classification on Twitter, which detects male-related, female-related, and brand-related (i.e., organization or institution) users. TWIROLE leverages features from tweet contents, user profiles, and profile images, and then applies our hybrid model to identify a user's role. To evaluate it, we used two existing large datasets about Twitter users, and conducted both intra- and inter-comparison experiments. TWIROLE outperforms existing methods and obtains more balanced results over the several roles. We also confirm that user names and profile images are good indicators for this task. Our research extends prior work that does not consider brand-related users, and is an aid to future evaluation efforts relative to investigations that rely upon self-labeled datasets.},
	urldate = {2023-01-27},
	publisher = {arXiv},
	author = {Li, Liuqing and Song, Ziqian and Zhang, Xuan and Fox, Edward A.},
	month = nov,
	year = {2018},
	note = {arXiv:1811.10202 [cs]},
	keywords = {Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:C\:\\Users\\roman\\Zotero\\storage\\Q92XGGTN\\Li et al. - 2018 - A Hybrid Model for Role-related User Classificatio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\roman\\Zotero\\storage\\AE2AZH24\\1811.html:text/html},
}

@article{mccorriston_organizations_2015,
	title = {Organizations {Are} {Users} {Too}: {Characterizing} and {Detecting} the {Presence} of {Organizations} on {Twitter}},
	volume = {9},
	copyright = {Copyright (c) 2021 Proceedings of the International AAAI Conference on Web and Social Media},
	issn = {2334-0770},
	shorttitle = {Organizations {Are} {Users} {Too}},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/14672},
	doi = {10.1609/icwsm.v9i1.14672},
	abstract = {Much work on the demographics of social media platforms such as Twitter has focused on the properties of individuals, such as gender or age.  However, because credible detectors for organization accounts do not exist, these and future large-scale studies of human behavior on social media can be contaminated by the presence of accounts belonging to organizations. We analyze organizations on Twitter to assess their distinct behavioral characteristics and determine what types of organizations are active.  We first create a dataset of manually classified accounts from a representative sample of Twitter and then introduce a classifier to distinguish between organizational and personal accounts. In addition, we find that although organizations make up less than 10\% of the accounts, they are significantly more connected, with an order of magnitude more friends and followers.},
	language = {en},
	number = {1},
	urldate = {2023-01-27},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	author = {McCorriston, James and Jurgens, David and Ruths, Derek},
	year = {2015},
	note = {Number: 1},
	keywords = {social media},
	pages = {650--653},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\TXHIFF2Q\\McCorriston et al. - 2015 - Organizations Are Users Too Characterizing and De.pdf:application/pdf},
}

@inproceedings{de_choudhury_unfolding_2012,
	address = {New York, NY, USA},
	series = {{CSCW} '12},
	title = {Unfolding the event landscape on twitter: classification and exploration of user categories},
	isbn = {978-1-4503-1086-4},
	shorttitle = {Unfolding the event landscape on twitter},
	url = {https://doi.org/10.1145/2145204.2145242},
	doi = {10.1145/2145204.2145242},
	abstract = {Social media platforms such as Twitter garner significant attention from very large audiences in response to real-world events. Automatically establishing who is participating in information production or conversation around events can improve event content consumption, help expose the stakeholders in the event and their varied interests, and even help steer subsequent coverage of an event by journalists. In this paper, we take initial steps towards building an automatic classifier for user types on Twitter, focusing on three core user categories that are reflective of the information production and consumption processes around events: organizations, journalists/media bloggers, and ordinary individuals. Exploration of the user categories on a range of events shows distinctive characteristics in terms of the proportion of each user type, as well as differences in the nature of content each shared around the events.},
	urldate = {2023-01-27},
	booktitle = {Proceedings of the {ACM} 2012 conference on {Computer} {Supported} {Cooperative} {Work}},
	publisher = {Association for Computing Machinery},
	author = {De Choudhury, Munmun and Diakopoulos, Nicholas and Naaman, Mor},
	month = feb,
	year = {2012},
	keywords = {social media, events, twitter, user classification},
	pages = {241--244},
	file = {Full Text PDF:C\:\\Users\\roman\\Zotero\\storage\\5AUZAFPK\\De Choudhury et al. - 2012 - Unfolding the event landscape on twitter classifi.pdf:application/pdf},
}

@misc{noauthor_disentangling_nodate,
	title = {Disentangling {User} {Samples}: {A} {Supervised} {Machine} {Learning} {Approach} to {Proxy}-population {Mismatch} in {Twitter} {Research}: {Communication} {Methods} and {Measures}: {Vol} 12, {No} 2-3},
	url = {https://www.tandfonline.com/doi/epdf/10.1080/19312458.2018.1430755?needAccess=true&role=button},
	urldate = {2023-01-30},
	file = {Disentangling User Samples\: A Supervised Machine Learning Approach to Proxy-population Mismatch in Twitter Research\: Communication Methods and Measures\: Vol 12, No 2-3:C\:\\Users\\roman\\Zotero\\storage\\UY8Y69YA\\19312458.2018.html:text/html},
}

@article{kwon_disentangling_2018,
	title = {Disentangling {User} {Samples}: {A} {Supervised} {Machine} {Learning} {Approach} to {Proxy}-population {Mismatch} in {Twitter} {Research}},
	copyright = {{\textcopyright} 2018 Taylor \& Francis Group, LLC},
	issn = {1931-2458},
	shorttitle = {Disentangling {User} {Samples}},
	url = {https://www.tandfonline.com/doi/full/10.1080/19312458.2018.1430755},
	abstract = {This study addresses the issue of sampling biases in social media data-driven communication research. The authors demonstrate how supervised machine learning could reduce Twitter sampling bias indu...},
	language = {en},
	urldate = {2023-01-30},
	journal = {Communication Methods and Measures},
	author = {Kwon, K. Hazel and Priniski, J. Hunter and Chadha, Monica},
	month = feb,
	year = {2018},
	note = {Publisher: Routledge},
	file = {Snapshot:C\:\\Users\\roman\\Zotero\\storage\\XYZ2PZJ9\\19312458.2018.html:text/html},
}

@inproceedings{nguyen_bertweet_2020,
	title = {{BERTweet}: {A} pre-trained language model for {English} {Tweets}},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	author = {Nguyen, Dat Quoc and Vu, Thanh and Nguyen, Anh Tuan},
	year = {2020},
	pages = {9--14},
}

@article{daouadi_organization_2018,
	title = {Organization vs. {Individual}: {Twitter} {User} {Classification}.},
	abstract = {This paper presents a novel technique for classifying user accounts on Twitter. The main purpose of our classification is to distinguish the patterns of users from those of individuals and organizations. However, such a task is nontrivial. Classic and consolidated approaches use textual features from Natural Language Processing (NLP) for classification. Nevertheless, such approaches still have some drawbacks like the computational cost and the fact that they depend on a specific language. In this work, we propose a statistical-based approach based on metadata of user profiles, popularity of posts and other statistical features in order to recognize the type of users without using the textual content. We performed a set of experiments over a twitter dataset and learn-based algorithms. This yielded an F-measure of 95.6\% using the Random Forest algorithm and synthetic minority oversampling technique.},
	language = {en},
	author = {Daouadi, Kheir Eddine and Reba{\"i}, Rim Zghal and Amous, Ikram},
	month = jan,
	year = {2018},
	file = {Daouadi et al. - Organization vs. Individual Twitter User Classifi.pdf:C\:\\Users\\roman\\Zotero\\storage\\7AZ5IWRX\\Daouadi et al. - Organization vs. Individual Twitter User Classifi.pdf:application/pdf},
}

@misc{wankmuller_introduction_2022,
	title = {Introduction to {Neural} {Transfer} {Learning} with {Transformers} for {Social} {Science} {Text} {Analysis}},
	url = {http://arxiv.org/abs/2102.02111},
	doi = {10.48550/arXiv.2102.02111},
	abstract = {Transformer-based models for transfer learning have the potential to achieve high prediction accuracies on text-based supervised learning tasks with relatively few training data instances. These models are thus likely to benefit social scientists that seek to have as accurate as possible text-based measures but only have limited resources for annotating training data. To enable social scientists to leverage these potential benefits for their research, this paper explains how these methods work, why they might be advantageous, and what their limitations are. Additionally, three Transformer-based models for transfer learning, BERT (Devlin et al. 2019), RoBERTa (Liu et al. 2019), and the Longformer (Beltagy et al. 2020), are compared to conventional machine learning algorithms on three applications. Across all evaluated tasks, textual styles, and training data set sizes, the conventional models are consistently outperformed by transfer learning with Transformers, thereby demonstrating the benefits these models can bring to text-based social science research.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Wankm{\"u}ller, Sandra},
	month = aug,
	year = {2022},
	note = {arXiv:2102.02111 [cs, stat]},
	keywords = {Computer Science - Computation and Language, I.2.7, Statistics - Applications},
	annote = {Comment: 80 pages, 12 figures; changed the title; more focused presentation of contents; moved contents to the appendix; created a new Figure 9; discussion of additional aspects (zero-shot learning, cross-lingual learning, interpretability, foundation models); removed old Figures 4 and 5; made non-essential changes to Figures 1, 2, 4, 6, 7, 8 and 10; changed notation. The original results are unchanged},
	file = {arXiv Fulltext PDF:C\:\\Users\\roman\\Zotero\\storage\\28ZH7NEP\\Wankm{\"u}ller - 2022 - Introduction to Neural Transfer Learning with Tran.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\roman\\Zotero\\storage\\C4X3MNMN\\2102.html:text/html},
}

@book{goodfellow_deep_2016-2,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	annote = {http://www.deeplearningbook.org},
}
