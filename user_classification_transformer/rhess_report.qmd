---
title: "Quarto Basics"
format:
  html:
    code-fold: true
jupyter: python3
---


# Title: Improving the classification of General Public and Instituational Twitter Users with Transformers.


# Introduction: brief explanation of the topic & its relevance plus the methodological approach

-	When using twitter data for research, important to differentiate between individual and organizational accounts
-	Present different definitions of individual and organizational
-	Present reasons why differentiating is important


Twitter is an interesting data source for computational communication scientists.
The Twitter API makes it comparably easy to obtain large amounts of text data.
This enables Big Data analyses for various purposes, but comes with its own challenges.
For example, Twitter users might not be representative of the general offline population.
Also, some twitter accounts might be fictious (cite Ahmed et al., 2017).
Especially the detection of bots has triggered a lot of research (e.g. cite Kudugunta, Feng et al., Shukla).
When researchers analyze Twitter data, they may want to exclude these cases from their datasets.
Or they might also want to isolate users from a certain country (cite Kwon) or gender (cite Vashtisth), depending on the research questions.

Another differentiation between Twitter users, which can potentially impact the findings of a given research question is the separation of general public and institutional users.
Institutional accounts differ from general public accounts in that they might represent a (governmental) institution, the media, or some other kind of organization.
Communication on Twitter can be social or non-social and spontaneous or strategic.
When researchers are mainly interested in natural communication between individuals, they might want to filter out the institutional users first (cite Kwon).
This filtering can also be helpful for recommendation engines, products opinion mining tools etc. (cite Kheir).
As institutional accounts make up 9% of Twitter accounts (cite McCorriston) they can skew the tweet sample significantly.

Defining who is a general public and who is an institutional user is not trivial and different approaches exist in the literature.
Li et al (cite) aimed at differentiating between male, female and *brand-related* twitter users.
Yan et al. (cite) differentiate between *open* and *closed* accounts, where open accounts publish information to the public, with their goal being to promote products, services or themselves.
Closed accounts on the other hand publish information about their daily lives and use twitter to communicate with their friends.
McCorriston et al. (cite)  created a tool to distinguish between organizational and individual accounts but did not define their definition of *organizational* in detail.
Lastly, De Choudhury et al. (cite) split Twitter users into ordinary individuals, organizations, and journalists/media bloggers.
The existing diversity and intransparency in defining user categories is not ideal.
There is potential for a unified system of deciding which users belong to which group to make it easier for researchers to filter out institutional users and make research more comparable.

Some differences between institutional and general public users have been highlighted by Kwon et al(cite).
In their study, general public users showed more retweeting than institutional users, less analytic language use and more affective language use.

Overall, for the above mentioned reasons, there is a need for reliable classification models, allowing researchers to filter out institutional accounts when their target sample should only contain general public users.
Several techniques for this purpose have already been introduced. They will be presented in the next section.


# Literature review: (short) review of existing literature incl. derivation and formulation of hypotheses/research questions

- Present different existing approaches for differentiating between individual and organizational accounts

- Present problem: even though many of them use text data, no transformer models used! No dense text embeddings.

- Present paper, stressing the advantages of transformer models in social sciences text analysis

- Present hypotheses:

    - Transformers achieve better performance within the datasets
    - Transformers achieve better performances across datasets (generalization)



- TWIROLE:
    - simple text feature
    - picture etc additionally

    - name, description, Twitter Follower-Friend Ratio, profile image, tweet
    - hybrid classifier
    - text features are rather simple (e.g. frequency based)
    - profile images: Convolutional Neural Network

Li et al. (cite) introduced TWIROLE, which uses Twitter users' name, description, the Follower-Friend Ratio, profile image and tweets as features for a hybrid classifier.
The text features are rather simple and frequency based in this case.



- Mc Corriston et al. (cite)
    - post content features, stylistic features, structural and behavioral features.
    - text features mostly frequency based, otherwise numerical features, fed into a support vector machine

McCorriston et al. (cite) used post content, stylistic as well as structural and behavioral features for their classifier.
The text features they implemented were mostly frequency-based, the other features were numerical.
All features were then fed into a support vector machine for classification

- Kheir et al (cite)
    - did not ue textual features at all, solely statistical-based on features such as number of followers, liked tweets, posts per day etc.

Kheir et al. (cite) did not use textual features at all.
Their features were solely statistical-based and included measures like the number of followers, liked tweets, posts per day etc.


- Kwon et al. (cite)
    - solely based on twitter profile descriptions
    - used the sci-kit learn package in python. Their representation of each profile text only considered the 500 most frequent terms
    - Thus the representation of each profile text was a 500 dimensional sparse vector containing the raw frequencies for the top 500 words.
    - Then a random forrest classifier was used to separate general public profiles from institutional accounts.

Kwon et al. (cite) trained a classifier solely based on twitter profile descriptions.
They used the sci-kit learn package in python and their representations of each profile text only considered the 500 most frequent terms in the data.
Thus the representation of each profile text was a 500 dimensional sparse vector containing the raw frequencies for the top 500 most frequent words.
A random forrest classifier was then used to separate general public profiles from institutional accounts.

None of the approaches we could find used dense (pretrained) text embeddings or transformers.
However, using such techniques yields high potential when working with natural language, as can be found in Twitter data.

Wankmüller (cite) illustrated this potential of Neural Transfer Learning using transformers for text analysis in the social sciences.
Transfer learning refers to a setting where something that has been learned in one situation is exploited in another situation. (cite Goodfellow p. 538)
For Transformer models such as BERT or RoBERTa, pretrained weights and token embeddings are freely available online.
These models have been trained with enormous amounts of data on well-designed pretraining tasks.
Researchers can use these pretrained encoders to obtain dense representations of their text data.
These features usually contain far more information than conventional, more simple text representations, as they use the knowledge inherent in the huge text corpora used for pretraining the transformer models.
They can then be fed into a smaller model which is then trained on the actual task the researcher is interested in.

This can improve the prediction performance on different NLP tasks (cite Wankmüller).

## Goal of the study:

We believe that this also holds for the task of Twitter user classification into general public and institutional users.
When text data is available, it is reasonable to expect that the usage of pretrained word embeddings and encoders can leverage performance.
Using a pretrained BERT model is especially promising in our case, because of the availability of BERTweet, which is based on the RoBERTa pre-training procedure.
It has been trained on 850 million english tweets and consists of 135 million parameters (cite Dat).

The goal of this work is to show the advantages of transformer models over conventional machine learning techniques in user classification.
In order to do that we tried to locate the datasets used by previous classification models and surpass the original authors' performance by using BERTweet for generating text features.
The only dataset we could find was that used by Kwon et al (cite), who used Twitter profile descriptions as the sole input for their classifier.
To be precise, the authors used five different datasets which were collected in the context of different events (*boston*, *brussels*, *mesa*, *quebec*, *random*).
They trained one model per dataset and then evaluated the performances within the same dataset as well as the classifier's generalization across the other datasets.

## Hypotheses

As the profile descriptions are the only input used, we expect BERTweet to enable a better performance than in the original study.
Thus we define our hypotheses as follows:

By designing our own classifier, using the BERTweet encoder to generate dense and meaningful representations of twitter profile descriptions, we expect to achieve

H1.1: a better performance on the *boston* test set, when trained on the *boston* training set than in the original study.
H1.2: a better performance on all other test sets, when trained on the *boston* training set than in the original study.

H2.1: a better performance on the *brussels* test set, when trained on the *brussels* training set than in the original study.
H2.2: a better performance on all other test sets, when trained on the *brussels* training set than in the original study.

H3.1: a better performance on the *mesa* test set, when trained on the *mesa* training set than in the original study.
H3.2: a better performance on all other test sets, when trained on the *mesa* training set than in the original study.

H4.1: a better performance on the *quebec* test set, when trained on the *quebec* training set than in the original study.
H4.2: a better performance on all other test sets, when trained on the *quebec* training set than in the original study.

H5.1: a better performance on the *random* test set, when trained on the *random* training set than in the original study.
H5.2: a better performance on all other test sets, when trained on the *random* training set than in the original study.


# Method: description of the data set (origins & structure of the data, sampling approach, data preprocessing) and the analysis logic (incl. detailed explanation of used software packages & functions/models)

- describe datasets
- describe preprocessing
- describe tokenization
- describe RoBERTa model


# Results: testing of hypotheses/answering of research questions (incl. data visualizations where applicable)

- present results (compared to original authors' results)

# Discussion: summary and interpretation of results, limitations of the methodological approach, outlook

- why did this work? what does it mean
- why did it not work?
- what could be improved?

